<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>FuckingCode</title>
  
  <subtitle>-安卓系统-  -源码分析-  -linux编程-  -设计模式-</subtitle>
  <link href="/FuckCode/atom.xml" rel="self"/>
  
  <link href="https://891904833.gitee.io/FuckCode/"/>
  <updated>2019-04-10T07:49:48.337Z</updated>
  <id>https://891904833.gitee.io/FuckCode/</id>
  
  <author>
    <name>Allies</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>源码分析之Handler、Looper及MessageQueue之间关系</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/04/09/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BHandler%E3%80%81Looper%E5%8F%8AMessageQueue%E4%B9%8B%E9%97%B4%E5%85%B3%E7%B3%BB/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/04/09/源码分析之Handler、Looper及MessageQueue之间关系/</id>
    <published>2019-04-09T06:20:05.000Z</published>
    <updated>2019-04-10T07:49:48.337Z</updated>
    
    <content type="html"><![CDATA[<p><strong>在 Android 的应用开发中，线程之间的通信机制主要是以 Handler 机制为主。我们都知道 Android 中的 UI 更新只能在主线程中执行，通常的做法是，我们在 Activity 中直接 new Handler，通过 handler 发送消息，在 handlerMessage 处理即可。本篇博文就根据源码来具体分析其中工作机制以及源码架构。</strong><br><a id="more"></a></p><h1 id="Java-层的三巨头"><a href="#Java-层的三巨头" class="headerlink" title="Java 层的三巨头"></a>Java 层的三巨头</h1><p>上层的 Java 代码，集中于以下三个类，Handler，Looper 以及 MessageQueue，下面我们集中分析它们。</p><h2 id="Handler"><a href="#Handler" class="headerlink" title="Handler"></a>Handler</h2><p>我们通常都会在 Activity 中直接新建 Handler 来实现主线程下 UI 的更新。我们思考一下为什么直接新建的 handler 便可以更新 UI，默认为主线程呢？（直接新建 Handler 可以更新 UI，但是涉及到 Activity 生命周期以及复杂的逻辑延时处理等，此处建议以弱引用、虚引用方式持有对象，否则在延时处理中，Activity 已经销毁了，才反过来处理消息必定会发生崩溃）。</p><h3 id="Handler-与-Looper-的绑定"><a href="#Handler-与-Looper-的绑定" class="headerlink" title="Handler 与 Looper 的绑定"></a>Handler 与 Looper 的绑定</h3><p>我们都知道 Activity 由 ActivityThread 启动，之后会调用 ActivityThread 的 main 函数，其内部有如下代码：</p><pre><code>public static void main(String[] args) {    Trace.traceBegin(Trace.TRACE_TAG_ACTIVITY_MANAGER, &quot;ActivityThreadMain&quot;);    SamplingProfilerIntegration.start();    // CloseGuard defaults to true and can be quite spammy.  We    // disable it here, but selectively enable it later (via    // StrictMode) on debug builds, but using DropBox, not logs.    CloseGuard.setEnabled(false);    Environment.initForCurrentUser();    // Set the reporter for event logging in libcore    EventLogger.setReporter(new EventLoggingReporter());    AndroidKeyStoreProvider.install();    // Make sure TrustedCertificateStore looks in the right place for CA certificates    final File configDir = Environment.getUserConfigDirectory(UserHandle.myUserId());    TrustedCertificateStore.setDefaultUserDirectory(configDir);    Process.setArgV0(&quot;&lt;pre-initialized&gt;&quot;);    // 创建主线程 looper,只创建一次    Looper.prepareMainLooper();    ActivityThread thread = new ActivityThread();    // attach到系统进程    thread.attach(false);    if (sMainThreadHandler == null) {        sMainThreadHandler = thread.getHandler();    }    if (false) {        Looper.myLooper().setMessageLogging(new                LogPrinter(Log.DEBUG, &quot;ActivityThread&quot;));    }    // End of event ActivityThreadMain.    Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER);    // 主线程进入循环状态    Looper.loop();    throw new RuntimeException(&quot;Main thread loop unexpectedly exited&quot;);}</code></pre><p>由此可见，Activity 中已经默认给我们创建了 Looper 轮训器。此时我们直接可以使用 Handler 机制进行消息通信了。</p><p>下面我们来看 prepareMainLooper</p><pre><code>public static void prepareMainLooper() {    prepare(false);    synchronized (Looper.class) {        if (sMainLooper != null) {            throw new IllegalStateException(&quot;The main Looper has already been prepared.&quot;);        }        sMainLooper = myLooper();    }}</code></pre><p>此处我们看到，直接调用 prepare，注意参数 false，之后调用 sMainLooper = myLooper()，将内部 sMainLooper 赋值，及主线程 looper 循环器。</p><pre><code>private static void prepare(boolean quitAllowed) {    if (sThreadLocal.get() != null) {        throw new RuntimeException(&quot;Only one Looper may be created per thread&quot;);    }    sThreadLocal.set(new Looper(quitAllowed));}</code></pre><p>此处最后通过线程本地存储 sThreadLocal 新建 Looper 对象并设置进去。sMainLooper 获取就是这个对象。</p><pre><code>private Looper(boolean quitAllowed) {    mQueue = new MessageQueue(quitAllowed);    mThread = Thread.currentThread();}</code></pre><p>此处在 Looper 的构造函数中，新建了 MessageQueue 对象。这样，在应用 Activity 启动之时，Looper 与 MessageQueue就已经被系统分配完毕，后续我们直接使用即可，此处的分析暂时告一段落。</p><h3 id="Handler-实例化"><a href="#Handler-实例化" class="headerlink" title="Handler 实例化"></a>Handler 实例化</h3><p>Handler 拥有多个构造函数，我们在 Activity 中直接使用的 Handler 为默认的构造函数，如下：</p><pre><code>// 默认的构造函数 public Handler() {    this(null, false);}// 参数为 null，falsepublic Handler(Callback callback, boolean async) {    if (FIND_POTENTIAL_LEAKS) {        final Class&lt;? extends Handler&gt; klass = getClass();        if ((klass.isAnonymousClass() || klass.isMemberClass() || klass.isLocalClass()) &amp;&amp;                (klass.getModifiers() &amp; Modifier.STATIC) == 0) {            Log.w(TAG, &quot;The following Handler class should be static or leaks might occur: &quot; +                klass.getCanonicalName());        }    }    // 直接从 Looper 中获取 mLooper 对象    mLooper = Looper.myLooper();    if (mLooper == null) {        throw new RuntimeException(            &quot;Can&apos;t create handler inside thread that has not called Looper.prepare()&quot;);    }    // Looper 创建即产生了 MessageQueue    mQueue = mLooper.mQueue;    mCallback = callback;    mAsynchronous = async;}</code></pre><p>有上面分析，此处直接获取了 mLooper 对象，由于在 Activity 之前调用了 Looper.prepareMainLooper() 函数，之前分析可以得知，此处获取的便是 sMainLooper 轮训器。Looper 创建之时便已经 与 MessageQueue 建立联系，此处直接可以获取消息队列 mQueue。</p><h3 id="Handler-获取消息"><a href="#Handler-获取消息" class="headerlink" title="Handler 获取消息"></a>Handler 获取消息</h3><p>Handler 内部维持了一个消息池，我们通过 obtainMessage 便可以获取一个消息实例 Message。obtainMessage 重载了很多方法，最终走向 Message.obtain 中去，Message.obtain 中根据多种参数的重载，最终指向了内部方法 obtain()。</p><pre><code>public static Message obtain() {    synchronized (sPoolSync) {        if (sPool != null) {            Message m = sPool;            sPool = m.next;            m.next = null;            m.flags = 0; // clear in-use flag            sPoolSize--;            return m;        }    }    return new Message();}</code></pre><p>此方法原译注释 </p><p><strong>Return a new Message instance from the global pool. Allows us to avoid allocating new objects in many cases.</strong></p><p>译文为：从全局池返回新的消息实例。允许我们在许多情况下避免分配新对象。</p><p>由此可见我们通过此方法可以避免自己 new Message，直接是使用提供的接口获取。减少分配对象造成的内存消耗。</p><p>关于 Message 内更多具体的分析，可以参考一下博文</p><p><a href="https://blog.csdn.net/xmh19936688/article/details/51901338" target="_blank" rel="noopener">Message详细解析</a></p><h3 id="Handler-发送消息、Runnable以及取消"><a href="#Handler-发送消息、Runnable以及取消" class="headerlink" title="Handler 发送消息、Runnable以及取消"></a>Handler 发送消息、Runnable以及取消</h3><p>通过 Handler 对象 send<em>Message</em>，或者 post* runnable 都可以，其内部方法重载众多，我们直接追溯根源，其最终调用到 enqueueMessage 方法，期间涉及延迟等不作过多分析。</p><pre><code>private boolean enqueueMessage(MessageQueue queue, Message msg, long uptimeMillis) {    msg.target = this;    if (mAsynchronous) {        msg.setAsynchronous(true);    }    return queue.enqueueMessage(msg, uptimeMillis);}</code></pre><p>代码可以看出，最终还是通过内部持有对象 MessageQueue queue 的 enqueueMessage 方法来实现。其内部具体逻辑由后面的 MessageQueue 章节具体分析。</p><p>通过 removeMessages 方法即可取消一个消息，通过 removeCallbacksAndMessages 可取消一个 Callback，其最终实现代码再后面章节中具体分析。</p><pre><code>public final void removeMessages(int what, Object object) {    mQueue.removeMessages(this, what, object);}public final void removeCallbacksAndMessages(Object token) {    mQueue.removeCallbacksAndMessages(this, token);}</code></pre><p>从上面分析我们可以得知为什么我们可以拿来主义，直接使用了 Handler，原因是在应用启动之后，系统已经自动帮我们完成了 Looper 以及 MessageQueue 的创建。</p><h3 id="Handler-处理消息"><a href="#Handler-处理消息" class="headerlink" title="Handler 处理消息"></a>Handler 处理消息</h3><p>Handler 内部对消息的处理进行了一定程度的划分，如下代码：</p><pre><code>public void handleMessage(Message msg) {}public void dispatchMessage(Message msg) {    // 首先调用消息中的 callback 回调    if (msg.callback != null) {        handleCallback(msg);    } else {        // 然后检查 Handler 中是否存在 mCallback，回调        if (mCallback != null) {            if (mCallback.handleMessage(msg)) {                return;            }        }        // 最后调用 Handler 自身的 handleMessage        handleMessage(msg);    }}</code></pre><p>其接口 handleMessage 由用户自己实现，dispatchMessage 为系统内部默认的处理方式，根据消息内容作不同的处理。</p><h2 id="Looper"><a href="#Looper" class="headerlink" title="Looper"></a>Looper</h2><p>Looper 我们可以称其为消息轮训器，其主要的工作是不断的读取 MessageQueue 内部的消息，取到之后便交给 Handler 来处理。</p><p>之前在 handler 中我们分析了其绑定过程，其调用的 prepareMainLooper 方法只能在主线程中调用一个，我们开发者使用时候不可以直接调用，下面我们用代码来说明。</p><h3 id="Looper-的使用"><a href="#Looper-的使用" class="headerlink" title="Looper 的使用"></a>Looper 的使用</h3><p>Looper 内部有一个静态变量 sThreadLocal，其具有单例唯一性。</p><pre><code>static final ThreadLocal&lt;Looper&gt; sThreadLocal = new ThreadLocal&lt;Looper&gt;();</code></pre><p>此时，如果你在主线程中调用了 prepare，那么你将会收到一个RuntimeException，代码中如下：</p><pre><code>public static void prepare() {    prepare(true);}private static void prepare(boolean quitAllowed) {    if (sThreadLocal.get() != null) {        throw new RuntimeException(&quot;Only one Looper may be created per thread&quot;);    }    sThreadLocal.set(new Looper(quitAllowed));}</code></pre><p>我们如果要在一个线程中使用 Looper，我们只需要做以下几步即可：</p><ol><li>调用 prepare 初始化 Looper 对象</li><li>调用 loop 方法，启动 Looper 消息轮训</li><li>处理 Handler 的 handlerMessage 方法</li><li>调用 quit，停止 Lopper 轮训工作</li></ol><h3 id="Looper-构造"><a href="#Looper-构造" class="headerlink" title="Looper 构造"></a>Looper 构造</h3><p>Looper 的构造函数，在 prepare 方法中就已经实现了，sThreadLocal.set(new Looper(quitAllowed))，具体代码如下：</p><pre><code>private Looper(boolean quitAllowed) {    mQueue = new MessageQueue(quitAllowed);    mThread = Thread.currentThread();}</code></pre><h3 id="prepare"><a href="#prepare" class="headerlink" title="prepare"></a>prepare</h3><p>prepare 方法用于初始化创建一个 Looper，内部新建 Looper 对象，其还会创建 MessageQueue 对象，与其绑定，完成消息队列与轮训的双重工作。具体代码见上。</p><h3 id="loop"><a href="#loop" class="headerlink" title="loop"></a>loop</h3><p>loop 使得 Looper 轮训器开始工作，不断检测 MessageQueue 内部是否存在消息需要处理，代码如下：</p><pre><code>public static void loop() {    final Looper me = myLooper();    if (me == null) {        throw new RuntimeException(&quot;No Looper; Looper.prepare() wasn&apos;t called on this thread.&quot;);    }    // 获取 loop 轮训器的消息队列 MessageQueue    final MessageQueue queue = me.mQueue;    // Make sure the identity of this thread is that of the local process,    // and keep track of what that identity token actually is.    Binder.clearCallingIdentity();    final long ident = Binder.clearCallingIdentity();    // 无限循环    for (;;) {        // 调用 MessageQueue 的 next 获取下一条消息        Message msg = queue.next(); // might block        if (msg == null) {            // No message indicates that the message queue is quitting.            return;        }        // This must be in a local variable, in case a UI event sets the logger        Printer logging = me.mLogging;        if (logging != null) {            logging.println(&quot;&gt;&gt;&gt;&gt;&gt; Dispatching to &quot; + msg.target + &quot; &quot; +                    msg.callback + &quot;: &quot; + msg.what);        }        // 分发消息，msg.target 为 msg 所依赖的 Handler对象        msg.target.dispatchMessage(msg);        if (logging != null) {            logging.println(&quot;&lt;&lt;&lt;&lt;&lt; Finished to &quot; + msg.target + &quot; &quot; + msg.callback);        }        // Make sure that during the course of dispatching the        // identity of the thread wasn&apos;t corrupted.        final long newIdent = Binder.clearCallingIdentity();        if (ident != newIdent) {            Log.wtf(TAG, &quot;Thread identity changed from 0x&quot;                    + Long.toHexString(ident) + &quot; to 0x&quot;                    + Long.toHexString(newIdent) + &quot; while dispatching to &quot;                    + msg.target.getClass().getName() + &quot; &quot;                    + msg.callback + &quot; what=&quot; + msg.what);        }        // 回收消息资源        msg.recycleUnchecked();    }}</code></pre><p>代码中我们可以看出，这里在无限循环中，通过 queue.next() 不断获取消息，得到的消息便调用 msg.target.dispatchMessage(msg) 将消息交与其 handler 进行处理，最终在将 msg 消息资源进行回收。</p><h3 id="quit"><a href="#quit" class="headerlink" title="quit"></a>quit</h3><p>通过 quit 即可停止 Looper 的工作，逻辑很简单，调用 MessageQueue 的 quit，其包含普通退出和安全退出</p><pre><code>public void quit() {    mQueue.quit(false);}public void quitSafely() {    mQueue.quit(true);}</code></pre><h2 id="MessageQueue"><a href="#MessageQueue" class="headerlink" title="MessageQueue"></a>MessageQueue</h2><p>MessageQueue 为消息队列，承载 Message 对象的结构。其可以安排 Messgae 的插入，删除，查找，退出等。我们分别来看一下其内部结构。</p><h3 id="MessageQueue-的初始化"><a href="#MessageQueue-的初始化" class="headerlink" title="MessageQueue 的初始化"></a>MessageQueue 的初始化</h3><p>Java 层的 MessageQueue 初始化，直接走向 Native 层，通过 JNI 指向 Native 层，后续分析 Native 层的逻辑。</p><pre><code>MessageQueue(boolean quitAllowed) {    mQuitAllowed = quitAllowed;    // 调用 native 层，对应于 android_os_MessageQueue.cpp    mPtr = nativeInit();}</code></pre><h3 id="enqueueMessage-发送消息"><a href="#enqueueMessage-发送消息" class="headerlink" title="enqueueMessage 发送消息"></a>enqueueMessage 发送消息</h3><p>Java 层的 Handler 通过 send*Mesage 等通过 MessageQueue 内部方法 enqueueMessage 来发送消息，代码如下：</p><pre><code>boolean enqueueMessage(Message msg, long when) {    if (msg.target == null) {        throw new IllegalArgumentException(&quot;Message must have a target.&quot;);    }    if (msg.isInUse()) {        throw new IllegalStateException(msg + &quot; This message is already in use.&quot;);    }    synchronized (this) {        // 退出轮训        if (mQuitting) {            IllegalStateException e = new IllegalStateException(                    msg.target + &quot; sending message to a Handler on a dead thread&quot;);            Log.w(TAG, e.getMessage(), e);            // 回收资源            msg.recycle();            return false;        }        msg.markInUse();        msg.when = when;        Message p = mMessages;        boolean needWake;        if (p == null || when == 0 || when &lt; p.when) {            // New head, wake up the event queue if blocked.            msg.next = p;            mMessages = msg;            needWake = mBlocked;        } else {            // Inserted within the middle of the queue.  Usually we don&apos;t have to wake            // up the event queue unless there is a barrier at the head of the queue            // and the message is the earliest asynchronous message in the queue.            needWake = mBlocked &amp;&amp; p.target == null &amp;&amp; msg.isAsynchronous();            Message prev;            // 找到合适的位置，添加 msg 到队列中去            for (;;) {                prev = p;                p = p.next;                if (p == null || when &lt; p.when) {                    break;                }                if (needWake &amp;&amp; p.isAsynchronous()) {                    needWake = false;                }            }            msg.next = p; // invariant: p == prev.next            prev.next = msg;        }        // We can assume mPtr != 0 because mQuitting is false.        // 需要唤醒操作，Native 操作        if (needWake) {            nativeWake(mPtr);        }    }    return true;}</code></pre><p>MessageQueue 通过 enqueueMessage 将 msg 封装到队列中去，之后如果需要唤醒，便会调用 nativeWake 进行唤醒。</p><h3 id="next-获取消息"><a href="#next-获取消息" class="headerlink" title="next 获取消息"></a>next 获取消息</h3><p>Looper 消息轮训器通过无限循环调用 MessageQueue 的 next方法，不断获取消息并处理，代码如下：</p><pre><code>Message next() {    // Return here if the message loop has already quit and been disposed.    // This can happen if the application tries to restart a looper after quit    // which is not supported.    final long ptr = mPtr;    if (ptr == 0) {        return null;    }    int pendingIdleHandlerCount = -1; // -1 only during first iteration    int nextPollTimeoutMillis = 0;    // 无限循环    for (;;) {        if (nextPollTimeoutMillis != 0) {            Binder.flushPendingCommands();        }        // 调用 Native 层的 nativePollOnce        nativePollOnce(ptr, nextPollTimeoutMillis);        synchronized (this) {            // Try to retrieve the next message.  Return if found.            final long now = SystemClock.uptimeMillis();            Message prevMsg = null;            Message msg = mMessages;            // 异步消息            if (msg != null &amp;&amp; msg.target == null) {                // Stalled by a barrier.  Find the next asynchronous message in the queue.                do {                    prevMsg = msg;                    msg = msg.next;                } while (msg != null &amp;&amp; !msg.isAsynchronous());            }            if (msg != null) {                // 延迟消息处理                if (now &lt; msg.when) {                    // Next message is not ready.  Set a timeout to wake up when it is ready.                    nextPollTimeoutMillis = (int) Math.min(msg.when - now, Integer.MAX_VALUE);                } else {                    // Got a message.                    // 获取消息并返回                    mBlocked = false;                    if (prevMsg != null) {                        prevMsg.next = msg.next;                    } else {                        mMessages = msg.next;                    }                    msg.next = null;                    if (DEBUG) Log.v(TAG, &quot;Returning message: &quot; + msg);                    msg.markInUse();                    return msg;                }            } else {                // No more messages.                nextPollTimeoutMillis = -1;            }            // Process the quit message now that all pending messages have been handled.            if (mQuitting) {                dispose();                return null;            }            // If first time idle, then get the number of idlers to run.            // Idle handles only run if the queue is empty or if the first message            // in the queue (possibly a barrier) is due to be handled in the future.            if (pendingIdleHandlerCount &lt; 0                    &amp;&amp; (mMessages == null || now &lt; mMessages.when)) {                pendingIdleHandlerCount = mIdleHandlers.size();            }            if (pendingIdleHandlerCount &lt;= 0) {                // No idle handlers to run.  Loop and wait some more.                mBlocked = true;                continue;            }            if (mPendingIdleHandlers == null) {                // 最大空闲线程                mPendingIdleHandlers = new IdleHandler[Math.max(pendingIdleHandlerCount, 4)];            }            mPendingIdleHandlers = mIdleHandlers.toArray(mPendingIdleHandlers);        }        // Run the idle handlers.        // We only ever reach this code block during the first iteration.        for (int i = 0; i &lt; pendingIdleHandlerCount; i++) {            final IdleHandler idler = mPendingIdleHandlers[i];            mPendingIdleHandlers[i] = null; // release the reference to the handler            boolean keep = false;            try {                // 调用空闲线程 handler 的 queueIdle                keep = idler.queueIdle();            } catch (Throwable t) {                Log.wtf(TAG, &quot;IdleHandler threw exception&quot;, t);            }            if (!keep) {                synchronized (this) {                    mIdleHandlers.remove(idler);                }            }        }        // Reset the idle handler count to 0 so we do not run them again.        pendingIdleHandlerCount = 0;        // While calling an idle handler, a new message could have been delivered        // so go back and look again for a pending message without waiting.        nextPollTimeoutMillis = 0;    }}</code></pre><p>从上面可以看出，next 优先调用 nativePollOnce 处理 Native 层的消息，其次在获取到 Java 层 MessageQueue 中的消息并返回。</p><h3 id="quit-退出"><a href="#quit-退出" class="headerlink" title="quit 退出"></a>quit 退出</h3><p>MessageQueue 通过 quit 方法，完成 Looper轮训器的退出操作。</p><pre><code>void quit(boolean safe) {    if (!mQuitAllowed) {        throw new IllegalStateException(&quot;Main thread not allowed to quit.&quot;);    }    synchronized (this) {        if (mQuitting) {            return;        }        mQuitting = true;        if (safe) {            removeAllFutureMessagesLocked();        } else {            removeAllMessagesLocked();        }        // We can assume mPtr != 0 because mQuitting was previously false.        nativeWake(mPtr);    }}</code></pre><p>其上层 Looper 通过普通模式和安全模式调用到此，最终通过 nativeWake 指向 Natvie。</p><h3 id="消息以及回调的删除"><a href="#消息以及回调的删除" class="headerlink" title="消息以及回调的删除"></a>消息以及回调的删除</h3><p>首先简单看一下消息的删除，方法涉及多个重载，我们这里只看其中一个，代码如下：</p><pre><code>// 根据 handler，what，以及 object 查找到 msg 并删除void removeMessages(Handler h, int what, Object object) {    if (h == null) {        return;    }    synchronized (this) {        // 获取当前消息        Message p = mMessages;        // Remove all messages at front.        // 便利找到对应的消息        while (p != null &amp;&amp; p.target == h &amp;&amp; p.what == what               &amp;&amp; (object == null || p.obj == object)) {            Message n = p.next;            mMessages = n;            p.recycleUnchecked();            p = n;        }        // Remove all messages after front.        // 删除消息，链接前后消息        while (p != null) {            Message n = p.next;            if (n != null) {                if (n.target == h &amp;&amp; n.what == what                    &amp;&amp; (object == null || n.obj == object)) {                    Message nn = n.next;                    n.recycleUnchecked();                    p.next = nn;                    continue;                }            }            p = n;        }    }}</code></pre><p>删除消息回调</p><pre><code>// 根据 handler 以及 object 删除回调void removeCallbacksAndMessages(Handler h, Object object) {    if (h == null) {        return;    }    synchronized (this) {        Message p = mMessages;        // Remove all messages at front.        while (p != null &amp;&amp; p.target == h                &amp;&amp; (object == null || p.obj == object)) {            Message n = p.next;            mMessages = n;            p.recycleUnchecked();            p = n;        }        // Remove all messages after front.        while (p != null) {            Message n = p.next;            if (n != null) {                if (n.target == h &amp;&amp; (object == null || n.obj == object)) {                    Message nn = n.next;                    n.recycleUnchecked();                    p.next = nn;                    continue;                }            }            p = n;        }    }}</code></pre><h1 id="JNI-层分析"><a href="#JNI-层分析" class="headerlink" title="JNI 层分析"></a>JNI 层分析</h1><p>Java 通过 JNI 方式，链接到 Native 层代码 android_os_MessageQueue.cpp，具体代码如下：</p><pre><code>static JNINativeMethod gMessageQueueMethods[] = {    /* name, signature, funcPtr */    { &quot;nativeInit&quot;, &quot;()J&quot;, (void*)android_os_MessageQueue_nativeInit },    { &quot;nativeDestroy&quot;, &quot;(J)V&quot;, (void*)android_os_MessageQueue_nativeDestroy },    { &quot;nativePollOnce&quot;, &quot;(JI)V&quot;, (void*)android_os_MessageQueue_nativePollOnce },    { &quot;nativeWake&quot;, &quot;(J)V&quot;, (void*)android_os_MessageQueue_nativeWake },    { &quot;nativeIsPolling&quot;, &quot;(J)Z&quot;, (void*)android_os_MessageQueue_nativeIsPolling },    { &quot;nativeSetFileDescriptorEvents&quot;, &quot;(JII)V&quot;,            (void*)android_os_MessageQueue_nativeSetFileDescriptorEvents },};int register_android_os_MessageQueue(JNIEnv* env) {    int res = RegisterMethodsOrDie(env, &quot;android/os/MessageQueue&quot;, gMessageQueueMethods,                                NELEM(gMessageQueueMethods));    jclass clazz = FindClassOrDie(env, &quot;android/os/MessageQueue&quot;);    gMessageQueueClassInfo.mPtr = GetFieldIDOrDie(env, clazz, &quot;mPtr&quot;, &quot;J&quot;);    gMessageQueueClassInfo.dispatchEvents = GetMethodIDOrDie(env, clazz,            &quot;dispatchEvents&quot;, &quot;(II)I&quot;);    return res;}</code></pre><h2 id="nativeInit"><a href="#nativeInit" class="headerlink" title="nativeInit"></a>nativeInit</h2><p>Java 层的 nativeInit 在 MessageQueue 初始化时调用，代码如下：</p><pre><code>static jlong android_os_MessageQueue_nativeInit(JNIEnv* env, jclass clazz) {    // 新建 NativeMessageQueue，对应与 Java 层    NativeMessageQueue* nativeMessageQueue = new NativeMessageQueue();    if (!nativeMessageQueue) {        jniThrowRuntimeException(env, &quot;Unable to allocate native queue&quot;);        return 0;    }    nativeMessageQueue-&gt;incStrong(env);    return reinterpret_cast&lt;jlong&gt;(nativeMessageQueue);}</code></pre><p>其对象 NativeMessageQueue 为内部类，其构造函数如下：</p><pre><code>NativeMessageQueue::NativeMessageQueue() :        mPollEnv(NULL), mPollObj(NULL), mExceptionObj(NULL) {    mLooper = Looper::getForThread();    // 创建 Native 层的 Looper 对象    if (mLooper == NULL) {        mLooper = new Looper(false);        Looper::setForThread(mLooper);    }}</code></pre><p>Native 层的 NativeMessageQueue 在构造时候回依赖 Looper 对象，此时回新建 Native 层的 Looper 对象。</p><h2 id="nativeWake"><a href="#nativeWake" class="headerlink" title="nativeWake"></a>nativeWake</h2><p>Java 层通过 enqueueMessage 发消息，quit 退出以及 removeSyncBarrier 方法都会调用到 nativeWake，其指向函数 android_os_MessageQueue_nativeWake，代码如下：</p><pre><code>static void android_os_MessageQueue_nativeWake(JNIEnv* env, jclass clazz, jlong ptr) {    NativeMessageQueue* nativeMessageQueue = reinterpret_cast&lt;NativeMessageQueue*&gt;(ptr);    nativeMessageQueue-&gt;wake();}</code></pre><p>同样依赖内部成员 nativeMessageQueue-&gt;wake() 方法，最终指向 mLooper-&gt;wake()。</p><pre><code>void NativeMessageQueue::wake() {    mLooper-&gt;wake();}</code></pre><h2 id="nativePollOnce"><a href="#nativePollOnce" class="headerlink" title="nativePollOnce"></a>nativePollOnce</h2><p>Java 层的 nativePollOnce 调用于 上层 MessageQueue 的 next 方法，用于不断获取数据处理操作，这里是优先处理 Native 的逻辑。在这里指向函数 android_os_MessageQueue_nativePollOnce，，如下：</p><pre><code>static void android_os_MessageQueue_nativePollOnce(JNIEnv* env, jobject obj,        jlong ptr, jint timeoutMillis) {    NativeMessageQueue* nativeMessageQueue = reinterpret_cast&lt;NativeMessageQueue*&gt;(ptr);    nativeMessageQueue-&gt;pollOnce(env, obj, timeoutMillis);}</code></pre><p>函数最终指向了 nativeMessageQueue-&gt;pollOnce，代码如下：</p><pre><code>void NativeMessageQueue::pollOnce(JNIEnv* env, jobject pollObj, int timeoutMillis) {    mPollEnv = env;    mPollObj = pollObj;    // 调用持有对象 looper 的 pollOnce    mLooper-&gt;pollOnce(timeoutMillis);    mPollObj = NULL;    mPollEnv = NULL;    if (mExceptionObj) {        env-&gt;Throw(mExceptionObj);        env-&gt;DeleteLocalRef(mExceptionObj);        mExceptionObj = NULL;    }}</code></pre><p>此处通过持有对象 Native 层的 looper，调用到 pollOnce 方法，后面具体分析。</p><p>分析到这里，我们之是大概的分析了其内部调用逻辑，JNI 部分也只是衔接 Java 和 Native 部分，具体的逻辑还是在下面的 Native 层章节具体分析。</p><h1 id="Native-层三巨头"><a href="#Native-层三巨头" class="headerlink" title="Native 层三巨头"></a>Native 层三巨头</h1><p>Java 中存在的三巨头，Native 也同样存在，下面我们来具体分析。</p><h2 id="NativeMessageQueue"><a href="#NativeMessageQueue" class="headerlink" title="NativeMessageQueue"></a>NativeMessageQueue</h2><p>Java 层建议对象 MessageQueue 时，便和 Native 曾建立联系，NativeMessageQueue 声明在 android_os_MessageQueue.h 中，其依赖两个父类，如下：</p><pre><code>class MessageQueue : public virtual RefBase {public:    inline sp&lt;Looper&gt; getLooper() const {        return mLooper;    }    bool raiseAndClearException(JNIEnv* env, const char* msg);    virtual void raiseException(JNIEnv* env, const char* msg, jthrowable exceptionObj) = 0;protected:    MessageQueue();    virtual ~MessageQueue();protected:    sp&lt;Looper&gt; mLooper;};class LooperCallback : public virtual RefBase {protected:    virtual ~LooperCallback() { }public:    virtual int handleEvent(int fd, int events, void* data) = 0;};class NativeMessageQueue : public MessageQueue, public LooperCallback {public:    NativeMessageQueue();    virtual ~NativeMessageQueue();    virtual void raiseException(JNIEnv* env, const char* msg, jthrowable exceptionObj);    void pollOnce(JNIEnv* env, jobject obj, int timeoutMillis);    void wake();    void setFileDescriptorEvents(int fd, int events);    virtual int handleEvent(int fd, int events, void* data);private:    JNIEnv* mPollEnv;    jobject mPollObj;    jthrowable mExceptionObj;};</code></pre><p>上面可以看出，父类 MessageQueue 主要提供 raiseAndClearException 和<br> raiseException 方法，同时持有成员变量 mLooper 轮训器。父类 LooperCallback 则更简单，提供一个共有接口 handleEvent。</p><h3 id="NativeMessageQueue-构造函数"><a href="#NativeMessageQueue-构造函数" class="headerlink" title="NativeMessageQueue 构造函数"></a>NativeMessageQueue 构造函数</h3><p>NativeMessageQueue 的构造函数如下：</p><pre><code>NativeMessageQueue::NativeMessageQueue() :        mPollEnv(NULL), mPollObj(NULL), mExceptionObj(NULL) {    mLooper = Looper::getForThread();    if (mLooper == NULL) {        mLooper = new Looper(false);        Looper::setForThread(mLooper);    }}</code></pre><p>创建 Native 层的 Looper 对象，设置 Looper 线程的 looper 对象。</p><h3 id="wake"><a href="#wake" class="headerlink" title="wake"></a>wake</h3><p>NativeMessageQueue 的 wake 函数相对简单，代码如下：</p><pre><code>void NativeMessageQueue::wake() {    mLooper-&gt;wake();}</code></pre><h3 id="pollOnce"><a href="#pollOnce" class="headerlink" title="pollOnce"></a>pollOnce</h3><p>NativeMessageQueue 的 pollOnce 实现对消息队列的轮训检查，具体代码如下：</p><pre><code>void NativeMessageQueue::pollOnce(JNIEnv* env, jobject pollObj, int timeoutMillis) {    mPollEnv = env;    mPollObj = pollObj;    // 调用持有对象 looper 的 pollOnce    mLooper-&gt;pollOnce(timeoutMillis);    mPollObj = NULL;    mPollEnv = NULL;    if (mExceptionObj) {        env-&gt;Throw(mExceptionObj);        env-&gt;DeleteLocalRef(mExceptionObj);        mExceptionObj = NULL;    }}</code></pre><p>最终实现方法还是依赖 Looper 的 pollOnce，看来 Native 层的 Looper 才是最终要干活的人啊！</p><h2 id="Looper（Native）"><a href="#Looper（Native）" class="headerlink" title="Looper（Native）"></a>Looper（Native）</h2><p>上面分析，均已 Looper 截断，此处我们具体分析 Looper 的逻辑。</p><h3 id="Looper-的构造函数"><a href="#Looper-的构造函数" class="headerlink" title="Looper 的构造函数"></a>Looper 的构造函数</h3><p>Native 层的 Looper 类相对最复杂，我们首先分析其工作流程，剩下的其他部分后续再补充说明。代码如下：</p><pre><code>Looper::Looper(bool allowNonCallbacks) :        mAllowNonCallbacks(allowNonCallbacks), mSendingMessage(false),        mPolling(false), mEpollFd(-1), mEpollRebuildRequired(false),        mNextRequestSeq(0), mResponseIndex(0), mNextMessageUptime(LLONG_MAX) {    // 创建事件对象 mWakeEventFd，不阻塞方式    mWakeEventFd = eventfd(0, EFD_NONBLOCK);    LOG_ALWAYS_FATAL_IF(mWakeEventFd &lt; 0, &quot;Could not make wake event fd.  errno=%d&quot;, errno);    AutoMutex _l(mLock);    rebuildEpollLocked();}</code></pre><p>Looper 初始化将创建一个时间对象用于唤醒，eventfd 方式的具体解释，可以参考一下博文</p><p><a href="https://blog.csdn.net/tanswer_/article/details/79008322" target="_blank" rel="noopener">eventfd 事件通知</a></p><p>构造函数最后，调用 rebuildEpollLocked 重新构建 epoll 下文件描述符的监听任务，如下代码：</p><pre><code>void Looper::Request::initEventItem(struct epoll_event* eventItem) const {    int epollEvents = 0;    if (events &amp; EVENT_INPUT) epollEvents |= EPOLLIN;    if (events &amp; EVENT_OUTPUT) epollEvents |= EPOLLOUT;    memset(eventItem, 0, sizeof(epoll_event)); // zero out unused members of data field union    eventItem-&gt;events = epollEvents;    eventItem-&gt;data.fd = fd;}void Looper::rebuildEpollLocked() {    // Close old epoll instance if we have one.    if (mEpollFd &gt;= 0) {#if DEBUG_CALLBACKS        ALOGD(&quot;%p ~ rebuildEpollLocked - rebuilding epoll set&quot;, this);#endif        close(mEpollFd);    }    // Allocate the new epoll instance and register the wake pipe.    // 创建 epoll 监听文件描述符，监数目 8    mEpollFd = epoll_create(EPOLL_SIZE_HINT);    LOG_ALWAYS_FATAL_IF(mEpollFd &lt; 0, &quot;Could not create epoll instance.  errno=%d&quot;, errno);    struct epoll_event eventItem;    memset(&amp; eventItem, 0, sizeof(epoll_event)); // zero out unused members of data field union    eventItem.events = EPOLLIN;    eventItem.data.fd = mWakeEventFd;    // 将 mWakeEventFd 文件描述符加入监听列表中    int result = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, mWakeEventFd, &amp; eventItem);    LOG_ALWAYS_FATAL_IF(result != 0, &quot;Could not add wake event fd to epoll instance.  errno=%d&quot;,            errno);    // mRequests 为 KeyedVector&lt;int, Request&gt; 类型，通过 index 和 Request 进行绑定，后续进行结构体的具体分析    for (size_t i = 0; i &lt; mRequests.size(); i++) {        // 获取对应的 Request        const Request&amp; request = mRequests.valueAt(i);        struct epoll_event eventItem;        // 初始化 eventItem        request.initEventItem(&amp;eventItem);        // 加入监听中        int epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, request.fd, &amp; eventItem);        if (epollResult &lt; 0) {            ALOGE(&quot;Error adding epoll events for fd %d while rebuilding epoll set, errno=%d&quot;,                    request.fd, errno);        }    }}</code></pre><p>如此一来，Looper 便完成了相关文件描述符的监听，其中包括事件通知对象 mWakeEventFd。至于 mRequests 内部对象的添加，后续分析。</p><h3 id="wake-1"><a href="#wake-1" class="headerlink" title="wake"></a>wake</h3><p>Looper 的 wake 函数实际上是最终调用的函数，代码如下：</p><pre><code>void Looper::wake() {#if DEBUG_POLL_AND_WAKE    ALOGD(&quot;%p ~ wake&quot;, this);#endif    uint64_t inc = 1;    // 想文件描述符中写入数据，以便唤醒    ssize_t nWrite = TEMP_FAILURE_RETRY(write(mWakeEventFd, &amp;inc, sizeof(uint64_t)));    if (nWrite != sizeof(uint64_t)) {        if (errno != EAGAIN) {            ALOGW(&quot;Could not write wake signal, errno=%d&quot;, errno);        }    }}</code></pre><p>wake 函数通过向 mWakeEventFd 写入数据，此时发生了数据写入操作，在 pollOnce 中监听便得到响应。</p><h3 id="pollOnce-1"><a href="#pollOnce-1" class="headerlink" title="pollOnce"></a>pollOnce</h3><p>Java 层的 Looper 在无限循环中，通过 next 获取消息并处理，优先处理 Native 的消息，机调用 pollOnce 走到次数，代码如下：</p><pre><code>int Looper::pollOnce(int timeoutMillis, int* outFd, int* outEvents, void** outData) {    int result = 0;    // 无限循环    for (;;) {        // 优先处理 response，此处的 mResponses 为一个 Vector&lt;Response&gt; 类型        while (mResponseIndex &lt; mResponses.size()) {            // 取出对应的 response            const Response&amp; response = mResponses.itemAt(mResponseIndex++);            int ident = response.request.ident;            if (ident &gt;= 0) {                int fd = response.request.fd;                int events = response.events;                void* data = response.request.data;#if DEBUG_POLL_AND_WAKE                ALOGD(&quot;%p ~ pollOnce - returning signalled identifier %d: &quot;                        &quot;fd=%d, events=0x%x, data=%p&quot;,                        this, ident, fd, events, data);#endif                // 赋值                if (outFd != NULL) *outFd = fd;                if (outEvents != NULL) *outEvents = events;                if (outData != NULL) *outData = data;                // 直接返回 ident，此时的 ident 为大于 0 的值                return ident;            }        }        // 一次轮训结束后，从下面返回的 result        if (result != 0) {#if DEBUG_POLL_AND_WAKE            ALOGD(&quot;%p ~ pollOnce - returning result %d&quot;, this, result);#endif            if (outFd != NULL) *outFd = 0;            if (outEvents != NULL) *outEvents = 0;            if (outData != NULL) *outData = NULL;            return result;        }        // 调用 pollInner 再次处理，依然在无限循环内部        result = pollInner(timeoutMillis);    }}</code></pre><p>代码逻辑中，优先处理 mResponses 中对应的 Response，存在赋值并直接返回 ident，否则调用 pollInner 进入下层逻辑。代码如下：</p><pre><code>int Looper::pollInner(int timeoutMillis) {#if DEBUG_POLL_AND_WAKE    ALOGD(&quot;%p ~ pollOnce - waiting: timeoutMillis=%d&quot;, this, timeoutMillis);#endif    // Adjust the timeout based on when the next message is due.    if (timeoutMillis != 0 &amp;&amp; mNextMessageUptime != LLONG_MAX) {        nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC);        int messageTimeoutMillis = toMillisecondTimeoutDelay(now, mNextMessageUptime);        if (messageTimeoutMillis &gt;= 0                &amp;&amp; (timeoutMillis &lt; 0 || messageTimeoutMillis &lt; timeoutMillis)) {            timeoutMillis = messageTimeoutMillis;        }#if DEBUG_POLL_AND_WAKE        ALOGD(&quot;%p ~ pollOnce - next message in %&quot; PRId64 &quot;ns, adjusted timeout: timeoutMillis=%d&quot;,                this, mNextMessageUptime - now, timeoutMillis);#endif    }    // Poll.    int result = POLL_WAKE;    // mResponses 处理完成，清理工作     mResponses.clear();    mResponseIndex = 0;    // We are about to idle.    mPolling = true;    // epoll_event 结构体，容纳 16 个文件描述符事件单位的数组    struct epoll_event eventItems[EPOLL_MAX_EVENTS];    // epoll_wait 等待在 mEpollFd 文件描述符上发生事件    int eventCount = epoll_wait(mEpollFd, eventItems, EPOLL_MAX_EVENTS, timeoutMillis);    // No longer idling.    mPolling = false;    // Acquire lock.    mLock.lock();    // Rebuild epoll set if needed.    // 是否有需要重新构建 epoll 监听列表    if (mEpollRebuildRequired) {        mEpollRebuildRequired = false;        rebuildEpollLocked();        goto Done;    }    // Check for poll error.    // epoll 监听发生错误    if (eventCount &lt; 0) {        if (errno == EINTR) {            goto Done;        }        ALOGW(&quot;Poll failed with an unexpected error, errno=%d&quot;, errno);        result = POLL_ERROR;        goto Done;    }    // Check for poll timeout.    // epoll 监听超时    if (eventCount == 0) {#if DEBUG_POLL_AND_WAKE        ALOGD(&quot;%p ~ pollOnce - timeout&quot;, this);#endif        result = POLL_TIMEOUT;        goto Done;    }    // Handle all events.#if DEBUG_POLL_AND_WAKE    ALOGD(&quot;%p ~ pollOnce - handling events from %d fds&quot;, this, eventCount);#endif    // 处理文件节点发生的事件，eventCount 为监听文件描述符发生事件的个数    for (int i = 0; i &lt; eventCount; i++) {        // 获取发生事件的具体文件描述符        int fd = eventItems[i].data.fd;        // 获取对应发生事件类型        uint32_t epollEvents = eventItems[i].events;        // 唤醒操作，即客户端调用 wake 向 mWakeEventFd 写入数据，追溯上层 Java 层可以调用 nativeWake 引起        if (fd == mWakeEventFd) {            if (epollEvents &amp; EPOLLIN) {                // 唤醒操作                awoken();            } else {                ALOGW(&quot;Ignoring unexpected epoll events 0x%x on wake event fd.&quot;, epollEvents);            }        } else {            // 其他文件描述符使用了 &lt;index，Request&gt; 来存储，这里从 mRequests 中获取获取 index，返回值 requestIndex 大于 0 即为获取成功，存在            ssize_t requestIndex = mRequests.indexOfKey(fd);            if (requestIndex &gt;= 0) {                int events = 0;                if (epollEvents &amp; EPOLLIN) events |= EVENT_INPUT;                if (epollEvents &amp; EPOLLOUT) events |= EVENT_OUTPUT;                if (epollEvents &amp; EPOLLERR) events |= EVENT_ERROR;                if (epollEvents &amp; EPOLLHUP) events |= EVENT_HANGUP;                // 调用 pushResponse 继续处理，从 mRequests 中 &lt;int，Request&gt; 获取fd对应的 Request，并添加到 mResponses 中                pushResponse(events, mRequests.valueAt(requestIndex));            } else {                ALOGW(&quot;Ignoring unexpected epoll events 0x%x on fd %d that is &quot;                        &quot;no longer registered.&quot;, epollEvents, fd);            }        }    }Done: ;    // Invoke pending message callbacks.    mNextMessageUptime = LLONG_MAX;    while (mMessageEnvelopes.size() != 0) {        nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC);        const MessageEnvelope&amp; messageEnvelope = mMessageEnvelopes.itemAt(0);        if (messageEnvelope.uptime &lt;= now) {            { // obtain handler                sp&lt;MessageHandler&gt; handler = messageEnvelope.handler;                Message message = messageEnvelope.message;                mMessageEnvelopes.removeAt(0);                mSendingMessage = true;                mLock.unlock();#if DEBUG_POLL_AND_WAKE || DEBUG_CALLBACKS                ALOGD(&quot;%p ~ pollOnce - sending message: handler=%p, what=%d&quot;,                        this, handler.get(), message.what);#endif                // 调用对应的 msg 的 handleMessage 方法，具体分析见后面                handler-&gt;handleMessage(message);            } // release handler            mLock.lock();            mSendingMessage = false;            result = POLL_CALLBACK;        } else {            // The last message left at the head of the queue determines the next wakeup time.            mNextMessageUptime = messageEnvelope.uptime;            break;        }    }    // Release lock.    mLock.unlock();    // Invoke all response callbacks.    // 处理 mResponses 中具体的 response    for (size_t i = 0; i &lt; mResponses.size(); i++) {        Response&amp; response = mResponses.editItemAt(i);        // response 中元素 request.ident 为 POLL_CALLBACK，即含有回调函数        if (response.request.ident == POLL_CALLBACK) {            int fd = response.request.fd;            int events = response.events;            void* data = response.request.data;#if DEBUG_POLL_AND_WAKE || DEBUG_CALLBACKS            ALOGD(&quot;%p ~ pollOnce - invoking fd event callback %p: fd=%d, events=0x%x, data=%p&quot;,                    this, response.request.callback.get(), fd, events, data);#endif            // 回调 callback 的 handleEvent 函数            int callbackResult = response.request.callback-&gt;handleEvent(fd, events, data);            if (callbackResult == 0) {                // 成功，移除工作                removeFd(fd, response.request.seq);            }            response.request.callback.clear();            result = POLL_CALLBACK;        }    }    return result;}void Looper::pushResponse(int events, const Request&amp; request) {    Response response;    response.events = events;    response.request = request;    // 再次使用 mResponses 进行处理，调用 push 后，mResponses 中便添加了新元素 response    mResponses.push(response);}</code></pre><p>以上代码众多，主要涉及到了 epoll 机制，如果读者对 epoll 不了解的话，可能需要重新学习一下 epoll 机制了，这里不做过多说明了，有需要请参考之前的博文，这里对上述代码做一个总结：</p><ol><li>pollOnce 中主要对 mResponses 信息进行处理，如果 response.request.ident 大于0，就取出信息直接返回，否则再调用 pollInner 继续处理</li><li>pollInner 中首先针对过期的消息设置超时时间，然后再清理 Responses 数据，因为之前已经处理了 Responses 中的数据，后续要对发生事件的文件描述符进行统计，再次集中记录到 Responses 处理</li><li>接下来便是基本的 epoll 模型，epoll_wait 监听文件描述符事件发生</li><li>分别处理 epoll 监听错误，监听超时等错误</li><li>接下来处理文件节点发生的事件，eventCount 返回发生事件文件描述符个数</li><li>如果 fd 为 mWakeEventFd，即进行唤醒操作，执行 awoken</li><li>对于其他文件描述符，Looper 使用了 &lt;int，Request&gt; 来存储，这里从 mRequests 中获取获取 index，返回值 requestIndex 大于 0 即为获取成功，之后调用 pushResponse 集中添加到 mResponses 中</li><li>遍历完成后，便开始对消息进行处理，优先处理 mMessageEnvelopes 中的消息回调</li><li>再处理 mResponses 中具体的 response，这里的是之前文件描述符发生事件写入的集合列表</li><li>最后执行相应的清理任务并返回执行结果</li></ol><p>上述代码中，需要读者对 epoll 机制有一定的了解，否则真的难以看懂，之前博主已经对 epoll 进行了相关的介绍，请自行查看，这里主要是处在两个数据集合，一个是 mResponses，另一个是 mMessageEnvelopes，下面我们具体分析一下这两个数据集合。</p><h3 id="mResponses-数据分析"><a href="#mResponses-数据分析" class="headerlink" title="mResponses 数据分析"></a>mResponses 数据分析</h3><p>Looper.h 的头文件中声明了 Vector<response> mResponses，其实一个 Vector 容器，类型为 Response，那么 Response 又是什么？</response></p><pre><code>struct Response {    int events;    Request request;};</code></pre><p>结构体 Response 内部有两个元素，events 即位事件，那么 Request 又是什么？</p><pre><code>struct Request {    int fd;    int ident;    int events;    int seq;    sp&lt;LooperCallback&gt; callback;    void* data;    void initEventItem(struct epoll_event* eventItem) const;};</code></pre><p>到这里就清晰明了了，Request 集中封装了事件发生的文件结构，例如文件描述符，事件类型，事件回调，以及额外数据，同时提供函数指针 initEventItem 用于 Looper 中实现快捷的将 int events 类型转换到 epoll 原型监听文件结构，如下：</p><pre><code>void Looper::Request::initEventItem(struct epoll_event* eventItem) const {    int epollEvents = 0;    // 根据 events 转换到 epoll 下的事件类型    if (events &amp; EVENT_INPUT) epollEvents |= EPOLLIN;    if (events &amp; EVENT_OUTPUT) epollEvents |= EPOLLOUT;    memset(eventItem, 0, sizeof(epoll_event));    // 赋值操作    eventItem-&gt;events = epollEvents;    eventItem-&gt;data.fd = fd;}</code></pre><p>由上可以看出，mResponses 中存放了 Vector<response> 类型的数据，其每一个元素对应一个需要监听的 epoll 文件描述符，其元素的 Request 节点的 指针函数 initEventItem 便于将数据结构快捷的转换到 epoll 所需的文件结构，实现 epoll 监听。</response></p><h4 id="数据添加"><a href="#数据添加" class="headerlink" title="数据添加"></a>数据添加</h4><p>到这里，数据结构分析完毕了，那么文件监听是如何添加进去的呢？Looper 中有一个函数如下：</p><pre><code>int Looper::addFd(int fd, int ident, int events, Looper_callbackFunc callback, void* data) {    return addFd(fd, ident, events, callback ? new SimpleLooperCallback(callback) : NULL, data);}</code></pre><p>SimpleLooperCallback 继承自 LooperCallback，如下：</p><pre><code>class SimpleLooperCallback : public LooperCallback {protected:    virtual ~SimpleLooperCallback();public:    SimpleLooperCallback(Looper_callbackFunc callback);    virtual int handleEvent(int fd, int events, void* data);private:    Looper_callbackFunc mCallback;};SimpleLooperCallback::SimpleLooperCallback(Looper_callbackFunc callback) :    mCallback(callback) {}int SimpleLooperCallback::handleEvent(int fd, int events, void* data{    return mCallback(fd, events, data);}</code></pre><p>子实现类 SimpleLooperCallback， 构造函数维持有 Looper_callbackFunc 对象，需要实现 handleEvent 方法，下面继续看 addFd 函数：</p><pre><code>int Looper::addFd(int fd, int ident, int events, const sp&lt;LooperCallback&gt;&amp; callback, void* data) {#if DEBUG_CALLBACKS    ALOGD(&quot;%p ~ addFd - fd=%d, ident=%d, events=0x%x, callback=%p, data=%p&quot;, this, fd, ident,            events, callback.get(), data);#endif    // 存在为空    if (!callback.get()) {        if (! mAllowNonCallbacks) {            ALOGE(&quot;Invalid attempt to set NULL callback but not allowed for this looper.&quot;);            return -1;        }        if (ident &lt; 0) {            ALOGE(&quot;Invalid attempt to set NULL callback with ident &lt; 0.&quot;);            return -1;        }    } else {        ident = POLL_CALLBACK;    }    { // acquire lock        AutoMutex _l(mLock);        // 使用 Request 结构体承载数据        Request request;        request.fd = fd;        request.ident = ident;        request.events = events;        request.seq = mNextRequestSeq++;        request.callback = callback;        request.data = data;        if (mNextRequestSeq == -1) mNextRequestSeq = 0; // reserve sequence number -1        struct epoll_event eventItem;        // 将 Request 转化成 epoll 需要的 eventItem 数据结构        request.initEventItem(&amp;eventItem);        ssize_t requestIndex = mRequests.indexOfKey(fd);        // 没有添加过，直接添加监听        if (requestIndex &lt; 0) {            int epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, fd, &amp; eventItem);            if (epollResult &lt; 0) {                ALOGE(&quot;Error adding epoll events for fd %d, errno=%d&quot;, fd, errno);                return -1;            }            // mRequests 为一个 KeyedVector&lt;int, Request&gt;，没有记录项，添加            mRequests.add(fd, request);        } else {            // 监听存在，修改属性            int epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_MOD, fd, &amp; eventItem);            // 添加出错            if (epollResult &lt; 0) {                if (errno == ENOENT) {#if DEBUG_CALLBACKS                    ALOGD(&quot;%p ~ addFd - EPOLL_CTL_MOD failed due to file descriptor &quot;                            &quot;being recycled, falling back on EPOLL_CTL_ADD, errno=%d&quot;,                            this, errno);#endif                    epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, fd, &amp; eventItem);                    if (epollResult &lt; 0) {                        ALOGE(&quot;Error modifying or adding epoll events for fd %d, errno=%d&quot;,                                fd, errno);                        return -1;                    }                    scheduleEpollRebuildLocked();                } else {                    ALOGE(&quot;Error modifying epoll events for fd %d, errno=%d&quot;, fd, errno);                    return -1;                }            }            // 替换对应元素            mRequests.replaceValueAt(requestIndex, request);        }    } // release lock    return 1;}</code></pre><p>Native 层提供的 addfd 函数，Java 层通过函数 nativeSetFileDescriptorEvents 连接到 Native，JNI 接口函数如下：</p><pre><code>static void android_os_MessageQueue_nativeSetFileDescriptorEvents(JNIEnv* env, jclass clazz,        jlong ptr, jint fd, jint events) {    NativeMessageQueue* nativeMessageQueue = reinterpret_cast&lt;NativeMessageQueue*&gt;(ptr);    nativeMessageQueue-&gt;setFileDescriptorEvents(fd, events);}</code></pre><p>至于 Java 层添加等详细流程，后续的分析这里就不展开了。</p><h4 id="数据删除"><a href="#数据删除" class="headerlink" title="数据删除"></a>数据删除</h4><p>既然有了添加，那么肯定有移除操作了，如下；</p><pre><code>int Looper::removeFd(int fd, int seq) {#if DEBUG_CALLBACKS    ALOGD(&quot;%p ~ removeFd - fd=%d, seq=%d&quot;, this, fd, seq);#endif    { // acquire lock        AutoMutex _l(mLock);        // 从 mRequests 集合中获取 fd 对应的 index        ssize_t requestIndex = mRequests.indexOfKey(fd);        if (requestIndex &lt; 0) {            return 0;        }        if (seq != -1 &amp;&amp; mRequests.valueAt(requestIndex).seq != seq) {#if DEBUG_CALLBACKS            ALOGD(&quot;%p ~ removeFd - sequence number mismatch, oldSeq=%d&quot;,                    this, mRequests.valueAt(requestIndex).seq);#endif            return 0;        }        // 移除        mRequests.removeItemsAt(requestIndex);        // 将 fd 从 epoll 监听节点 mEpollFd 中删除        int epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_DEL, fd, NULL);        if (epollResult &lt; 0) {            if (seq != -1 &amp;&amp; (errno == EBADF || errno == ENOENT)) {#if DEBUG_CALLBACKS                ALOGD(&quot;%p ~ removeFd - EPOLL_CTL_DEL failed due to file descriptor &quot;                        &quot;being closed, errno=%d&quot;, this, errno);#endif                scheduleEpollRebuildLocked();            } else {                ALOGE(&quot;Error removing epoll events for fd %d, errno=%d&quot;, fd, errno);                scheduleEpollRebuildLocked();                return -1;            }        }    } // release lock    return 1;}</code></pre><p>同样，Java 层也可以通过 JNI 进行回调。</p><h4 id="数据回调"><a href="#数据回调" class="headerlink" title="数据回调"></a>数据回调</h4><p>mResponses 中的数据发生改变是如何回调的呢，之前我们在函数 pollInner 中，有如下代码</p><pre><code>for (size_t i = 0; i &lt; mResponses.size(); i++) {    Response&amp; response = mResponses.editItemAt(i);    // response 中元素 request.ident 为 POLL_CALLBACK，即含有回调函数    if (response.request.ident == POLL_CALLBACK) {        int fd = response.request.fd;        int events = response.events;        void* data = response.request.data;        // 回调 callback 的 handleEvent 函数        int callbackResult = response.request.callback-&gt;handleEvent(fd, events, data);        if (callbackResult == 0) {            // 成功，移除工作            removeFd(fd, response.request.seq);        }        response.request.callback.clear();        result = POLL_CALLBACK;    }}</code></pre><p>具体对应的函数如下：</p><pre><code>int NativeMessageQueue::handleEvent(int fd, int looperEvents, void* data) {    int events = 0;    // 事件类型转换    if (looperEvents &amp; Looper::EVENT_INPUT) {        events |= CALLBACK_EVENT_INPUT;    }    if (looperEvents &amp; Looper::EVENT_OUTPUT) {        events |= CALLBACK_EVENT_OUTPUT;    }    if (looperEvents &amp; (Looper::EVENT_ERROR | Looper::EVENT_HANGUP | Looper::EVENT_INVALID)) {        events |= CALLBACK_EVENT_ERROR;    }    int oldWatchedEvents = reinterpret_cast&lt;intptr_t&gt;(data);    // 反过来回调 Java 层的 dispatchEvents 方法    int newWatchedEvents = mPollEnv-&gt;CallIntMethod(mPollObj,            gMessageQueueClassInfo.dispatchEvents, fd, events);    if (!newWatchedEvents) {        return 0; // unregister the fd    }    if (newWatchedEvents != oldWatchedEvents) {        // 更改 Native 层的 fd        setFileDescriptorEvents(fd, newWatchedEvents);    }    return 1;}void NativeMessageQueue::setFileDescriptorEvents(int fd, int events) {    if (events) {        int looperEvents = 0;        if (events &amp; CALLBACK_EVENT_INPUT) {            looperEvents |= Looper::EVENT_INPUT;        }        if (events &amp; CALLBACK_EVENT_OUTPUT) {            looperEvents |= Looper::EVENT_OUTPUT;        }        mLooper-&gt;addFd(fd, Looper::POLL_CALLBACK, looperEvents, this,                reinterpret_cast&lt;void*&gt;(events));    } else {        mLooper-&gt;removeFd(fd);    }}</code></pre><p>函数中 mPollEnv-&gt;CallIntMethod(mPollObj,gMessageQueueClassInfo.dispatchEvents, fd, events) 中指明了函数的出处，此处通过 JNI 指向了 Java 层接口回调，如下：</p><pre><code>private int dispatchEvents(int fd, int events) {    // Get the file descriptor record and any state that might change.    final FileDescriptorRecord record;    final int oldWatchedEvents;    final OnFileDescriptorEventListener listener;    final int seq;    synchronized (this) {        record = mFileDescriptorRecords.get(fd);        if (record == null) {            return 0; // spurious, no listener registered        }        oldWatchedEvents = record.mEvents;        events &amp;= oldWatchedEvents; // filter events based on current watched set        if (events == 0) {            return oldWatchedEvents; // spurious, watched events changed        }        listener = record.mListener;        seq = record.mSeq;    }    // Invoke the listener outside of the lock.    // 通过 listener 监听接口，回调函数 onFileDescriptorEvents    int newWatchedEvents = listener.onFileDescriptorEvents(            record.mDescriptor, events);    if (newWatchedEvents != 0) {        newWatchedEvents |= OnFileDescriptorEventListener.EVENT_ERROR;    }    if (newWatchedEvents != oldWatchedEvents) {        synchronized (this) {            int index = mFileDescriptorRecords.indexOfKey(fd);            if (index &gt;= 0 &amp;&amp; mFileDescriptorRecords.valueAt(index) == record                    &amp;&amp; record.mSeq == seq) {                record.mEvents = newWatchedEvents;                if (newWatchedEvents == 0) {                    mFileDescriptorRecords.removeAt(index);                }            }        }    }    return newWatchedEvents;}</code></pre><p>至此，基本分析完了 mResponses 中的数据结构，包括数据的添加，回调以及删除，下面我们来看 mMessageEnvelopes 数据结构。</p><h3 id="mMessageEnvelopes-数据分析"><a href="#mMessageEnvelopes-数据分析" class="headerlink" title="mMessageEnvelopes 数据分析"></a>mMessageEnvelopes 数据分析</h3><p>首先来看其声明部分，如下：</p><pre><code>Vector&lt;MessageEnvelope&gt; mMessageEnvelopes; // 承载消息队列的信封struct MessageEnvelope {    MessageEnvelope() : uptime(0) { }    MessageEnvelope(nsecs_t uptime, const sp&lt;MessageHandler&gt; handler,            const Message&amp; message) : uptime(uptime), handler(handler), message(message) {    }    nsecs_t uptime;    sp&lt;MessageHandler&gt; handler;    Message message;};</code></pre><p>mMessageEnvelopes 为元素为 MessageEnvelope 类型的 Vector 容器，容器内每一个元素都代表着一个 消息 msg，MessageEnvelope 通过 sp 强引用与 handler 建立链接，每封信封封装了消息的信息，事件，handler接受人等。</p><h4 id="MessageEnvelope-的发送"><a href="#MessageEnvelope-的发送" class="headerlink" title="MessageEnvelope 的发送"></a>MessageEnvelope 的发送</h4><p>Native 层通过 send*Message 接口发送消息，最终走向 sendMessageAtTime 如下代码：</p><pre><code>void Looper::sendMessageAtTime(nsecs_t uptime, const sp&lt;MessageHandler&gt;&amp; handler,        const Message&amp; message) {#if DEBUG_CALLBACKS    ALOGD(&quot;%p ~ sendMessageAtTime - uptime=%&quot; PRId64 &quot;, handler=%p, what=%d&quot;,            this, uptime, handler.get(), message.what);#endif    size_t i = 0;    { // acquire lock        AutoMutex _l(mLock);        // 获取 mMessageEnvelopes 的大小        size_t messageCount = mMessageEnvelopes.size();        // 根据时间，找到合适位置进行插入        while (i &lt; messageCount &amp;&amp; uptime &gt;= mMessageEnvelopes.itemAt(i).uptime) {            i += 1;        }        // 新建信封单位 messageEnvelope 并加入到 mMessageEnvelopes        MessageEnvelope messageEnvelope(uptime, handler, message);        mMessageEnvelopes.insertAt(messageEnvelope, i, 1);        if (mSendingMessage) {            return;        }    } // release lock    // 信封没有信件，进入 wake 状态    if (i == 0) {        wake();    }}</code></pre><p>代码注释很详细，下面来看信封中信息的删除。</p><h4 id="MessageEnvelope-的删除"><a href="#MessageEnvelope-的删除" class="headerlink" title="MessageEnvelope 的删除"></a>MessageEnvelope 的删除</h4><p>Looper 中的函数 removeMessages 有多个重载，逻辑差不太多，我们直接看其中一个的一个，根据 handler 与 what 类型进行删除</p><pre><code>void Looper::removeMessages(const sp&lt;MessageHandler&gt;&amp; handler, int what) {#if DEBUG_CALLBACKS    ALOGD(&quot;%p ~ removeMessages - handler=%p, what=%d&quot;, this, handler.get(), what);#endif    { // acquire lock        AutoMutex _l(mLock);        // 遍历 mMessageEnvelopes 数据结构，找到对应的并删除        for (size_t i = mMessageEnvelopes.size(); i != 0; ) {            const MessageEnvelope&amp; messageEnvelope = mMessageEnvelopes.itemAt(--i);            if (messageEnvelope.handler == handler                    &amp;&amp; messageEnvelope.message.what == what) {                mMessageEnvelopes.removeAt(i);            }        }    } // release lock}</code></pre><h4 id="MessageEnvelope-的回调"><a href="#MessageEnvelope-的回调" class="headerlink" title="MessageEnvelope 的回调"></a>MessageEnvelope 的回调</h4><p>对于 MessageEnvelope 的添加和删除操作，同样，在函数 pollOnce 的子函数 pollInner 中，有如下代码实现了信息的回调：</p><pre><code>while (mMessageEnvelopes.size() != 0) {    nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC);    const MessageEnvelope&amp; messageEnvelope = mMessageEnvelopes.itemAt(0);    if (messageEnvelope.uptime &lt;= now) {        { // obtain handler            sp&lt;MessageHandler&gt; handler = messageEnvelope.handler;            Message message = messageEnvelope.message;            mMessageEnvelopes.removeAt(0);            mSendingMessage = true;            mLock.unlock();            // 调用对应的 msg 的 handleMessage 方法            handler-&gt;handleMessage(message);        } // release handler        mLock.lock();        mSendingMessage = false;        result = POLL_CALLBACK;    } else {        mNextMessageUptime = messageEnvelope.uptime;        break;    }}</code></pre><p>通览 pollOnce 函数，可以看出，MessageEnvelope 的信息处理优先级高于 Response 类型的 mResponses 回调。对于 MessageHandler 到下一节中介绍。</p><h2 id="MessageHandler"><a href="#MessageHandler" class="headerlink" title="MessageHandler"></a>MessageHandler</h2><p>Native 层的 Handler 对应于 MessageHandler，其在 Looper 中声明如下：</p><pre><code>class MessageHandler : public virtual RefBase {protected:    virtual ~MessageHandler() { }public:    /**    * Handles a message.    */    virtual void handleMessage(const Message&amp; message) = 0;};</code></pre><p>基类 MessageHandler 只声明了 handleMessage 接口，而实际上实现的确实子类 WeakMessageHandler，如下：</p><pre><code>class WeakMessageHandler : public MessageHandler {protected:    virtual ~WeakMessageHandler();public:    WeakMessageHandler(const wp&lt;MessageHandler&gt;&amp; handler);    virtual void handleMessage(const Message&amp; message);private:    wp&lt;MessageHandler&gt; mHandler;};WeakMessageHandler::WeakMessageHandler(const wp&lt;MessageHandler&gt;&amp; handler) :        mHandler(handler) {}WeakMessageHandler::~WeakMessageHandler() {}</code></pre><p>子类 WeakMessageHandler 通过弱引用方式维持了 MessageHandler 类型的 handler 对象，其 handleMessage 方法如下：</p><pre><code>void WeakMessageHandler::handleMessage(const Message&amp; message) {    sp&lt;MessageHandler&gt; handler = mHandler.promote();    if (handler != NULL) {        handler-&gt;handleMessage(message);    }}</code></pre><p>代码中并未直接处理 handleMessage，因为弱饮用的关系，可能饮用对象不存在，需要尝试升级到强引用 mHandler.promote，然后再处理 handleMessage。关于智能指针的引用计数，有需要请参考之前的博文。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过以上内容的介绍，相信读者对安卓中的 Handler 通信机制有更深的了解。本文通过源码分析的方式，从 Java 到 JNI 到 Native，分别分析了 Handler、Looper、MessageQueue三者的关系，网上很多博主对其进行了总结，感觉也十分出众，这里本人也画了一幅小图来总结一下其之间的关系，如下：</p><img src="/FuckCode/2019/04/09/源码分析之Handler、Looper及MessageQueue之间关系/Handler.png" class="Handler模型"><p>在分析 MessageQueue 的过程中，本人使用 Xmind 思维导图将函数之间的调用关系进行了相关的汇总，图片如下：</p><img src="/FuckCode/2019/04/09/源码分析之Handler、Looper及MessageQueue之间关系/MessageQueue.png" class="MessageQueue关系思维导图"><p>文章内容涉及 epoll 模型，强烈推荐读者了解一下 select，poll，epoll等高并发模型，在后续代码分析中将会如鱼得水。</p><p><a href="https://891904833.gitee.io/fuckingcode/2019/01/23/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%A4%9A%E8%B7%AFIO%E8%BD%AC%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8/#more">高并发模型推荐</a></p><p>文件从邓凡平前辈的相关博文中汲取灵感，推荐博文如下：</p><p><a href="https://blog.csdn.net/Innost/article/details/47252865" target="_blank" rel="noopener">参考链接</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;在 Android 的应用开发中，线程之间的通信机制主要是以 Handler 机制为主。我们都知道 Android 中的 UI 更新只能在主线程中执行，通常的做法是，我们在 Activity 中直接 new Handler，通过 handler 发送消息，在 handlerMessage 处理即可。本篇博文就根据源码来具体分析其中工作机制以及源码架构。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="源码分析" scheme="https://891904833.gitee.io/FuckCode/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Handler" scheme="https://891904833.gitee.io/FuckCode/tags/Handler/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之多路IO转接服务器</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/23/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%A4%9A%E8%B7%AFIO%E8%BD%AC%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/23/Linux系统编程之多路IO转接服务器/</id>
    <published>2019-01-23T02:41:30.000Z</published>
    <updated>2019-01-23T10:39:45.567Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本章节主要介绍Linux中高并发服务器下的三种多路IO转接模型：select，poll，epoll。各模型分别从原理，系统函数，实现代码三个方面一一说明。文章内容稍有深度，代码理解不易，需要读者结合代码注释以及参考链接内容，反复比较思考。文章最后，对几种多路IO模型的优缺点进行总结，算是对此章节做的收尾工作。</strong><br><a id="more"></a></p><h1 id="多路IO转接服务器"><a href="#多路IO转接服务器" class="headerlink" title="多路IO转接服务器"></a>多路IO转接服务器</h1><p>IO多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程。IO多路复用适用如下场合：</p><ol><li>当客户处理多个描述字时（一般是交互式输入和网络套接口），必须使用I/O复用</li><li>当一个客户同时处理多个套接口时，而这种情况是可能的，但很少出现</li><li>如果一个TCP服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到I/O复用</li><li>如果一个服务器即要处理TCP，又要处理UDP，一般要使用I/O复用</li><li>如果一个服务器要处理多个服务或多个协议，一般要使用I/O复用</li></ol><p>与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。</p><h2 id="select服务器模型"><a href="#select服务器模型" class="headerlink" title="select服务器模型"></a>select服务器模型</h2><p>select函数能够实现多路IO复用机制，通过select系统调用，让我们的程序监视多个文件句柄的状态变化的，其使用一个 fd_set 来监测数据是否到达，状态是否改变，提供三种数据：读数据，写数据，异常数据。同时，select还可以指定文件阻塞方式和超时等待，非阻塞直接返回，阻塞式直到被监视的文件句柄有一个或多个发生了状态改变才会返回，返回值为变化的文件描述符个数。</p><p>使用 select 需要注意以下两点：</p><ol><li>select能监听的文件描述符个数受限于FD_SETSIZE,一般为1024,单纯改变进程打开的文件描述符个数并不能改变select监听文件个数</li><li>解决1024以下客户端时使用select是很合适的,但如果链接客户端过多,select采用的是轮询模型,会大大降低服务器响应效率,不应在select上投入更多精力</li></ol><h3 id="函数原型"><a href="#函数原型" class="headerlink" title="函数原型"></a>函数原型</h3><pre><code>#include &lt;sys/select.h&gt;#include &lt;sys/time.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);nfds: 监控的文件描述符集里最大文件描述符加1,因为此参数会告诉内核检测前多少个文件描述符的状态readfds: 监控有读数据到达文件描述符集合,传入传出参数writefds: 监控写数据到达文件描述符集合,传入传出参数exceptfds: 监控异常发生达文件描述符集合,如带外数据到达异常,传入传出参数timeout: 定时阻塞监控时间,3种情况  1. NULL,永远等下去  2. 设置timeval,等待固定时间  3. 设置timeval里时间均为0,检查描述字后立即返回,轮询返回值：  1. -1，执行错误  2. 0，timeout时间到达  3. 其他，正确执行，并且有就绪事件到达。结构体 timevalstruct timeval {  long tv_sec; /* seconds */  long tv_usec; /* microseconds */};文件描述符集 fd_set 操作函数void FD_CLR(int fd, fd_set *set); 把文件描述符集合里fd清 0int FD_ISSET(int fd, fd_set *set); 测试文件描述符集合里fd是否置 1void FD_SET(int fd, fd_set *set); 把文件描述符集合里fd位置 1void FD_ZERO(fd_set *set); 把文件描述符集合里所有位清 0</code></pre><p>select 函数参数众多，从函数原型上看得到所以然，下面展示模板代码，理解起来稍微不易，需要认真思考分析。</p><h3 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h3><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;sys/select.h&gt;#include &quot;wrap.h&quot;#define SERVER_PORT 8000#define BUF_SIZE 1024int main(int argc ,char *argv[]){    struct sockaddr_in serveraddr, clientaddr;    socklen_t clientlen;    char buf[BUF_SIZE];    // INET_ADDRSTRLEN 宏，ipv4地址大小    char clientip[INET_ADDRSTRLEN];    int readlen, sockfd, connfd;    int maxfd, maxi,i;    int selectfd;    // FD_SETSIZE 大小为 1024    int clientset[FD_SETSIZE];    // select 中需要的文件描述符集    fd_set curset, oriset;    sockfd = Socket(AF_INET,SOCK_STREAM,0);    bzero(&amp;serveraddr,sizeof (serveraddr));    serveraddr.sin_family = AF_INET;    serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);    serveraddr.sin_port = htons(SERVER_PORT);    Bind(sockfd,(struct sockaddr *)&amp;serveraddr,sizeof (serveraddr));    Listen(sockfd,20);    maxi = -1;    maxfd = sockfd;    // 初始化 clientset    for(i=0; i &lt; FD_SETSIZE; i++){        clientset[i] = -1;    }    // 清空 oriset，将 sockfd 加入监听    FD_ZERO(&amp;oriset);    FD_SET(sockfd,&amp;oriset);    printf(&quot;Select... \n&quot;);    //　循环监测socket文件描述符    for(;;){        curset = oriset;        // 使用select监测scoket文件，监测读断，返回链接服务器的客户端个数        // 由于参数位 &amp;curset 是传入传出参数，每次调用 select 都会发生改变。这里采用临时备份值记录        // 监听最大文件描述符 sockfd+1，只关心读数据到达，不关心写数据到达，异常数据到达，阻塞式        selectfd = select(maxfd+1,&amp;curset,NULL,NULL,NULL);        // 有数据到达，做处理判断，返回已到达的链接数        if(selectfd &lt; 0)            perr_exit(&quot;select&quot;);        //　检查读文件描述符集set中sockfd是否被置位，即是否有新客户端链接上，处理新客户端链接上操作        if(FD_ISSET(sockfd,&amp;curset)){            // sockfd 文件描述符有读到达，即有客户端链接请求            clientlen = sizeof (clientaddr);            // 调用Accept，内核分配文件描述符进行数据交互            connfd = Accept(sockfd,(struct sockaddr *)&amp;clientaddr,&amp;clientlen);            printf(&quot;Clinet IP: %s, port: %d \n&quot;,                   inet_ntop(AF_INET,&amp;clientaddr.sin_addr,clientip,INET_ADDRSTRLEN),                   ntohs(clientaddr.sin_port)                   );            //　将产生的数据交互文件描述符添加到数据集合中，等待下一轮轮训统一处理            for(i = 0; i&lt; FD_SETSIZE;i++){                if(clientset[i] &lt; 0){                    clientset[i] = connfd;                    break;                }            }            // 异常监测，链接数大于可承载范围1024            if(i == FD_SETSIZE){                fputs(&quot;too many clients!!!&quot;,stderr);                exit(1);            }            //　将分配的新文件描述符（客户端链接上，用于数据交互）添加到 select 中继续监听是否有数据到达            FD_SET(connfd,&amp;oriset);            // 更新 select 内最大文件描述符值（新加入一个）            if(connfd &gt; maxfd)                maxfd = connfd;            // 此处的 maxi 为监听文件描述符数组内的数量，用于后续统一进行数据交互之用            if(i &gt; maxi)                maxi = i;            // 这句代码需要重点理解，进入此模块，前提是有新客户端请求链接，客户端通信数据需要判断            // 1. select监听到 1个读数据到达，那么就是新客户端链接请求（上面处理完），直接返回开头重新监听新客户端链接请求和已连接上客户端的数据交互请求            // 2. select监听到 2个及以上数据到达，进入此模块肯定有新客户端链接请求（已对其分配描述符，可以进行数据交互），此时--select不为 0，继续向下处理已连接上的客户端数据交互请求            if( --selectfd == 0)                continue;        }        // select 监听到数据到达返回，但不是新的客户端链接上的请求，这里统一处理已连接上客户端的数据交互        // 遍历 clientset，统一处理客户端数据请求        for(i =0;i&lt;=maxi;i++){            int temp = clientset[i];            if(temp&lt;0)                continue;            // 找出数据到达的文件描述符，处理具体的客户端数据请求            if(FD_ISSET(temp,&amp;curset)){                if((readlen = Read(temp,buf,BUF_SIZE)) == 0){                    // client端关闭连接，服务端也将关闭                    // 关闭链接文件描述符                    Close(temp);                    // 修改set集合对应元素                    clientset[i] = -1;                    //　清除监听select中set元素                    FD_CLR(temp,&amp;oriset);                    maxfd--;                }else {                    // 服务器具体的数据回传处理                    for(int j=0;j&lt;readlen;j++)                        buf[j] = toupper(buf[j]);                    Write(temp,buf,readlen);                }                // 自减 select 返回的数据到达数量，避免不必要的遍历操作                if( --selectfd == 0)                    break;            }        }    }    Close(sockfd);    return 0;}</code></pre><p>上述代码有相关的注释说明，有以下几点要注意：</p><ol><li>select 返回数据改变的文件描述符个数，需要对其个数进行具体分析</li><li>select返回 1，判断是新客户端链接请求，Accept分配描述符进行数据读写，加入 select 监听，更新中间变量，加入 clientset 数据集监听数据交互请求，后续统一数据交互处理，–select后为 0 直接返回开头，select 重新监听</li><li>select返回大于 1，判断是否有新客户端链接请求，是则同上 1 处理后，–select后不为 0，进入第二层 for 循环，统一处理客户端的数据交互</li><li>select 返回 1或者大于 1，但不是新客户端链接请求，那么进入底层 for 循环，统一处理客户端的数据交互请求</li><li>底层for遍历循环中，遍历 clientset 数据集，通过 select 返回的 curset，找出具体文件描述符一一进行回复，处理完成一个， –select</li><li>当数据处理中发现有客户端断开（读数据长度为0），需要将 clientset 中对应为清除，同时 select 中对应的 fd_set 文件描述符集对应元素也要清除，</li><li>当–select后为 0时，代表 select 中监听到数据到达都已经处理完成了，循环结束，重新开始 select 的监听</li></ol><h3 id="小结-select-服务器模型"><a href="#小结-select-服务器模型" class="headerlink" title="小结 select 服务器模型"></a>小结 select 服务器模型</h3><p>select相比多进程多线程高效的原因</p><p>首先要知道一个概念，一次I/O分两个部分（①等待数据就绪 ②进行I/O），减少等的比重，增加I/O的比重就可以达到高效服务器的目的。select工作原理就是这个，同时监控多个文件描述符（或者说文件句柄），一旦其中某一个进入就绪状态，就进行I/O操作。监控多个文件句柄可以达到提高就绪状态出现的概率，就可以使CPU在大多数时间下都处于忙碌状态，大大提高CPU的性能。达到高效服务器的目的。 可以理解为select轮询监控多个文件句柄或套接字。</p><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol><li>不需要建立多个线程、进程就可以实现一对多的通信。</li><li>可以同时等待多个文件描述符，效率比起多进程多线程来说要高很多</li><li>跨平台性，windows、linux、micOS、unix、mips都支持 select</li></ol><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol><li>每次进行 select 都要把文件描述符集 fd 由用户态拷贝到内核态，这样的开销会很大</li><li>实现 select 服务器，内部要不断对文件描述符集 fd 进行循环遍历，当 fd 很多时，开销也很大</li><li>select 能监控文件描述符的数量有限，一般为1024。（sizeof（fd_set） * 8 = 1024（fd_set内部是以位图表示文件描述符））</li></ol><h4 id="pselect"><a href="#pselect" class="headerlink" title="pselect"></a>pselect</h4><p>pselect 相比 select 支持了信号屏蔽字的操作，函数原型如下</p><pre><code>#include &lt;sys/select.h&gt;int pselect(int nfds, fd_set *readfds, fd_set *writefds,fd_set *exceptfds, const struct timespec *timeout, const sigset_t *sigmask);struct timespec {  long tv_sec; /* seconds */  long tv_nsec; /* nanoseconds */};</code></pre><p>用sigmask替代当前进程的阻塞信号集,调用返回后还原原有阻塞信号集，具体代码略</p><p><a href="https://blog.csdn.net/qq_35116371/article/details/75020664" target="_blank" rel="noopener">参考博客1</a></p><p><a href="https://blog.csdn.net/weixin_40204595/article/details/83212900" target="_blank" rel="noopener">参考博客2</a></p><h2 id="poll-服务器模型"><a href="#poll-服务器模型" class="headerlink" title="poll 服务器模型"></a>poll 服务器模型</h2><p>poll 的机制与 select 类似，与 select 在本质上没有多大差别，管理多个描述符也是进行轮询，根据描述符的状态进行处理，但是 poll 没有最大文件描述符数量的限制。</p><h3 id="函数原型-1"><a href="#函数原型-1" class="headerlink" title="函数原型"></a>函数原型</h3><pre><code>#include &lt;poll.h&gt;int poll(struct pollfd *fds, nfds_t nfds, int timeout);struct pollfd {  int fd; /* 文件描述符 */  short events; /* 监控的事件 */  short revents; /* 监控事件中满足条件返回的事件 */};events取值：  POLLIN普通或带外优先数据可读,即POLLRDNORM | POLLRDBAND  POLLRDNORM-数据可读  POLLRDBAND-优先级带数据可读  POLLPRI 高优先级可读数据  POLLOUT普通或带外数据可写  POLLWRNORM-数据可写  POLLWRBAND-优先级带数据可写  POLLERR 发生错误  POLLHUP 发生挂起  POLLNVAL 描述字不是一个打开的文件nfds 监控数组中有多少文件描述符需要被监控timeout 毫秒级等待  -1: 阻塞等,#define INFTIM，-1 Linux中没有定义此宏  0: 立即返回,不阻塞进程  &gt;0: 等待指定毫秒数,如当前系统时间精度不够毫秒,向上取值返回值：  返回监控文件描述符状态改变的个数，等待超时返回 0，失败返回 -1，设置 errno</code></pre><p>函数相关说明：</p><ol><li>结构体 pollfd 指定了一个被监视的文件描述符，可以传递结构体数组，指示 poll 监视多个文件描述符。每个结构体的 events 域是监视该文件描述符的事件掩码，由用户来设置这个域。revents 域是文件描述符的操作结果事件掩码，内核在调用返回时设置这个域。events 域中请求的任何事件都可能在 revents 域中返回</li><li>POLLIN | POLLPRI 等价于 select() 的读事件，POLLOUT |POLLWRBAND 等价于 select() 的写事件。POLLIN 等价于 POLLRDNORM |POLLRDBAND，而 POLLOUT 则等价于 POLLWRNORM。例如，要同时监视一个文件描述符是否可读和可写，我们可以设置 events 为 POLLIN |POLLOUT。在 poll 返回时，我们可以检查 revents 中的标志，对应于文件描述符请求的 events 结构体。如果 POLLIN 事件被设置，则文件描述符可以被读取而不阻塞。如果 POLLOUT 被设置，则文件描述符可以写入而不导致阻塞。这些标志并不是互斥的：它们可能被同时设置，表示这个文件描述符的读取和写入操作都会正常返回而不阻塞。</li><li>成功时，poll() 返回结构体中 revents 域不为 0 的文件描述符个数；如果在超时前没有任何事件发生，poll() 返回 0；失败时，poll() 返回 -1，并设置 errno。</li></ol><h3 id="实例代码-1"><a href="#实例代码-1" class="headerlink" title="实例代码"></a>实例代码</h3><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;poll.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;netinet/in.h&gt;#include &lt;errno.h&gt;#include &quot;wrap.h&quot;#define BUF_SIZE 1024#define SERVER_PORT 8000#define FILE_MAX 1024int main(int argc,char  *argv[]){    int i,j, maxi, ready,readlen;    int sockfd, tempfd, connfd;    socklen_t clientlen;    struct sockaddr_in serveraddr, clientaddr;    struct pollfd clientset[FILE_MAX];    char buf[FILE_MAX], clientip[INET_ADDRSTRLEN];    sockfd = Socket(AF_INET,SOCK_STREAM,0);    bzero(&amp;serveraddr,sizeof (serveraddr));    serveraddr.sin_family = AF_INET;    serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);    serveraddr.sin_port = htons(SERVER_PORT);    Bind(sockfd,(struct sockaddr *)&amp;serveraddr,sizeof (serveraddr));    Listen(sockfd,20);    // 将客户端集合0号元素置位于sockfd,设置元素监听为普通数据可读    clientset[0].fd = sockfd;    clientset[0].events = POLLRDNORM;    // 初始化其他元素，注意此时从１开始    for(i = 1;i&lt; FILE_MAX;i++){      clientset[i].fd = -1;    }    maxi = 0;    printf(&quot;poll...\n&quot;);    for(;;){        // 阻塞式监听文件描述符事件sockfd,注意参数为集合元素个数，返回状态改变的文件描述符个数        ready = poll(clientset,maxi+1,-1);        // 0 元素存在数据可读状态        if(clientset[0].revents &amp; POLLRDNORM){            // 为新链接客户端分配新的描述符            clientlen = sizeof (clientaddr);            connfd = Accept(sockfd,(struct sockaddr *)&amp;clientaddr,&amp;clientlen);            printf(&quot;received from: %s at PORT: %d\n&quot;,                   inet_ntop(AF_INET, &amp;clientaddr.sin_addr, clientip, INET_ADDRSTRLEN),                   ntohs(clientaddr.sin_port));            // 遍历数组 clientset，将新客户端分配的数据交互描述符添加到 poll 监听中            for(i =1;i&lt;FILE_MAX;i++){                if(clientset[i].fd &lt; 0){                    clientset[i].fd = connfd;                    clientset[i].events = POLLRDNORM;                    break;                }            }            // 临界数据检测            if(i &gt;= FILE_MAX)                perr_exit(&quot;too many clients&quot;);            // 初始值为 0，poll 监测个数为 maxi+1，遍历时 i 从 1 开始            if(i &gt; maxi)                maxi = i;            // 只有一个事件是有新客户端链接，分配新的描述子并添加到监听中，跳出此次循环从头开始从新监听            if(--ready &lt;= 0)                continue;        }        // 遍历处理监听字符集内数据相应,此时只对　1 后面的字符集和操作        // 注意此处截止到 (i &lt;= maxi)        for(i=1; i &lt;= maxi; i++){            tempfd = clientset[i].fd;            if(tempfd&lt;0)                continue;            if(clientset[i].revents &amp; (POLLRDNORM | POLLERR)){                if((readlen = Read(tempfd,buf,BUF_SIZE))&lt;0){                    // 数据交互收到 RST 标志                    if(errno == ECONNABORTED){                        printf(&quot;client[%d] aborted connection&quot;,i);                        Close(tempfd);                        clientset[i].fd = -1;                    }else {                        perr_exit(1);                    }                }else if (readlen == 0) {                    // 数据读出0，客户端关闭了                    printf(&quot;client[%d] closed connection&quot;,i);                    Close(tempfd);                    clientset[i].fd = -1;                }else {                    // 正常数据交互                    for(j=0;j&lt;readlen;j++){                        buf[j] = toupper(buf[j]);                    }                    Write(tempfd,buf,readlen);                }                // 遍历结束条件，状态改变文件描述符个数减为 0                if(--ready ==0)                    break;            }        }    }    Close(sockfd);    return 0;}</code></pre><p>上述代码和 select逻辑基本相似，只是采用了 poll 方式，逻辑上与 select 并无二至，有以下几点说明</p><ol><li>poll 第一个参数接受了一个数组，监测一个 pollfd 的数组最大为 1024</li><li>poll 返回监测文件描述符数组内状态改变的个数，同样通过 poll 返回值，遍历处理其中的对应文件描述符的数据</li><li>初始化时候，0 号元素对应sockfd， 后面的数据处理中需要从 1 开始，中间变量的矫正需要注意 maxi 的值</li><li>状态发生改变的文件描述符，通过传出参数 revents来获取，通过其位状态，判断其数据是否可读可写</li><li>对于不想监听的某个描述符时候，直接将其 pollfd 中的 fd 置位 -1，poll 将不再对其监听<br>6。 具体的处理流程详见 select 中说明。</li></ol><h3 id="poll-小结"><a href="#poll-小结" class="headerlink" title="poll 小结"></a>poll 小结</h3><ol><li>poll 和 select 原理上都是通过轮训的方式对文件描述符进行监控，相比于 select 来说，poll 不限制文件描述符 1024</li><li>其自定义结构体 pollfd 内实现了监听事件和返回事件分离。但是其不能跨平台，只能在linux中使用。无法直接定位满足监听事件的文件描述符，需要轮询数组</li><li>poll 和 select 同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大</li><li>ppoll 为 poll 的升级版本，GNU定义了 ppoll(非 POSIX 标准),可以支持设置信号屏蔽字,具体代码略</li></ol><p><a href="https://blog.csdn.net/weixin_40204595/article/details/83212900" target="_blank" rel="noopener">参考链接</a></p><h2 id="epoll-服务器模型"><a href="#epoll-服务器模型" class="headerlink" title="epoll 服务器模型"></a>epoll 服务器模型</h2><p>目前 epell 是 linux 大规模并发网络程序中的热门首选模型。</p><p>epoll 是 Linux 下多路复用IO接口 select/poll 的增强版本,它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。epoll 模型相对 select、poll 来说不再是一个系统函数，而是需要具体的 api 来支持，因此其效率异常高效。</p><h3 id="epoll-API"><a href="#epoll-API" class="headerlink" title="epoll API"></a>epoll API</h3><p>epoll 不再是一个单独的系统调用，而是由 epoll_create、epoll_ctl、epoll_wait 三个系统调用组成。如下：</p><h4 id="epoll-create"><a href="#epoll-create" class="headerlink" title="epoll_create"></a>epoll_create</h4><p>创建一个epoll句柄,参数size用来告诉内核监听的文件描述符个数,跟内存大小有关。</p><pre><code>#include &lt;epoll.h&gt;int epoll_create(int size)size: 告诉内核监听的数目返回值： 成功时，返回一个非负整数的文件描述符，作为创建好的 epoll 句柄。调用失败时，返回 -1，错误信息可以通过 errno 获得</code></pre><p> 说明：<br> 创建一个 epoll 句柄，size 用来告诉内核这个监听的数目一共有多大。这个参数不同于 select 中的第一个参数，给出最大监听的 fd+1 的值。需要注意的是，当创建好 epoll 句柄后，它就是会占用一个 fd 值，所以在使用完 epoll 后，必须调用 close 关闭，否则可能导致 fd 被耗尽。</p><h4 id="epoll-ctl"><a href="#epoll-ctl" class="headerlink" title="epoll_ctl"></a>epoll_ctl</h4><p>控制某个epoll监控的文件描述符上的事件:注册、修改、删除。</p><pre><code>int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);epfd： epoll_create 函数返回的 epoll 句柄op： 操作选项，可选值有以下3个  EPOLL_CTL_ADD：注册新的 fd 到 epfd 中；  EPOLL_CTL_MOD：修改已经注册的 fd 的监听事件；  EPOLL_CTL_DEL：从epfd中删除一个 fd；fd： 要进行操作的目标文件描述符event： struct epoll_event结构指针，将fd和要进行的操作关联起来。返回值： 成功时，返回 0，作为创建好的 epoll 句柄。调用失败时，返回 -1，错误信息可以通过 errno 获得。</code></pre><p>相关说明：</p><ol><li><p>epoll 的事件注册函数，它不同与 select 是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。</p></li><li><p>struct epoll_event结构如下：</p><pre><code>typedef union epoll_data {    void *ptr;    int fd;    __uint32_t u32;    __uint64_t u64;  } epoll_data_t;  struct epoll_event {    __uint32_t events; /* Epoll events */    epoll_data_t data; /* User data variable */  };</code></pre></li><li><p>结构体 epoll_event 中 events可以是以下几个宏的集合</p><pre><code>EPOLLIN： 表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；EPOLLOUT： 表示对应的文件描述符可以写；EPOLLPRI： 表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；EPOLLERR： 表示对应的文件描述符发生错误；EPOLLHUP： 表示对应的文件描述符被挂断；EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。EPOLLONESHOT： 只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里</code></pre></li></ol><h4 id="epoll-wait"><a href="#epoll-wait" class="headerlink" title="epoll_wait"></a>epoll_wait</h4><p>等待所监控文件描述符上有事件的产生,类似于select()调用。</p><pre><code>int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);epfd： epoll_create 函数返回的 epoll 句柄events： struct epoll_event 结构指针，用来从内核得到事件的集合maxevents： 告诉内核这个 events 有多大timeout: 等待时的超时时间，以毫秒为单位返回值： 成功时，返回需要处理的事件数目。调用失败时，返回 0，表示等待超时</code></pre><p>说明：</p><p>epoll_wait 调用成功后，返回处理数目大小，待处理数据文件描述符都被封装到了 epoll_event * events 中去，此时只需要遍历这里面的数据就可以了，大大提高效率</p><h3 id="实例代码-2"><a href="#实例代码-2" class="headerlink" title="实例代码"></a>实例代码</h3><pre><code>#include &quot;wrap.h&quot;#include &lt;sys/epoll.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;errno.h&gt;#include &lt;netinet/in.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#define SERVER_PORT 8000#define BUF_SIZE 1024#define FILE_MAX 1024int main(int argc,char *argv[]){    int sockfd, connfd,tempfd;    int i,j, maxi, ready, epfd, readlen;    socklen_t clientlen;    char buf[BUF_SIZE], clientip[INET_ADDRSTRLEN];    int clientset[FILE_MAX];    struct sockaddr_in serveraddr,clientaddr;    struct epoll_event temp_event, ep_events[FILE_MAX];    sockfd = Socket(AF_INET,SOCK_STREAM,0);    // 初始化服务器地址参数    bzero(&amp;serveraddr,sizeof (serveraddr));    serveraddr.sin_family = AF_INET;    serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);    serveraddr.sin_port = htons(SERVER_PORT);    // 执行绑定    Bind(sockfd,(struct sockaddr *)&amp;serveraddr,sizeof (serveraddr));    // 执行listen    Listen(sockfd,20);    // 初始化局部参数，已链接客户端数据集    for(i = 0; i&lt; FILE_MAX;i++){        clientset[i] = -1;    }    maxi = -1;    //　创建 epoll 句柄    epfd = epoll_create(FILE_MAX);    if(epfd == -1){        perr_exit(&quot;epoll_create&quot;);    }    //　操作句柄监听事件    temp_event.events = EPOLLIN;    temp_event.data.fd = sockfd;    // 将 sockfd 加入到 epoll 监听中，监测事件为 EPOLLIN    int ret = epoll_ctl(epfd,EPOLL_CTL_ADD,sockfd,&amp;temp_event);    if(ret ==-1){        perr_exit(&quot;epoll_ctl sockfd&quot;);    }    printf(&quot;epoll_wait...\n&quot;);    // 循环监听事件发生    for(;;){        // 阻塞等待链接事件发生        ready = epoll_wait(epfd,ep_events,FILE_MAX,-1);        if(ready == -1){            perr_exit(&quot;epoll_wait&quot;);        }        // 遍历描述符变化的数据集 ep_events        for(i = 0;i&lt;ready;i++){            // 非数据读入事件，跳出此循环，继续阻塞等待下次事件发生            if(!(ep_events[i].events &amp; EPOLLIN))                continue;            // sockfd 描述符有状态变化，即有新的客户端链接请求，执行 accept            if (ep_events[i].data.fd == sockfd) {                clientlen = sizeof (clientaddr);                // 为新链接客户端分配新的描述符                connfd = Accept(sockfd,(struct sockaddr *)&amp;clientaddr,&amp;clientlen);                printf(&quot;received from: %s at PORT: %d\n&quot;,                       inet_ntop(AF_INET, &amp;clientaddr.sin_addr, clientip, INET_ADDRSTRLEN),                       ntohs(clientaddr.sin_port));                //　加入到服务端集合,for 结束后，j 的值为已链接客户端数目                for(j =0; j&lt; FILE_MAX;j++){                    if(clientset[j] &lt; 0){                        clientset[j] = connfd;                        break;                    }                }                // 矫正局部参数变量                if(j &gt; FILE_MAX)                    perr_exit(&quot;too many clients&quot;);                if(j&gt;maxi)                    maxi = j;                //　将新分配的客户端描述符添加到 epoll 监听中客户端的数据请求                temp_event.data.fd = connfd;                temp_event.events = EPOLLIN;                int ret = epoll_ctl(epfd,EPOLL_CTL_ADD,connfd,&amp;temp_event);                if(ret == -1)                    perr_exit(&quot;epoll_ctl sockfd&quot;);            } else {                // 此处为客户端请求服务器进行数据交互操作                // 遍历取出客户端集合中对应数据进行处理                tempfd = ep_events[i].data.fd;                readlen = Read(tempfd,buf,BUF_SIZE);                // 数据读出为0,客户端关闭了                if(readlen == 0){                    // 客户端关闭，删除 clientset 集合元素，删除 epoll_event 监听事件，关闭 socket 文件                    for(j = 0;j&lt;maxi;j++){                        if(clientset[j] = tempfd)                            clientset[i]=-1;                        break;                    }                    // 客户端关闭了，解除 epoll 中对应文件描述符的监听                    int ret = epoll_ctl(epfd,EPOLL_CTL_DEL,tempfd,NULL);                    if(ret == -1){                        perr_exit(&quot;epoll_ctl del&quot;);                    }                    Close(tempfd);                    printf(&quot;clientset[%d] closed connection! \n&quot;, i);                }else {                    // 数据读出不为 0,正常数据请求操作                    for(j =0; j&lt; readlen;j++){                        buf[j]= toupper(buf[j]);                    }                    Write(tempfd,buf,readlen);                }            }        }    }    //程序结束，关闭 socket 文件，关闭 epoll 句柄    Close(sockfd);    close(epfd);    return 0;}</code></pre><p>程序说明：</p><ol><li>epoll 监听文件描述符，要事先通过 epoll_create 创建句柄，然后将需要监听的描述符信息填充到结构体中，最后通过 epoll_ctl 函数设置到 epoll 中去</li><li>epoll 监听到文件描述符有数据到达，直接通过函数 epoll_wait中的 ep_events 参数，将文件描述符集传递出来，只需要对其进行遍历即可</li><li>由于遍历的文件描述符集就是有数据改变的文件描述符，因此效率非常高，避免无用的遍历操作</li><li>不同于 select、poll，epoll 完全没有文件描述符的限制，只限制于进程可打开文件数量的最大值（可修改），使用完成后需要关闭 epoll_create 创建的句柄</li></ol><p>相关补充：</p><p>一个进程打开大数目的socket描述符</p><pre><code>cat /proc/sys/fs/file-max</code></pre><p>设置最大打开文件描述符限制</p><pre><code>sudo vi /etc/security/limits.conf写入以下配置,soft软限制,hard硬限制* soft nofile 65536* hard nofile 100000</code></pre><h3 id="epoll总结"><a href="#epoll总结" class="headerlink" title="epoll总结"></a>epoll总结</h3><p>epoll 是 Linux 内核为处理大批量文件描述符而作了改进的 poll，是 Linux 下多路复用IO接口 select/poll 的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。另一点原因就是获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合就行了。</p><p>epoll除了提供 select/poll 那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少 epoll_wait/epoll_pwait 的调用，提高应用程序效率。epoll支持一个进程打开大数目的 socket 描述符，io效率不随 fd 数目增加而线性下降，使用 mmap 加速内核与用户空间的消息传递。</p><p>epoll 相比 select、poll 的优势：</p><ol><li>支持一个进程打开大数目的socket描述符</li><li>IO效率不随FD数目增加而线性下降</li><li>内核微调</li></ol><p><a href="https://blog.csdn.net/iEearth/article/details/46738555" target="_blank" rel="noopener">参考链接</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本章节主要介绍了 Linux 下几种常见的多路IO转接模型，以适应高并发，大数据量服务器编程实例，每个模型都有详细的代码演示和说明，相信读者看完心中会有所收货，也仅此作为自己的学习笔记，后续遗忘追溯回来，看两眼代码就能够想起来。</p><p>在博客撰文中，搜集到网络上前辈们写的一些博文，文章角度和思考深度都值得学习，希望大家看博文期间多参考文章后面的链接，多总结思考，相信会收获更多的。</p><h3 id="几种模型的优缺点比较"><a href="#几种模型的优缺点比较" class="headerlink" title="几种模型的优缺点比较"></a>几种模型的优缺点比较</h3><p>直接看下表:</p><table><thead><tr><th style="text-align:center">函数模型</th><th style="text-align:center">select</th><th style="text-align:center">poll</th><th style="text-align:center">epoll</th></tr></thead><tbody><tr><td style="text-align:center">事件集合</td><td style="text-align:center">内核会修改用户注册监听的文件描述符集，用以反馈就绪事件，每次调用 select 都需要重新填入监听文件描述符集</td><td style="text-align:center">使用 pollfd.events传入监听事件，使用 pollfd.revents来反馈就绪事件</td><td style="text-align:center">使用内内核事件表来管理用户事件，epoll_wait仅用来保存就绪事件</td></tr><tr><td style="text-align:center">程序索引就绪文件描述符集的时间复杂度</td><td style="text-align:center">O(n)</td><td style="text-align:center">O(n)</td><td style="text-align:center">O(1)</td></tr><tr><td style="text-align:center">最大支持监听的文件描述符个数</td><td style="text-align:center">有限制，一般1024</td><td style="text-align:center">65535</td><td style="text-align:center">65535</td></tr><tr><td style="text-align:center">工作模式</td><td style="text-align:center">LT</td><td style="text-align:center">LT</td><td style="text-align:center">支持ET高效模式</td></tr><tr><td style="text-align:center">内核实现原理</td><td style="text-align:center">轮训方式</td><td style="text-align:center">轮训方式</td><td style="text-align:center">回调方式</td></tr></tbody></table><p>更多详细细节比较，参考链接如下：</p><p><a href="https://blog.csdn.net/sinat_31532941/article/details/80386944" target="_blank" rel="noopener">点我没错1</a></p><p><a href="http://www.embeddedlinux.org.cn/html/yingjianqudong/201303/11-2477.html" target="_blank" rel="noopener">点我没错2</a></p><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;本章节主要介绍Linux中高并发服务器下的三种多路IO转接模型：select，poll，epoll。各模型分别从原理，系统函数，实现代码三个方面一一说明。文章内容稍有深度，代码理解不易，需要读者结合代码注释以及参考链接内容，反复比较思考。文章最后，对几种多路IO模型的优缺点进行总结，算是对此章节做的收尾工作。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="多路IO转接" scheme="https://891904833.gitee.io/FuckCode/tags/%E5%A4%9A%E8%B7%AFIO%E8%BD%AC%E6%8E%A5/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之多进程、多线程并发服务器</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/21/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%A4%9A%E8%BF%9B%E7%A8%8B%E3%80%81%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%B9%B6%E5%8F%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/21/Linux系统编程之多进程、多线程并发服务器/</id>
    <published>2019-01-21T01:41:10.000Z</published>
    <updated>2019-01-23T02:26:40.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本章节就服务器的开章，仅介绍多进程，多线程下高并发服务器模型的实现。这两种模型实现简单，逻辑清晰，但同时局限性也很大，受限于系统资源以及文件描述符上限等，后续会就多路IO转接服务器进行进一步分析。</strong><br><a id="more"></a></p><h1 id="高并发服务器"><a href="#高并发服务器" class="headerlink" title="高并发服务器"></a>高并发服务器</h1><h2 id="多进程并发服务器"><a href="#多进程并发服务器" class="headerlink" title="多进程并发服务器"></a>多进程并发服务器</h2><p>多进程并发服务器，顾名思义，采用Linux下多进程机制，对于多个客户端链接请求，服务器端对应多个进程与其进行数据通信交互。可模型理解简单，代码实现也容易，但其缺点也显而易见，主要几种以下几个方面：</p><ol><li>父进程能够创建的最大文件描述个数(父进程中需要close关闭accept返回的新文件描述符)</li><li>系统内创建进程个数(内存大小相关)</li><li>进程创建过多是否降低整体服务性能(进程调度)</li></ol><h3 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h3><pre><code>#include &quot;wrap.h&quot;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;ctype.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/wait.h&gt;#define SERVER_PORT 8000#define BUF_SIZE 1024// 信号捕捉函数，处理子进程退出，回收系统资源void do_sigchild(void* arg){    // 0 -&gt; 回收和当前调用waitpid一组的所有子进程    // 即当前进程内的所有子进程    waitpid(0,NULL,WNOHANG);}int main(int argc, char *argv[]){    int sockfd,confd, readlen;    struct sockaddr_in serveraddr,clientaddr;    socklen_t clientaddr_len;    char clientIP[128];    char buf[BUF_SIZE];    pid_t pid;    // 设置信号捕追函数，捕追子进程退出，节约资源    struct sigaction newact;    newact.sa_flags = 0;    newact.sa_handler = do_sigchild;    sigemptyset(&amp;newact);    sigaction(SIGCHLD,&amp;newact,NULL);    sockfd = Socket(AF_INET, SOCK_STREAM, 0);    serveraddr.sin_family = AF_INET;    serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);    serveraddr.sin_port = htons(SERVER_PORT);    Bind(sockfd, (struct sockaddr *)&amp;serveraddr, sizeof(serveraddr));    Listen(sockfd,30);    printf(&quot; Accepting connections ...\n&quot;);    while(1){        clientaddr_len = sizeof(clientaddr);        // 监听客户端链接，成功链接返回描述符，否则阻塞        confd = Accept(sockfd, (struct sockaddr*)&amp;clientaddr, &amp;clientaddr_len);        // 此时有客户端链接上服务器，创建子进程与其进行通信，父进程继续监听新的链接请求        pid = fork();        if(pid == 0){            // 子进程内，关闭父子进程继承的文件描述符，节约系统资源            Close(sockfd);            // 数据交互            while (1) {                readlen = Read(confd, buf, BUF_SIZE);                if (readlen == 0) {                    printf(&quot;the other side has been closed.\n&quot;);                    break;                }                printf(&quot;received from %s at PORT %d received: %s\n&quot;,                       inet_ntop(AF_INET, &amp;clientaddr.sin_addr, clientIP, sizeof(clientIP)),                       ntohs(clientaddr.sin_port),                       buf);                for (int i = 0; i &lt; readlen; i++)                    buf[i] = toupper(buf[i]);                Write(confd, buf, readlen);            }            // 数据交互结束，关闭socket链接，等待信号捕捉函数回收进程资源            Close(confd);            return 0;        }else if (pid &gt; 0) {            // 父进程内关闭链接客户端的描述符，父进程只关心链接情况，而不参与数据请求（子进程参与数据请求）            Close(confd);        }else {            // fork失败，退出            perr_exit(&quot;fork&quot;);        }    }}</code></pre><p>以上代码需要注意几点：</p><ol><li>子进程的回收处理需要在fork之前进程，因为信号捕捉函数会fork继承到子进程内，因为子进程的退出（发送SIGCHLD）可以得到处理，回收系统资源</li><li>进程的创建在链接请求操作完成之后进行，如果没有客户端链接，那么就没有子进程创建</li><li>父进程只负责监听客户端链接，分配描述符进行子进程的创建，继承下来的通信confd描述符需要关闭</li><li>子进程也需要关闭继承下来的sockfd，子进程只关心数据通信通信（confd），进一步节省系统资源</li></ol><p>从模型中可以看出，多进程下高并发服务器下，对于进程的创建开销极其大，非常考验系统内存容量，并且受限于文件描述符的大小。为了进一步节省资源，需要对进程内不需要使用的文件描述的进行适当的关闭。同时，新的链接请求产生，就意味着新进程的创建，因此高并发下多个客户端同时链接，会造成服务器某一时刻负载极其大，容易出现未知的问题。</p><h3 id="Clinet"><a href="#Clinet" class="headerlink" title="Clinet"></a>Clinet</h3><pre><code>#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;netinet/in.h&gt;#include &quot;wrap.h&quot;#define MAXLINE 80#define SERV_PORT 8000int main(int argc, char *argv[]){    struct sockaddr_in servaddr;    char buf[MAXLINE];    int sockfd, n;    char ServerIP[] =&quot;127.0.0.1&quot;;    sockfd = Socket(AF_INET, SOCK_STREAM, 0);    bzero(&amp;servaddr, sizeof(servaddr));    servaddr.sin_family = AF_INET;    inet_pton(AF_INET, ServerIP, &amp;servaddr.sin_addr);    servaddr.sin_port = htons(SERV_PORT);    Connect(sockfd, (struct sockaddr *)&amp;servaddr, sizeof(servaddr));    while (fgets(buf, MAXLINE, stdin) != NULL) {        Write(sockfd, buf, strlen(buf));        n = Read(sockfd, buf, MAXLINE);        if (n == 0)            printf(&quot;the other side has been closed.\n&quot;);        else            Write(STDOUT_FILENO, buf, n);    }    Close(sockfd);    return 0;}</code></pre><p>客户端代码就容易许多了，主要进行连接服务器，等待用户输入字符传入服务器处理，再将服务器回传数据显示打印到输出窗口中。</p><h2 id="多线程并发服务器"><a href="#多线程并发服务器" class="headerlink" title="多线程并发服务器"></a>多线程并发服务器</h2><p>从上面介绍的多进程服务模型可以推出，多线程服务模型也就是将进程概念转换为线程而已，但是考虑创建线程的开销肯定比进程来的小，所以多线程高并发服务器还是有一定的优势的。</p><p>多线程下高并发服务器的编程中需要注意以下几点：</p><ol><li>调整进程内最大文件描述符上限</li><li>线程如有共享数据,考虑线程同步</li><li>服务于客户端线程退出时,退出处理。(退出值,分离态)</li><li>系统负载,随着链接客户端增加,导致其它线程不能及时得到CPU</li></ol><h3 id="Server-1"><a href="#Server-1" class="headerlink" title="Server"></a>Server</h3><pre><code>#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;pthread.h&gt;#include &quot;wrap.h&quot;#define MAXLINE 80#define SERV_PORT 8000#define MAX_PTHREAD_SIZE 256// 自定义结构体，存储客户端地址以及对应的服务器connfdstruct s_info {    struct sockaddr_in cliaddr;    int connfd;};// 线程工作载入实体void *do_work(void *arg){    int n,i;    // 强转数据，得到s_info    struct s_info *ts = (struct s_info*)arg;    char buf[MAXLINE];    char str[INET_ADDRSTRLEN];    /* 可以在创建线程前设置线程创建属性,设为分离态,哪种效率高内? */    pthread_detach(pthread_self());    while (1) {        n = Read(ts-&gt;connfd, buf, MAXLINE);        if (n == 0) {            // 客户端关闭，跳出循环            printf(&quot;the other side has been closed.\n&quot;);            break;        }        printf(&quot;received from %s at PORT %d\n&quot;,               inet_ntop(AF_INET, &amp;(*ts).cliaddr.sin_addr, str, sizeof(str)),               ntohs((*ts).cliaddr.sin_port));        for (i = 0; i &lt; n; i++)            buf[i] = toupper(buf[i]);        Write(ts-&gt;connfd, buf, n);    }    // 客户端关闭，服务端线程关闭文件描述符    // 由于线程已置位分离态，系统自动回收线程资源    Close(ts-&gt;connfd);}int main(void){    struct sockaddr_in servaddr, cliaddr;    socklen_t cliaddr_len;    int listenfd, connfd;    int i = 0;    pthread_t tid;    // 配置同一时刻线程峰值数    struct s_info ts[MAX_PTHREAD_SIZE];    listenfd = Socket(AF_INET, SOCK_STREAM, 0);    bzero(&amp;servaddr, sizeof(servaddr));    servaddr.sin_family = AF_INET;    servaddr.sin_addr.s_addr = htonl(INADDR_ANY);    servaddr.sin_port = htons(SERV_PORT);    Bind(listenfd, (struct sockaddr *)&amp;servaddr, sizeof(servaddr));    Listen(listenfd, 20);    printf(&quot;Accepting connections ...\n&quot;);    // 主函数内主负责监听客户端链接，创建子线程    while (1) {        cliaddr_len = sizeof(cliaddr);        connfd = Accept(listenfd, (struct sockaddr *)&amp;cliaddr, &amp;cliaddr_len);        // 将必要的信息封装到结构体传入子线程中        ts[i].cliaddr = cliaddr;        ts[i].connfd = connfd;        /* 达到线程最大数时,pthread_create出错处理, 增加服务器稳定性 */        pthread_create(&amp;tid, NULL, do_work, (void*)&amp;ts[i]);        i++;        // 子线程数量过载，退出        if(i&gt;=MAX_PTHREAD_SIZE)            break;    }    Close(listenfd);    return 0;}</code></pre><p>以上代码仅仅是个模型，许多细节有待优化，集中以下几点：</p><ol><li>线程峰值数量问题：上述采用的数组容器，其峰值预先定好，对于先前创建的线程因为客户端关闭而退出，后被系统回收了情况下，此时数组容器应当记录并将给予新创建的子线程使用</li><li>子线程创建时分离态的问题：子线程创建之前就通过 pthread_attr_setdetachstate 设置，相比创建时通过 pthread_detach 来说，更有效率</li><li>和多进程模型相似，新链接的产生，对应子线程的创建，同一时刻下过多客户端链接下多线程的创建造成服务器负载过高的问题</li><li>进程pcb大小问题，适当提高进程下线程数量，也可以提高多线程高并发下的优势</li></ol><h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><p>客户端不涉及多进程和多线程概念，同上多进程下 Client 代码一致，此处不再贴源码了。</p><h2 id="线程池并发服务器"><a href="#线程池并发服务器" class="headerlink" title="线程池并发服务器"></a>线程池并发服务器</h2><p>通过以上两个高并发服务器，都有一个共同特点，那就是一对一的数据交互服务连接。一个客户端，服务器分配一个进程或者线程与其对接完成数据交互。然而应用到现实中去，如果同一时间多个客户端同时访问链接服务器，那么对于服务器端的负载过大是显而易见的，因此在此基础上，有一种更加高效的模型就诞生了，线程池高并发服务器。</p><p>同样采用线程来处理客户端的数据请求，事先分配一定数额的工作线程，然后由一个线程池管理。线程池内约束一定量的线程数量，优先保障这些线程的工作效率。先到来的客户端链接请求会优先得到线程的处理，如果同一时刻客户端访问过多，那么后面需要链接请求的，就需要等待工作线程处理结束，才能得到响应。</p><h3 id="Server-2"><a href="#Server-2" class="headerlink" title="Server"></a>Server</h3><pre><code>#include &lt;stdio.h&gt;#include &lt;assert.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;pthread.h&gt;#include &lt;semaphore.h&gt;#define MAX 10int fds[MAX]={0};sem_t sem;// 静态初始化锁pthread_mutex_t luck = PTHREAD_MUTEX_INITIALIZER;void fds_init(){    pthread_mutex_lock(&amp;luck);    int i=0;    for(;i&lt;MAX;i++)    {        fds[i]=-1;    }    pthread_mutex_unlock(&amp;luck);}int fds_add(int c){    pthread_mutex_lock(&amp;luck);    int i=0;    for(;i&lt;MAX;i++)    {        if(fds[i]==-1)        {            fds[i]=c;            return 1;        }    }    pthread_mutex_unlock(&amp;luck);    return 0;}void fds_sub(int c){    pthread_mutex_lock(&amp;luck);    int i=0;    for(;i&lt;MAX-1;i++)    {        fds[i]=fds[i+1];        if(fds[i+1]==-1)            break;    }    pthread_mutex_unlock(&amp;luck);    fds[MAX-1]=-1;}int fds_get(){    pthread_mutex_lock(&amp;luck);    int i=0;    for(;i&lt;MAX;i++)    {        if(fds[i]!=-1)        {            int c=fds[i];            fds_sub(c);            return c;        }    }    pthread_mutex_unlock(&amp;luck);    return -1;}// 线程函数载入实体void *pthread_fun(void *arg){    while(1)    {        // 获取信号量，有剩余空余线程即进入，否则等待信号号产生        sem_wait(&amp;sem);        // 通过锁机制获取待处理的客户端链接请求        int c=fds_get();        while(1)        {            char buff[128]={0};            int n = Read(c,buff,127);            if(n&lt;=0)            {                Close(c);                break;            }            Write(c,&quot;I accept&quot;,sizeof(&quot;I accept&quot;));        }    }}int main(){    // 初始化信号量    sem_init(&amp;sem,0,0);    // 初始化    fds_init();    // 线程池工作队列 3 个    for(int i=0;i&lt;3;i++)    {        pthread_t id;        int rt=pthread_create(&amp;id,NULL,(void *)pthread_fun,NULL);        assert(rt==0);    }    int sockfd=Socket(AF_INET,SOCK_STREAM,0);    assert(sockfd!=-1);    struct sockaddr_in ser,cli;    memset(&amp;ser,0,sizeof(ser));    ser.sin_family=AF_INET;    ser.sin_addr.s_addr=inet_addr(&quot;127.0.0.1&quot;);    ser.sin_port=htons(8000);    int res=Bind(sockfd,(struct sockaddr *)&amp;ser,sizeof(ser));    assert(res!=-1);    Listen(sockfd,5);    printf(&quot;Accepting connections ...\n&quot;);    while(1)    {        int len=sizeof(cli);        int c=Accept(sockfd,(struct sockaddr *)&amp;cli,&amp;len);        if(c&gt;=0)        {            if(!fds_add(c))            {                printf(&quot;Please wait a memmet&quot;);                Close(c);                continue;            }            // 产生一个信号量，其中一个工作线程开始处理            sem_post(&amp;sem);        }        else        {            printf(&quot;error\n&quot;);            continue;        }    }}</code></pre><p>上述代码中：</p><ol><li>线程池维护三个固定线程，其主要负责处理客户端的请求</li><li>客户端同时链接请求峰值为 10，同时得到响应的是工作线程峰值 3，其他的记录在数组中，工作线程处理完成后通过取得信号量，继续处理</li><li>工作线程处理请求时，使用锁机制保证各个线程处理请求时数据唯一</li><li>工作线程中有两个循环，内层循环处理和已连接上的客户端进行数据交互，外层循环监测信号量，处理文件描述符集合新的客户端请求</li></ol><p><a href="https://blog.csdn.net/zhuoya_/article/details/78724637" target="_blank" rel="noopener">参考链接</a></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章主要对Linux下高并发服务中常见的多进程，多线程模型进行实现，其模型搭建简单快速，但是局限性也很大。一来进程和线程的频繁创建太消耗资源（一个客户端对应一个进程或者线程），二来创建的进程、线程的数量大小也受限于系统资源大小。线程池模型是之前两种模型上的升级版，是一种相对理想的模型，其可以动态的响应客户端的请求，内部使用信号量、锁等同步机制保证访问资源的唯一性，同时内部使用线程代替进程，消耗资源也得到有效控制。当然最理想的还要在下章节中介绍的多路IO转接模型 select、poll、epoll 等模型。</p><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;本章节就服务器的开章，仅介绍多进程，多线程下高并发服务器模型的实现。这两种模型实现简单，逻辑清晰，但同时局限性也很大，受限于系统资源以及文件描述符上限等，后续会就多路IO转接服务器进行进一步分析。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="并发服务器" scheme="https://891904833.gitee.io/FuckCode/tags/%E5%B9%B6%E5%8F%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之Socket编程</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/18/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8BSocket%E7%BC%96%E7%A8%8B/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/18/Linux系统编程之Socket编程/</id>
    <published>2019-01-18T03:45:20.000Z</published>
    <updated>2019-01-19T07:00:37.985Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本章节主要介绍Linux下如何通过系统提供的API接口实现Socket编程。通过结合之前几篇博文中的内容，包括网络基础，TCP/IP模型以及其之间建议通信的链接细节，从理论到代码完整的贯通，毕竟Socket编程的重要性，对于程序员来说不言而喻了。</strong><br><a id="more"></a></p><h1 id="Socket编程"><a href="#Socket编程" class="headerlink" title="Socket编程"></a>Socket编程</h1><h2 id="开胃菜"><a href="#开胃菜" class="headerlink" title="开胃菜"></a>开胃菜</h2><p>在TCP/IP协议中，“IP地址+TCP或UDP端口号”就唯一标识网络通讯中的一个进程，因此，“IP 地址+端口号”就称为socket。</p><p>在TCP协议中，建立连接的两个进程各自有一个socket来标识，那么这两个socket组成 的socket pair就唯一标识一个连接。socket本身有“插座”的意思，因此用来描述网络连接的一对一关系。</p><p>TCP/IP协议最早在BSD UNIX上实现，为TCP/IP协议设计的应用层编程接口称为socket API。</p><p>再介绍具体的编程之前，我们还需要了解一下几个重要概念。</p><h3 id="网络字节序"><a href="#网络字节序" class="headerlink" title="网络字节序"></a>网络字节序</h3><p>我们已经知道，内存中的多字节数据相对于内存地址有大端和小端之分，磁盘文件中的多字节数据相对于文件中的偏移地址也有大端小端之分。网络数据流同样有大端小端之分，那么如何定义网络数据流的地址呢?</p><p>发送主机通常将发送缓冲区中的数据按内存地址从低到高的顺序发出，接收主机把从网络上接到的字节依次保存在接收缓冲区中，也是按内存地址从低到高的顺序保存，因此，网络数据流的地址应这样规定:先发出的数据是低地址，后发出的数据是高地址。</p><p>TCP/IP协议规定，网络数据流应采用大端字节序，即低地址高字节。例如上一节的UDP段格式，地址0-1是16位的源端口号，如果这个端口号是1000(0x3e8)，则地址0是0x03，地址1是0xe8，也就是先发0x03，再发0xe8，这16位在发送主机的缓冲区中也应该是低地址存0x03，高地址存0xe8。但是，如果发送主机是小端字节序的，这16位被解释成0xe803，而不是1000。因此，发送主机把1000填到发送缓冲区之前需要做字节序的转换。同样地，接收主机如果是小端字节序的，接到16位的源端口号也要做字节序的转换。如果主机是大端字节序的，发送和接收都不需要做转换。同理，32位的IP地址也要考虑网络字节序和主机字节序的问题。</p><p>为使网络程序具有可移植性，使同样的C代码在大端和小端计算机上编译后都能正常运行，可以调用以下库函数做网络字节序和主机字节序的转换。</p><pre><code>#include &lt;arpa/inet.h&gt;uint32_t htonl(uint32_t hostlong);uint16_t htons(uint16_t hostshort);uint32_t ntohl(uint32_t netlong);uint16_t ntohs(uint16_t netshort);h表示host，n表示network，l表示32位长整数，s表示16位短整数如果主机是小端字节序，这些函数将参数做相应的大小端转换后返回，如果主机是大端字节序，这些函数不作转换，将参数原封不动的返回。</code></pre><h3 id="IP地址相关函数"><a href="#IP地址相关函数" class="headerlink" title="IP地址相关函数"></a>IP地址相关函数</h3><p>IP转换函数再早期只支持IPv4，也不支持可重入，具体如下：</p><pre><code>#include &lt;sys/socket.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;int inet_aton(const *cp, struct in_addr *inp);in_addr_t inet_addr(const char *cp);char *inet_ntoa(struct in_addr in);</code></pre><p>在后续编程中，使用下面新的API接口实现方式：</p><pre><code>#include &lt;arpa/inet.h&gt;int inet_pton(int af, const char *src, void *dst);const char *inet_ntop(int af, const void *src, char *dst, socklen_t size);支持IPv4和IPv6可重入函数</code></pre><p>其中inet_pton和inet_ntop不仅可以转换IPv4的in_addr，还可以转换IPv6的in6_addr，因此函数接口是void *addrptr。</p><p>这里就不先贴代码了，总体先有个印象，后续代码中会有详细注释。</p><h3 id="结构体sockaddr"><a href="#结构体sockaddr" class="headerlink" title="结构体sockaddr"></a>结构体sockaddr</h3><p>strcut sockaddr 很多网络编程函数诞生早于IPv4协议，那时候都使用的是sockaddr结构体,为了向前兼容，现在sockaddr退化成了(void *)的作用，传递一个地址给函数，至于这个函数是sockaddr_in还是sockaddr_in6，由地址族确定，然后函数内部再强制类型转化为所需的地址类型。</p><img src="/FuckCode/2019/01/18/Linux系统编程之Socket编程/sockaddr数据结构.png" class="sockaddr数据结构"><pre><code>struct sockaddr {sa_family_t sa_family;char  sa_data[14];};struct sockaddr_in {__kernel_sa_family_t  sin_family;__be16 sin_port;struct in_addr sin_addr;unsigned char __pad[__SOCK_SIZE__ - sizeof(short int) -    sizeof(unsigned short int) - sizeof(struct in_addr)];};/* Internet address. */struct in_addr {__be32  s_addr;};struct sockaddr_in6 {unsigned short int  sin6_family;__be16 sin6_port;__be32 sin6_flowinfo;struct in6_addr sin6_addr;__u32 sin6_scope_id;};struct in6_addr {union {    __u8    u6_addr8[16];    __be16 u6_addr16[8];    __be32 u6_addr32[4];} in6_u;#define s6_addr in6_u.u6_addr8#define s6_addr16 in6_u.u6_addr16#define s6_addr32 in6_u.u6_addr32};#define UNIX_PATH_MAX 108struct sockaddr_un {__kernel_sa_family_t sun_family;char sun_path[UNIX_PATH_MAX];};</code></pre><p>IPv4和IPv6的地址格式定义在 netinet/in.h 中，IPv4地址用 sockaddr_in 结构体表示，包括16位端口号和32位IP地址，IPv6地址用 sockaddr_in6 结构体表示，包括16位端口号、128位IP地址和一些控制字段。</p><p>UNIX Domain Socket的地址格式定义在 sys/un.h 中，用 sock_addr_un 结构体表示。各种socket地址结构体的开头都是相同的，前16位表示整个结构体的长度(并不是所有UNIX的实现都有长度字段，如Linux就没有)，后16位表示地址类型。</p><p>IPv4、IPv6和Unix Domain Socket的地址类型分别定义为常数AF_INET、AF_INET6、AF_UNIX。 这样，只要取得某种sockaddr结构体的首地址，不需要知道具体是哪种类型的sockaddr结构体，就可以根据地址类型字段确定结构体中的内容。</p><p>因此，socket API可以接受各种类型的sockaddr结构体指针做参数，例如bind、accept、connect等函数，这些函数的参数应该设计成void <em>类型以便接受各种类型的指针，但是sock API的实现早于ANSI C标准化，那时还没有void </em>类型，因此这些函数的参数都用struct sockaddr *类型表示，在传递参数之前要强制类型转换一下，例如:</p><pre><code>bind(sockfd, (struct sockaddr *)&amp;serveraddr, sizeof(serveraddr));</code></pre><p>更多关于sockaddr的信息参考以下链接</p><p><a href="https://blog.csdn.net/albertsh/article/details/80991684" target="_blank" rel="noopener">关于sockaddr的相关解释</a></p><h2 id="食材"><a href="#食材" class="headerlink" title="食材"></a>食材</h2><p>在介绍具体的编程模型之前，我们先来了解一下其中涉及到的重要函数，只有了解了这些函数的相关概念，在后续的实战编程中，我们结合模型，才能更好的理解其流程。</p><h3 id="socket"><a href="#socket" class="headerlink" title="socket"></a>socket</h3><p>创建一个socket网络通讯端口。</p><p>函数原型</p><pre><code>#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int socket(int domain, int type, int protocol);domain:    AF_INET 这是大多数用来产生socket的协议,使用TCP或UDP来传输,用IPv4的地址    AF_INET6 与上面类似,不过是来用IPv6的地址    AF_UNIX 本地协议,使用在Unix和Linux系统上,一般都是当客户端和服务器在同一台及其上的时候使用type:    SOCK_STREAM 这个协议是按照顺序的、可靠的、数据完整的基于字节流的连接。这是一个使用最多的socket类型,这个socket是使用TCP来进行传输。    SOCK_DGRAM 这个协议是无连接的、固定长度的传输调用。该协议是不可靠的,使用UDP来进行它的连接。    SOCK_SEQPACKET 这个协议是双线路的、可靠的连接,发送固定长度的数据包进行传输。必须把这个包完整的接受才能进行读取。    SOCK_RAW 这个socket类型提供单一的网络访问,这个socket类型使用ICMP公共协议。(ping、traceroute使用该协议)    SOCK_RDM 这个类型是很少使用的,在大部分的操作系统上没有实现,它是提供给数据链路层使用,不保证数据包的顺序protocol:    0 默认协议返回值:    成功返回一个新的文件描述符,失败返回-1,设置errno</code></pre><p>socket()打开一个网络通讯端口,如果成功的话,就像 open() 一样返回一个文件描用出错则返回 -1。对于 IPv4,domain 参数指定为 AF_INET。对于TCP协议,type 参数指定为 SOCK_STREAM,表示面向流的传输协议。如果是 UDP 协议,则 type 参数指定为 SOCK_DGRAM,表示面向数据报的传输协议。protocol 参数的介绍从略,指定为 0 即可。</p><h3 id="bind"><a href="#bind" class="headerlink" title="bind"></a>bind</h3><p>执行socket端口与设备端口号和ip地址绑定。</p><p>函数原型</p><pre><code>  #include &lt;sys/types.h&gt;  #include &lt;sys/socket.h&gt;  int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);  sockfd:      socket文件描述符  addr:      构造出IP地址加端口号  addrlen:      sizeof(addr)长度  返回值:成功返回0,失败返回-1, 设置errno</code></pre><p>服务器程序所监听的网络地址和端口号通常是固定不变的,客户端程序得知服务器程序的地址和端口号后就可以向服务器发起连接,因此服务器需要调用 bind 绑定一个固定的网络地址和端口号。</p><p>bind()的作用是将参数 sockfd 和 结构体 sockaddr addr绑定在一起,使 sockfd 这个用于网络通讯的文件描述符监听 addr 所描述的地址和端口号。struct sockaddr *是一个通用指针类型,addr 参数实际上可以接受多种协议的 sockaddr 结构体,而它们的长度各不相同,所以需要第三个参数 addrlen 指定结构体的长度。函数接口会根据其长度自动分析出其地址是 IPV4 还是 IPV6，例如:</p><pre><code>struct sockaddr_in servaddr;// 清零bzero(&amp;servaddr, sizeof(servaddr));// 指定 IP地址为 IPV4servaddr.sin_family = AF_INET;// 任意本地 IP地址servaddr.sin_addr.s_addr = htonl(INADDR_ANY);servaddr.sin_port = htons(8000);</code></pre><p>首先将整个结构体清零,然后设置地址类型为 AF_INET,网络地址为 INADDR_ANY,这个宏表示本地的任意IP地址,因为服务器可能有多个网卡,每个网卡也可能绑定多个IP地址,这样设置可以在所有的IP地址上监听,直到与某个客户端建立了连接时才确定下来到底用哪个IP地址,端口号为8000。</p><h3 id="listen"><a href="#listen" class="headerlink" title="listen"></a>listen</h3><p>接受客户端连接，就绪等待处理。</p><p>函数原型</p><pre><code>#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int listen(int sockfd, int backlog);sockfd:    socket文件描述符backlog:    排队建立3次握手队列和刚刚建立3次握手队列的链接数和</code></pre><p>查看系统默认backlog</p><pre><code>cat /proc/sys/net/ipv4/tcp_max_syn_backlog</code></pre><p>典型的服务器程序可以同时服务于多个客户端,当有客户端发起连接时,服务器调用的 accept() 返回并接受这个连接,如果有大量的客户端发起连接而服务器来不及处理,尚未 accept 的客户端就处于连接等待状态, listen() 声明 sockfd 处于监听状态,并且最多允许有 backlog 个客户端处于连接待状态,如果接收到更多的连接请求就忽略。listen() 成功返回0,失败返回 -1。</p><h3 id="accept"><a href="#accept" class="headerlink" title="accept"></a>accept</h3><p>接受客户端的连接请求，得到一个用于读写数据的通道标识符。</p><p>函数原型</p><pre><code>#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);sockdf:    socket文件描述符addr:    传出参数,返回链接客户端地址信息,含IP地址和端口号addrlen:    传入传出参数(值-结果),传入sizeof(addr)大小,函数返回时返回真正接收到地址结构体的大小返回值:    成功返回一个新的socket文件描述符,用于和客户端通信,失败返回-1,设置errno</code></pre><p>三方握手完成后,服务器调用 accept() 接受连接,如果服务器调用 accept() 时还没有客户端的连接请求,就阻塞等待直到有客户端连接上来。addr 是一个传出参数,accept() 返回时传出客户端的地址和端口号。addrlen 参数是一个传入传出参数(value-resultargument),传入的是调用者提供的缓冲区 addr 的长度以避免缓冲区溢出问题,传出的是客户端地址结构体的实际长度(有可能没有占满调用者提供的缓冲区)。如果给 addr 参数传 NULL,表示不关心客户端的地址。</p><p>我们的服务器程序结构是这样的:</p><pre><code>while (1) {    cliaddr_len = sizeof(cliaddr);    connfd = accept(listenfd, (struct sockaddr *)&amp;cliaddr, &amp;cliaddr_len);    n = read(connfd, buf, MAXLINE);    ......    close(connfd);}</code></pre><p>整个是一个 while 死循环,每次循环处理一个客户端连接。由于 cliaddr_len 是传入传出参数,每次调用 accept() 之前应该重新赋初值。 accept() 的参数 listenfd 是先前的监听文件描述符，而 accept() 的返回值是另外一个文件描述符 connfd,之后与客户端之间就通过这个 connfd 通讯,最后关闭 connfd 断开连接,而不关闭 listenfd,再次回到循环开头 listenfd 仍然用作 accept 的参数。accept() 成功返回一个文件描述符,出错返回 -1。</p><h3 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h3><p>通过 accept 返回的新文件描述符，建立数据交互的连接通道。</p><p>函数原型</p><pre><code>#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);sockdf:    socket文件描述符addr:    传入参数,指定服务器端地址信息,含IP地址和端口号addrlen:    传入参数,传入sizeof(addr)大小返回值:    成功返回 0,失败返回 -1,设置 errno</code></pre><p>客户端需要调用 connect() 连接服务器,connect 和 bind 的参数形式一致,区别在于 bind 的参数是自己的地址,而 connect 的参数是对方的地址。服务器可以通过 connect 中的具体参数获取到当前已连接的客户端的 ip 地址和端口号。connect() 成功返回 0,出错返回 -1。</p><h2 id="美食1-TCP"><a href="#美食1-TCP" class="headerlink" title="美食1-TCP"></a>美食1-TCP</h2><p>上面具体介绍了Socket编程中用的几个重要函数，下面就来实战Socket编程中TCP的代码实现。</p><p>TCP协议的通信流程在之前TCP、UDP章节中具体分析过了，包括其三次握手建立连接，四次握手断开链接，连接中状态装换以及滑动窗口等概念和原理。下图具体展示了TCP协议的通信模型，包括客户端和服务器连接过程中具体函数接口的调用时机，两端设备端口的状态，数据导向等，再此基础上加深理解具体代码的实现过程。</p><img src="/FuckCode/2019/01/18/Linux系统编程之Socket编程/TCP协议通讯流程.png" class="TCP协议通讯流程"><h3 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h3><p>服务器的工作任务很简单，根据连接上的客户端传来的数据，进行转大写回传。</p><pre><code>#include &quot;wrap.h&quot;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;ctype.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;// 服务端端口定义 8000#define SERVER_PORT 8000// 缓冲区 4096#define BUF_SIZE 4096int main(int argc, char *argv[]){    int sockfd, confd, readlen;    struct sockaddr_in serveraddr,clientaddr;    socklen_t clientaddr_len;    char clientIP[128];    char buf[BUF_SIZE];    // 新建Socket通道，指定TCP协议    sockfd = Socket(AF_INET, SOCK_STREAM, 0);    // 填充结构体 sockaddr_in    serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);    serveraddr.sin_port = htons(SERVER_PORT);    // 绑定指定socket    Bind(sockfd, (struct sockaddr *)&amp;serveraddr, sizeof(serveraddr));    // 开始监听    Listen(sockfd,30);    printf(&quot; Accepting connections ...\n&quot;);    // 死循环等待客户链接    while(1){        clientaddr_len = sizeof(clientaddr);        // 连接上客户端        confd = Accept(sockfd, (struct sockaddr*)&amp;clientaddr, &amp;clientaddr_len);        // 打印已链接的客户端数据        printf(&quot;client IP: %s, Port: %d \n&quot;,            inet_ntop(AF_INET, &amp;clientaddr.sin_addr.s_addr,clientIP, sizeof(clientIP)),            ntohs(clientaddr.sin_port)            );        // 获取客户端传来数据        readlen = Read(confd,buf,sizeof(buf));        int i = 0;        while(i&lt;readlen){            // 小写转大写            buf[i] = toupper(buf[i]);            i++;        }        // 回传客户端        Write(confd,buf,readlen);        Close(confd);    }}</code></pre><h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><p>client.c的作用是从命令行参数中获得一个字符串发给服务器,然后接收服务器返回的字符串并打印。</p><pre><code>#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &quot;wrap.h&quot;// 与服务端的一致#define SERVER_PORT 8000#define BUF_SIZE 4096int main(int argc, char * argv[]){    int serverfd, readlen;    struct sockaddr_in serveraddr;    // ServerIP，可以自己指定自己的IP，也可以默认本地IP    char serverIP[] = &quot;127.0.0.1&quot;;    char buf[BUF_SIZE];    if(argc &lt; 2){        printf(&quot;./client agc...\n&quot;);        exit(1);    }    serverfd = Socket(AF_INET, SOCK_STREAM, 0);    // 清空结构体serveraddr    bzero(&amp;serveraddr,sizeof(serveraddr));    serveraddr.sin_family = AF_INET;    // IP地址转换    inet_pton(AF_INET, serverIP, &amp;serveraddr.sin_addr.s_addr);    serveraddr.sin_port = htons(SERVER_PORT);  　// 连接服务器    Connect(serverfd, (struct sockaddr *)&amp;serveraddr, sizeof(serveraddr));    Write(serverfd, argv[1],sizeof(argv[1]));    readlen = Read(serverfd,buf,sizeof(buf));    Write(STDOUT_FILENO, buf,readlen);    Close(serverfd);}</code></pre><p>由于客户端不需要固定的端口号,因此不必调用 bind(),客户端的端口号由内核自动分配。注意,客户端不是不允许调用 bind(),只是没有必要调用 bind() 固定一个端口号,服务器也不是必须调用 bind(),但如果服务器不调用 bind(),内核会自动给服务器分配监听端口,每次启动服务器时端口号都不一样,客户端要连接服务器就会遇到麻烦。</p><p>客户端和服务器启动后可以查看链接情况:</p><pre><code>netstat -apn|grep 8000</code></pre><p>调试相关说明：</p><ol><li>启动shell窗口，运行服务  ./server</li><li>新建shell窗口，再运行Client客户端，记得添加需要转换的额外数据，例如： ./client abcdefg</li><li>连接成功后，服务器窗口返回已连接客户端的ip地址和端口号，客户端则会打印命令中额外参数对应的大写数据，对应上面的则是：ABCDEFG</li></ol><h2 id="美食2-UDP"><a href="#美食2-UDP" class="headerlink" title="美食2-UDP"></a>美食2-UDP</h2><p>这里我们来看一下 UDP 的实战编程。由于 UDP 不需要维护连接,程序逻辑简单了很多,但是 UDP 协议是不可靠的,实际上有很多保证通讯可靠性的机制需要在应用层实现。</p><p>UDP 通信模型如下图：</p><img src="/FuckCode/2019/01/18/Linux系统编程之Socket编程/UDP协议通讯流程.png" class="TDP协议通讯流程"><p>UDP 的通信模型和 TCP 在建立链接之前的配置基本相似，但是对于后面建立连接后的数据通信过程就简单多了。sendto 负责发送数据，recvfrom 负责接受数据，其每次数据传输过程中（不论是发送还是接受数据），都需要传入地址参数，而这一切在 TCP 模型中，都被 connect 取代了。</p><h3 id="Server-1"><a href="#Server-1" class="headerlink" title="Server"></a>Server</h3><pre><code>#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;ctype.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#define SERVER_PORT 8001#define BUF_SIZE 1024int main(int argc, char const *argv[]){    int sockfd, readlen;    char buf[BUF_SIZE];    char clientIP[INET_ADDRSTRLEN];    socklen_t clientlen;    struct sockaddr_in serveraddr, clientaddr;// 创建网络通信端口，指定SOCK_DGRAM为ＵＤＰ    sockfd = socket(AF_INET, SOCK_DGRAM, 0);// 设置服务器端口信息    bzero(&amp;serveraddr, sizeof(serveraddr));    serveraddr.sin_family =  AF_INET;    serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);    serveraddr.sin_port = htons(SERVER_PORT);// 执行绑定    bind(sockfd,(struct sockaddr *)&amp;serveraddr,sizeof(serveraddr));    printf(&quot;Accepting connections ...\n&quot;);    while(1){        bzero(&amp;clientaddr,sizeof(clientaddr));        clientlen = sizeof(clientlen);    // 从已连接的客户端读数据，传出参数clientaddr包含客户端信息        readlen = recvfrom(sockfd, buf, BUF_SIZE, 0,            (struct sockaddr *)&amp;clientaddr,&amp;clientlen);        printf(&quot;client IP: %s, Port: %d \n&quot;,            inet_ntop(AF_INET, &amp;clientaddr.sin_addr,clientIP, sizeof(clientIP)),            ntohs(clientaddr.sin_port)            );        int i = 0;        while(i&lt;readlen){            buf[i] = toupper(buf[i]);            i++;        }    // 回传数据到客户端，传出参数客户端clientaddr结构体信息        sendto(sockfd,buf,readlen,0,            (struct sockaddr *)&amp;clientaddr,sizeof(clientaddr));    }    close(sockfd);    return 0;}</code></pre><h3 id="Client-1"><a href="#Client-1" class="headerlink" title="Client"></a>Client</h3><pre><code>#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#define SERVER_PORT 8001#define BUF_SIZE 1024int main(int argc, char * argv[]){    int serverfd, readlen;    struct sockaddr_in serveraddr;    char serverIP[] = &quot;127.0.0.1&quot;;    char buf[BUF_SIZE];    socklen_t serverlen;    if(argc &lt; 2){        printf(&quot;./client agc...\n&quot;);        exit(1);    }    serverfd = socket(AF_INET, SOCK_DGRAM, 0);    bzero(&amp;serveraddr,sizeof(serveraddr));    serveraddr.sin_family = AF_INET;    inet_pton(AF_INET, serverIP, &amp;serveraddr.sin_addr);    serveraddr.sin_port = htons(SERVER_PORT);    sendto(serverfd,argv[1],sizeof(argv[1]),0,        (struct sockaddr *)&amp;serveraddr, sizeof(serveraddr));    bzero(&amp;serverlen,sizeof(serverlen));    readlen = recvfrom(serverfd,buf,BUF_SIZE,0,        NULL, 0);    write(STDOUT_FILENO, buf,readlen);    close(serverfd);    return 0;}</code></pre><p>UDP 模型下的实例代码，运行流程和 TCP 相似，首先启动 Server，然后开启 Client，发送数据到服务器进行大写转换，客户端读取收到的数据打印出来。</p><p>相信有读者已经发现了其两者之间通信模型的差距，其实理解起来也很容易，TCP 相当于打电话，开始接通之前喂喂喂确认是对方后，知道挂断前都不用再确认对方身份，也就是在数据交互过程中不需要夹杂地址信息了；然而 UDP 则必须每次数据交互中都需要填入对方地址信息，就犹如寄信，每次张信都需要地址和邮票一样。</p><p>总的话来说，还是其通信的本质有区别，TCP 是基于数据流方式，UDP 是基于消息方式。</p><h2 id="饭后甜点"><a href="#饭后甜点" class="headerlink" title="饭后甜点"></a>饭后甜点</h2><p>上面的两个实例不仅功能简单，而且简单到几乎没有什么错误处理。然而在系统调用不能保证每次都成功，必须进行出错处理，这样一方面可以保证程序逻辑正常，另一方面可以迅速得到故障信息。</p><p>为使错误处理的代码不影响主程序的可读性，我们把与 socket 相关的一些系统函数加上错误处理代码包装成新的函数，做成一个模块 wrap.c，提供一个头文件 wrap.h 依赖，具体的wrap.c 如下：</p><h3 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h3><pre><code>#ifndef WRAP_H#define WRAP_H#include &lt;stdlib.h&gt;#include &lt;sys/socket.h&gt;#include &lt;errno.h&gt;void perr_exit(const char *s);int Accept(int fd, struct sockaddr *sa, socklen_t *salenptr);void Bind(int fd, const struct sockaddr *sa, socklen_t salen);void Connect(int fd, const struct sockaddr *sa, socklen_t salen);void Listen(int fd, int backlog);int Socket(int family, int type, int protocol);ssize_t Read(int fd, void *ptr, size_t nbytes);ssize_t Write(int fd, const void *ptr, size_t nbytes);void Close(int fd);ssize_t Readn(int fd, void *vptr, size_t n);ssize_t Writen(int fd, const void *vptr, size_t n);static ssize_t my_read(int fd, char *ptr);ssize_t Readline(int fd, void *vptr, size_t maxlen);#endif // WRAP_H</code></pre><p>###　实现文件</p><pre><code>#include &quot;wrap.h&quot;void perr_exit(const char *s){    perror(s);    exit(1);}int Accept(int fd, struct sockaddr *sa, socklen_t *salenptr){    int n;again:    if((n = accept(fd,sa,salenptr)) &lt; 0){        if((errno == ECONNABORTED) || (errno == EINTR)){            goto again;        }else {            perr_exit(&quot;accept&quot;);        }        return n;    }}void Bind(int fd, const struct sockaddr *sa, socklen_t salen){    if(bind(fd,sa,salen)&lt;0)        perr_exit(&quot;bind&quot;);}void Connect(int fd, const struct sockaddr *sa, socklen_t salen){    if (connect(fd, sa, salen) &lt; 0)        perr_exit(&quot;connect error&quot;);}void Listen(int fd, int backlog){    if (listen(fd, backlog) &lt; 0)        perr_exit(&quot;listen error&quot;);}int Socket(int family, int type, int protocol){    int n;    if((n = socket(family,type,protocol))&lt;0)        perr_exit(&quot;socket&quot;);    return n;}ssize_t Read(int fd, void *ptr, size_t nbytes){    ssize_t n;again:    if((n = read(fd,ptr,nbytes))==-1){        if(errno == EINTR)            goto again;        else            return -1;    }    return n;}ssize_t Write(int fd, const void *ptr, size_t nbytes){    ssize_t n;again:    if ( (n = write(fd, ptr, nbytes)) == -1) {        if (errno == EINTR)            goto again;        else            return -1;    }    return n;}void Close(int fd){    if (close(fd) == -1)        perr_exit(&quot;close error&quot;);}ssize_t Readn(int fd, void *vptr, size_t n){    size_t nleft;    ssize_t nread;    char            *ptr;    ptr = vptr;    nleft = n;    while (nleft &gt; 0) {        if ( (nread = read(fd, ptr, nleft)) &lt; 0) {            if (errno == EINTR)                nread = 0;            else                return -1;        } else if (nread == 0)            break;        nleft -= nread;        ptr += nread;    }    return n - nleft;}ssize_t Writen(int fd, const void *vptr, size_t n){    size_t nleft;    ssize_t nwritten;    const char *ptr;    ptr = vptr;    nleft = n;    while (nleft &gt; 0) {        if ( (nwritten = write(fd, ptr, nleft)) &lt;= 0) {            if (nwritten &lt; 0 &amp;&amp; errno == EINTR)                nwritten = 0;            else                return -1;        }        nleft -= nwritten;        ptr += nwritten;    }    return n;}ssize_t my_read(int fd, char *ptr){    static int read_cnt;    static char *read_ptr;    static char read_buf[100];    if (read_cnt &lt;= 0) {again:        if ( (read_cnt = read(fd, read_buf, sizeof(read_buf))) &lt; 0) {            if (errno == EINTR)                goto again;            return -1;        } else if (read_cnt == 0)            return 0;        read_ptr = read_buf;    }    read_cnt--;    *ptr = *read_ptr++;    return 1;}ssize_t Readline(int fd, void *vptr, size_t maxlen){    ssize_t n, rc;    char            c, *ptr;    ptr = vptr;    for (n = 1; n &lt; maxlen; n++) {        if ( (rc = my_read(fd, &amp;c)) == 1) {            *ptr++ = c;            if (c                    == &apos;\n&apos;)                break;        } else if (rc == 0) {            *ptr = 0;            return n - 1;        } else            return -1;    }    *ptr = 0;    return n;}</code></pre><p>对于上述模块，可以采用以下几种调用方式</p><ol><li>源代码直接包含，简单直接，在 Makefile 中直接声明 -I 方式将头文件包含进去，记得 wrap.c 文件需要和源文件 server.c client.c 同级目录。</li><li>直接包含头文件，源文件以动态库形式存在，隐藏源码，需要事先将模块编译成 so 动态链接库，具体方法，请参考之前静态、动态链接库实战章节。</li></ol><h2 id="真香"><a href="#真香" class="headerlink" title="真香"></a>真香</h2><p>本章节介绍了Socket编程下，TCP,UDP模型的实例代码。由于其通信模型的不同，其代码实现上也有不同，但是大同小异。相同的是建立连接服务过程中，服务端必须指定ip地址和端口号，绑定后，客户端连接时也要和服务器的一致；不同的是在数据交互过程中，TCP基于流，一旦建立链接(connect)后就可以直接数据传输，无需地址信息的参与，而UDP则需要在数据交互中，有一个传入传出参数负责地址的接受和发送，每次发送数据都需要将接收数据的地址信息包含进去，谁发来，回传给谁。</p><p>总而言之，对于上述代码实现以及章节中的模型图，相信对这两种通信协议能有很好的理解。</p><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;本章节主要介绍Linux下如何通过系统提供的API接口实现Socket编程。通过结合之前几篇博文中的内容，包括网络基础，TCP/IP模型以及其之间建议通信的链接细节，从理论到代码完整的贯通，毕竟Socket编程的重要性，对于程序员来说不言而喻了。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="Socket" scheme="https://891904833.gitee.io/FuckCode/tags/Socket/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之TCP/UDP协议</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/17/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8BTCP-UDP%E5%8D%8F%E8%AE%AE/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/17/Linux系统编程之TCP-UDP协议/</id>
    <published>2019-01-17T06:34:21.000Z</published>
    <updated>2019-01-19T06:59:49.982Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本章节集中分析网络编程必备知识TCP，UDP数据报格式，熟悉其原理，优缺点以及通信细节，尤其是对于TCP下的三次握手，状态转换，滑动窗口等复杂机制的理解，为后续Socket编程打下牢牢的基础。</strong><br><a id="more"></a></p><h1 id="TCP-UDP"><a href="#TCP-UDP" class="headerlink" title="TCP/UDP"></a>TCP/UDP</h1><h2 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h2><p>从之前网络基础中，我们了解到，UDP协议不面向连接，也不保证可靠性，有点像寄信，写好信放到邮筒里，既不能保证信件在邮递过程中不会丢失，也不能保证信件是按顺序寄到目的地的。 使用UDP协议的应用程序需要自己完成丢包重发、消息排序等工作。</p><p>其协议相对简单，并且更加灵活，对服务器和网络资源的负载也相对小一些。底层协议提供的数据不保障可靠性，那么就需要在上层应用中自己试试对应的政策来保证数据传输的有效性了。</p><h3 id="UDP数据报格式"><a href="#UDP数据报格式" class="headerlink" title="UDP数据报格式"></a>UDP数据报格式</h3><p>UDP数据段如下图所示：</p><img src="/FuckCode/2019/01/17/Linux系统编程之TCP-UDP协议/UDP数据段.png" class="UDP数据段"><p>图示很简单，UDP格式中8字节64bit的数据是必须的，只包含了源端口号，目的端口号，UDP长度以及校验位，结合之前网络基础的各层协议数据报格式，这里具体分析一下。</p><h3 id="UDP通信分析"><a href="#UDP通信分析" class="headerlink" title="UDP通信分析"></a>UDP通信分析</h3><p>下面分析一帧基于UDP的TFTP协议帧</p><pre><code>以太网首部0000: 00055d67d0b100055d6158a80800IP首部0000: 45 000010: 005393250000801125ecc0a80037c0a80020: 00 01UDP首部0020: 05 d4 00 45 00 3f ac 40TFTP协议0020: 00 01 ‘c”:‘”’q’0030: ‘w’‘e’‘r’‘q”.‘’q’‘w’‘e’00 ’n’‘e”t’‘a”s’‘c’‘i’ 0040: ‘i’00 ’b’‘l’‘k”s’‘i’‘z’‘e’00 ’5’‘1’‘2’00 ’t’‘i’ 0050: ’m’‘e’‘o’‘u”t’00 ‘1’‘0’00 ’t”s’‘i’‘z’‘e’00 ’0’ 0060: 00</code></pre><ol><li>以太网首部: 源MAC地址是 00:05:5d:61:58:a8，目的MAC地址是 00:05:5d:67:d0:b1，上层协议类型 0x0800表示IP。</li><li>IP首部: 每一个字节0x45包含4位版本号和4位首部长度，版本号为4，即IPv4，首部长度为5，说明IP首部不带有选项字段。服务类型为0，没有使用服务。16位总长度字段(包括IP首部和IP层payload的长度)为0x0053，即83字节，加上以太网首部14字节可知整个帧长度是97字节。IP报标识是0x9325，标志字段和片偏移字段设置为0x0000，就是DF=0允许分片，MF=0此数据报没有更多分片，没有分片偏移。TTL是0x80，也就是128。上层协议0x11表示UDP协议。IP首部校验和为0x25ec，源主机IP是 c0 a8 00 37(192.168.0.55)，目的主机IP是 c0 a8 00 01(192.168.0.1)。</li><li>UDP首部: 源端口号 0x05d4(1492)是客户端的端口号，目的端口号 0x0045(69)是TFTP服务的well-known端口号。UDP报长度为0x003f，即63字节，包括UDP首部和UDP层pay-load的长度。UDP首部和UDP层payload的校验和为0xac40。</li><li><p>TFTP是基于文本的协议，各字段之间用字节0分隔，开头的00 01表示请求读取一个文件，接下来的各字段是:</p><pre><code>c:\qwerq.qwenetasciiblksize 512timeout 10tsize 0</code></pre></li></ol><p>一般的网络通信都是像TFTP协议这样，通信的双方分别是客户端和服务器，客户端主动发起请求(上面的例子就是客户端发起的请求帧)，而服务器被动地等待、接收和应答请求。客户端的IP地址和端口号唯一标识了该主机上的TFTP客户端进程，服务器的IP地址和端口号唯一标识了该主机上的TFTP服务进程，由于客户端是主动发起请求的一方，它必须知道服务器的IP地址和TFTP服务进程的端口号，所以，一些常见的网络协议有默认的服务器端口，例如HTTP服务默认TCP协议的80端口，FTP服务默认TCP协议的21端口，TFTP服务默认UDP协议的69端口(如上例所示)。在使用客户端程序时，必须指定服务器的主机名或IP地址， 如果不明确指定端口号则采用默认端口，请读者查阅ftp、tftp等程序的man page了解如何指定端口号。/etc/services中列出了所有well-known的服务端口和对应的传输层协议，这是由IANA(Internet Assigned Numbers Authority)规定的，其中有些服务既可以用TCP也可以用UDP，为了清晰，IANA规定这样的服务采用相同的TCP或UDP默认端口号，而另外一些TCP和UDP的相同端口号却对应不同的服务。</p><p>很多服务有well-known的端口号，然而客户端程序的端口号却不必是well-known的，往往是每次运行客户端程序时由系统自动分配一个空闲的端口号，用完就释放掉，称为 ephemeral的端口号.</p><p>发送端的UDP协议层只管把应用层传来的数据封装成段交给IP协议层就算完成任务了， 如果因为网络故障该段无法发到对方，UDP协议层也不会给应用层返回任何错误信息。接收端的UDP协议层只管把收到的数据根据端口号交给相应的应用程序就算完成任务了，如果发送端发来多个数据包并且在网络上经过不同的路由，到达接收端时顺序已经错乱了，UDP协议层也不保证按发送时的顺序交给应用层。</p><p>通常接收端的UDP协议层将收到的数据放在一个固定大小的缓冲区中等待应用程序来提取和处理，如果应用程序提取和处理的速度很慢，而发送端发送的速度很快，就会丢失数据包，UDP协议层并不报告这种错误。</p><p>因此，使用UDP协议的应用程序必须考虑到这些可能的问题并实现适当的解决方案，例如等待应答、超时重发、为数据包编号、流量控制等。一般使用UDP协议的应用程序实现都比较简单，只是发送一些对可靠性要求不高的消息，而不发送大量的数据。例如，基于UDP的TFTP协议一般只用于传送小文件(所以才叫trivial的ftp)，而基于TCP的FTP协议适用于各种文件的传输。</p><h2 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h2><p>TCP是一种面向连接的、可靠的协议。TCP传输的双方需要首先建立连接，之后由TCP协议保证数据收发的可靠性，丢失的数据包自动重发，上层应用程序收到的总是可靠的数据流，通讯之后关闭连接。</p><h3 id="TCP数据报格式"><a href="#TCP数据报格式" class="headerlink" title="TCP数据报格式"></a>TCP数据报格式</h3><p>作为一种可靠的协议，其格式相比UDP来说自然更加复杂些，具体如下图所示：</p><img src="/FuckCode/2019/01/17/Linux系统编程之TCP-UDP协议/TCP数据段.png" class="TCP数据段"><p>和UDP协议一样也有源端口号和目的端口号，通讯的双方由IP地址和端口号标识。32位序号、32位确认序号、窗口大小稍后详细解释。4位首部长度和IP协议头类似，表示TCP协议头的长度，以4字节为单位，因此TCP协议头最长可以是4x15=60字节，如果没有选项字段，TCP协议头最短20字节。URG、ACK、PSH、RST、SYN、FIN是六个控制位，本节稍后将解释SYN、ACK、FIN、RST四个位，其它位的解释从略。16位检验和将TCP协议头和数据都计算在内。紧急指针和各种选项的解释从略。</p><h3 id="TCP通信时序"><a href="#TCP通信时序" class="headerlink" title="TCP通信时序"></a>TCP通信时序</h3><p>下图是一次TCP通讯的时序图：</p><img src="/FuckCode/2019/01/17/Linux系统编程之TCP-UDP协议/TCP时序图.png" class="TCP时序图"><p>首先客户端主动发起连接、发送请求，然后服务器端响应请求，然后客户端主动关闭连接。两条竖线表示通讯的两端，从上到下表示时间的先后顺序，注意，数据从一端传到网络的另一端也需要时间，所以图中的箭头都是斜的。双方发送的段按时间顺序编号为1-10，各段中的主要信息在箭头上标出，例如段2的箭头上标着 SYN, 8000(0), ACK 1001,表示该段中的 SYN 位置1，32位序号是 8000，该段不携带有效载荷(数据字节数为0)，ACK 位置1，32位确认序号是 1001，带有一个 mss 选项值为1024。</p><h4 id="建立连接"><a href="#建立连接" class="headerlink" title="建立连接"></a>建立连接</h4><p>TCP建立连接的过程，或者称为三次握手，如下：</p><ol><li>客户端发出段1，SYN 位表示连接请求。序号是1000，这个序号在网络通讯中用作临时的地址，每发一个数据字节，这个序号要加1，这样在接收端可以根据序号排出数据包的正确顺序，也可以发现丢包的情况，另外，规定 SYN 位和 FIN 位也要占一个序号，这次虽然没发数据，但是由于发了 SYN 位，因此下次再发送应该用序号 1001。mss 表示最大段尺寸，如果一个段太大，封装成帧后超过了链路层的最大帧长度，就必须在IP层分片，为了避免这种情况，客户端声明自己的最大段尺寸，建议服务器端发来的段不要超过这个长度。</li><li>服务器发出段2，也带有 SYN 位，同时置 ACK 位表示确认，确认序号是 1001，表示“我接收到序号1000及其以前所有的段，请你下次发送序号为1001的段”，也就是应答了客户端的连接请求，同时也给客户端发出一个连接请求，同时声明最大尺寸为1024。</li><li>客户端发出段3，对服务器的连接请求进行应答，其中带有标志 ACK，，确认序号在接受数据上加一，是8001。</li></ol><p>在这个过程中，客户端和服务器分别给对方发了连接请求，也应答了对方的连接请求，其中服务器的请求和应答在一个段中发出，因此一共有三个段用于建立连接，称为“三方握手(three-way-handshake)”。在建立连接的同时，双方协商了一些信息，例如双方发送序号的初始值、最大段尺寸等。</p><p>在TCP通讯中，如果一方收到另一方发来的段，读出其中的目的端口号，发现本机并没有任何进程使用这个端口，就会应答一个包含 RST 位的段给另一方。</p><h4 id="数据传输的过程"><a href="#数据传输的过程" class="headerlink" title="数据传输的过程"></a>数据传输的过程</h4><p>由图中中间段数据传输图示，可以得知：</p><ol><li>客户端发出段4，包含从序号1001开始的20个字节数据， ACK 确认序号 8001（确认三次握手建立连接的到的 ACK 8001）。</li><li>服务器发出段5，确认序号 ACK 为1021，对序号为1001-1020的数据表示确认收到，同时请求发送序号1021开始的数据，服务器在应答的同时也向客户端发送从序号8001开始的10个字节数据，这称为piggyback。</li><li>客户端发出段6，对服务器发来的序号为8001-8010的数据表示确认收到，请求发送序号8011开始的数据。</li></ol><h4 id="断开连接"><a href="#断开连接" class="headerlink" title="断开连接"></a>断开连接</h4><p>TCP关闭连接的过程，或称为四次握手:</p><ol><li>客户端发出段7，FIN 位表示客户端主动关闭彼此连接的请求。</li><li>服务器发出段8，应答客户端的关闭连接请求， ACK 在之前 1021 基础上加一。</li><li>服务器发出段9，其中也包含 FIN 位，服务端开始关闭，向客户端发送关闭连接的请求。</li><li>客户端发出段10，应答服务器的关闭连接请求，表示以确认服务端关闭。</li></ol><p>以上，基本描述了TCP一次完整通信过程中的详细细节。</p><h3 id="TCP状态转换图"><a href="#TCP状态转换图" class="headerlink" title="TCP状态转换图"></a>TCP状态转换图</h3><p>相信对于上面的时序图来说，大多数都不陌生。上述过程在非常理想状态下是完全ok的，也就说是排除了定位网络或系统故障。对于具体的每个状态及其装换过程，尤其涉及到网络波动时，就需要对每个状态之间的切换作了解，同时在了解了状态图后，那么对于时序图的理解会更加充分。下面对这张图的11种状态详细解析一下，以便加强记忆!不过在这之前，先回顾一下TCP建立连接的三次握手过程，以及关闭连接的四次握手过程。</p><h4 id="TCP的连接与断开"><a href="#TCP的连接与断开" class="headerlink" title="TCP的连接与断开"></a>TCP的连接与断开</h4><h5 id="三次握手建立连接"><a href="#三次握手建立连接" class="headerlink" title="三次握手建立连接"></a>三次握手建立连接</h5><ol><li>客户端发送一个带 SYN 标志的TCP报文到服务器。这是三次握手过程中的报文1。</li><li>服务器端回应客户端的，这是三次握手中的第2个报文，这个报文同时带 ACK 标志和 SYN 标志。因此它表示对刚才客户端 SYN 报文的回应;同时又标志 SYN 给客户端，询问客户端是否准备好进行数据通讯。</li><li>客户必须再次回应服务段一个 ACK 报文，这是报文段3。</li></ol><h5 id="四次握手终止连接"><a href="#四次握手终止连接" class="headerlink" title="四次握手终止连接"></a>四次握手终止连接</h5><p>由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这原则是当一方完成它的数据发送任务后就能发送一个 FIN 来终止这个方向的连接。收到一个 FIN 只意味着这一方向上没有数据流动，一个TCP连接在收到一个 FIN 后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。</p><ol><li>TCP客户端发送一个 FIN ，用来关闭客户到服务器的数据传送(报文段4)。</li><li>服务器收到这个 FIN ，它发回一个 ACK ，确认序号为收到的序号加1(报文段5)。和 SYN 一样，一个 FIN 将占用一个序号。</li><li>服务器关闭客户端的连接，发送一个 FIN 给客户端(报文段6)。</li><li>客户段发回 ACK 报文确认，并将确认序号设置为收到序号加1(报文段7)。</li></ol><h4 id="状态转换图"><a href="#状态转换图" class="headerlink" title="状态转换图"></a>状态转换图</h4><p>TCP通信过程中，各个阶段涉及的具体的状态如下图所示：</p><img src="/FuckCode/2019/01/17/Linux系统编程之TCP-UDP协议/TCP状态转换图.png" class="TCP状态转换图"><p>其图示中涉及的具体状态点如下解释：</p><ol><li>CLOSED: 这个没什么好说的了,表示初始状态。</li><li>LISTEN: 这个也是非常容易理解的一个状态,表示服务器端的某个SOCKET处于监听状态,可以接受连接了。</li><li>SYN_RCVD: 这个状态表示接受到了SYN报文,在正常情况下,这个状态是服务器端的SOCKET在建立TCP连接时的三次握手会话过程中的一个中间状态,很短暂,基本上用netstat你是很难看到这种状态的,除非你特意写了一个客户端测试程序,故意将三次TCP握手过程中最后一个ACK报文不予发送。因此这种状态时,当收到客户端的ACK报文<br>后,它会进入到ESTABLISHED状态。</li><li>SYN_SENT: 这个状态与SYN_RCVD遥想呼应,当客户端SOCKET执行CONNECT连接时,它首先发送 SYN 报文,因此也随即它会进入到了SYN_SENT状态,并等待服务端的发送三次握手中的第2个报文。SYN_SENT状态表示客户端已发送 SYN 报文。</li><li>ESTABLISHED:这个容易理解了,表示连接已经建立了。</li><li>FIN_WAIT_1: 这个状态要好好解释一下,其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是:FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时,它想主动关闭连接,向对方发送了 FIN 报文,此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应 ACK 报文后,则进入到FIN_WAIT_2状态,当然在实际的正常情况下,无论对方何种情况下,都应该马上回应ACK报文,所以FIN_WAIT_1状态一般是比较难见到的,而FIN_WAIT_2状态还有时常常可以用netstat看到。</li><li>FIN_WAIT_2:上面已经详细解释了这种状态,实际上FIN_WAIT_2状态下的SOCKET,表示半连接,也即有一方要求close连接,但另外还告诉对方,我暂时还有点数据需要传送给你,稍后再关闭连接。</li><li>TIME_WAIT: 表示收到了对方的FIN报文,并发送出了ACK报文,就等2MSL后即可回到CLOSED可用状态了。如果FIN_WAIT_1状态下,收到了对方同时带 FIN 标志和 ACK 标志的报文时,可以直接进入到TIME_WAIT状态,而无须经过FIN_WAIT_2状态。</li><li>CLOSING: 这种状态比较特殊,实际情况中应该是很少见,属于一种比较罕见的例外状态。正常情况下,当你发送 FIN 报文后,按理来说是应该先收到(或同时收到)对方的  ACK 报文,再收到对方的 FIN 报文。但是CLOSING状态表示你发送FIN报文后,并没有收到对方的ACK报文,反而却也收到了对方的 FIN 报文。什么情况下会出现此种情况<br>呢?其实细想一下,也不难得出结论:那就是如果双方几乎在同时close一个SOCKET的话,那么就出现了双方同时发送 FIN 报文的情况,也即会出现CLOSING状态,表示双方都正在关闭SOCKET连接。</li><li>CLOSE_WAIT: 这种状态的含义其实是表示在等待关闭。怎么理解呢?当对方close一个SOCKET后发送 FIN 报文给自己,你系统毫无疑问地会回应一个ACK报文给对方,此时则进入到CLOSE_WAIT状态。接下来呢,实际上你真正需要考虑的事情是察看你是否还有数据发送给对方,如果没有的话,那么你也就可以close这个SOCKET,发送 FIN 报文给对方,也即关闭连接。所以你在CLOSE_WAIT状态下,需要完成的事情是等待你去关闭连接。</li><li>LAST_ACK: 这个状态还是比较容易好理解的,它是被动关闭一方在发送 FIN 报文后,最后等待对方的 ACK 报文。当收到 ACK 报文后,也即可以进入到CLOSED可用状态了。</li></ol><p>以上分析需要结合上面两张图一起观察，此种涉及到了通信过程中的种种细节，尤其是客户端和服务端发送相应数据后的状态变化，其中还有一些对未知情况的响应处理策略，需要认真回味。</p><h3 id="TCP流量控制（滑动窗口）"><a href="#TCP流量控制（滑动窗口）" class="headerlink" title="TCP流量控制（滑动窗口）"></a>TCP流量控制（滑动窗口）</h3><p>在介绍UDP时，我们描述了这样的问题:如果发送端发送的速度较快，接收端接收到数据后 处理的速度较慢，而接收缓冲区的大小是固定的，就会丢失数据。TCP协议可以通过’滑动窗口 (Sliding Window)’机制解决这一问题。看下图的通讯过程。</p><img src="/FuckCode/2019/01/17/Linux系统编程之TCP-UDP协议/TCP滑动窗口.png" class="TCP滑动窗口"><p>提下对图示进行解释：</p><ol><li>发送端发起连接，声明最大段尺寸mss是1460，初始序号是0，窗口大小是4K，表示“我的接收缓冲区还有4K字节空闲，你发的数据不要超过4K”。接收端应答连接请求，声明最大段尺寸是1024，初始序号是8000，窗口大小是6K。发送端应答，三方握手结束。</li><li>发送端发出段4-9，每个段带1K的数据，发送端根据窗口大小知道接收端的缓冲区满了，因此停止发送数据。</li><li>接收端的应用程序提走2K数据，接收缓冲区又有了2K空闲，接收端发出段10，在应答已收到6K数据的同时声明窗口大小为2K（取走数据2k，就剩下了2k的空余窗口大小）。</li><li>接收端的应用程序又提走2K数据，接收缓冲区有4K空闲，接收端发出段11，重新声明窗口大小为4K。</li><li>发送端发出段12-13，每个段带1K数据，段13同时还包含FIN位。</li><li>接收端应答接收到的2K数据(6145-8192)，再加上 FIN 位占一个序号8193，因此应答序号是8194，连接处于半关闭状态，接收端同时声明窗口大小为2K（原剩余空间4k-2k接受新的数据，剩余大小2k）。</li><li>接收端的应用程序提走2K数据，接收端重新声明窗口大小为4K。</li><li>接收端的应用程序提走剩下的2K数据，接收缓冲区全空，接收端重新声明窗口大小为 6K。</li><li>接收端的应用程序在提走全部数据后，决定关闭连接，发出段17包含 FIN 位，发送端应答，连接完全关闭。</li></ol><p>上图在接收端用小方块表示1K数据，实心的小方块表示已接收到的数据，虚线框表示接 收缓冲区，因此套在虚线框中的空心小方块表示窗口大小，从图中可以看出，随着应用程序提走数据，虚线框是向右滑动的，因此称为滑动窗口。</p><p>从这个例子还可以看出，发送端是1K、1K地发送数据，而接收端的应用程序可以2K、2K地提走数据，当然也有可能一次提走3K或6K数据，或者一次只提走几个字节的数据，也就是说，应用程序所看到的数据是一个整体，或说是一个流(stream)，在底层通讯中这些数据可能被拆成很多数据包来发送，但是一个数据包有多少字节对应用程序是不可见的，因此TCP协议是面向流的协议。而UDP是面向消息的协议，每个UDP段都是一条消息，应用程序必须以消息为单位提取数据，不能一次提取任意字节的数据，这一点和TCP是很不同的。</p><h3 id="TCP半链接状态"><a href="#TCP半链接状态" class="headerlink" title="TCP半链接状态"></a>TCP半链接状态</h3><p>当TCP链接中A发送 FIN 请求关闭，另一段B回应 ACK 后，B没有立即发送 FIN 给A时，A方处在半链接状态，此时A可以接收B发送的数据，但是A已不能再向B发送数据。</p><p>如下函数原型</p><pre><code>#include &lt;sys/socket.h&gt;int shutdown(int sockfd, int how)sockfd: 需要关闭的socket的描述符how: 允许为shutdown操作选择以下几种方式:        SHUT_RD:关闭连接的读端。也就是该套接字不再接受数据,任何当前在套接字接受缓冲区的数据将被丢弃。进程将不能对该套接字发出任何读操作。对TCP套接字该调用之后接受到的任何数据将被确认然后无声的丢弃掉。        SHUT_WR:关闭连接的写端,进程不能在对此套接字发出写操作        SHUT_RDWR:相当于调用shutdown两次:首先是以SHUT_RD,然后以SHUT_WR</code></pre><p>使用close中止一个连接，但它只是减少描述符的参考数，并不直接关闭连接，只有当描述符的参考数为0时才关闭连接。shutdown可直接关闭描述符，不考虑描述符的参考数，可选择中止一个方向的连接。</p><p>注意: </p><ol><li>如果有多个进程共享一个套接字，close每被调用一次，计数减1，直到计数为0时，也就是所用进程都调用了close，套接字将被释放。</li><li>在多进程中如果一个进程中shutdown(sfd, SHUT_RDWR)后，其它的进程将无法进行通信。如果一个进程close(sfd)将不会影响到其它进程。</li></ol><h3 id="2MSL-TIME-WAIT"><a href="#2MSL-TIME-WAIT" class="headerlink" title="2MSL/TIME_WAIT"></a>2MSL/TIME_WAIT</h3><p>TIME_WAIT状态的存在有两个理由:</p><ol><li>让4次握手关闭流程更加可靠;4次握手的最后一个 ACK 是由主动关闭方发送出去的，若这个 ACK 丢失，被动关闭方会再次发一个 FIN 过来。若主动关闭方能够保持一个2MSL的TIME_WAIT状态，则有更大的机会让丢失的 ACK 被再次发送出去。</li><li>防止lost duplicate对后续新建正常链接的传输造成破坏。lost duplicate在实际的网络中非常常见，经常是由于路由器产生故障，路径无法收敛，导致一个packet在路由器A，B，C之间做类似死循环的跳转。IP头部有个TTL，限制了一个包在网络中的最大跳数，因此这个包有两种命运，要么最后TTL变为0，在网络中消失;要么TTL在变为0之前路由器路径收敛，它凭借剩余的TTL跳数终于到达目的地。但非常可惜的是TCP通过超时重传机制在早些时候发送了一个跟它一模一样的包，并先于它达到了目的地，因此它的命运也就注定被TCP协议栈抛弃。另外一个概念叫做incarnation connection，指跟上次的socket pair一摸一样的新连接，叫做incarnation of previous connection。lost duplicate加上incarnation connection，则会对我们的传输造成致命的错误。大家都知道TCP是流式的，所有包到达的顺序是不一致的，依靠序列号由TCP协议栈做顺序的拼接;假设一个incarnation connection这时收到的seq=1000, 来了一个lost duplicate为seq=1000, len=1000, 则tcp认为这个lost duplicate合法，并存放入了receive buffer，导致传输出现错误。通过一个2MSL TIME_WAIT状态，确保所有的lost duplicate都会消失掉，避免对新连接造成错误。</li></ol><p>该状态为什么设计在主动关闭这一方:</p><ol><li>发最后 ACK 的是主动关闭一方</li><li>只要有一方保持TIME_WAIT状态，就能起到避免incarnation connection在2MSL内的重新建立，不需要两方都有</li></ol><p>如何正确对待2MSL TIME_WAIT?</p><p>RFC要求socket pair在处于TIME_WAIT时,不能再起一个incarnation connection。但绝大部分TCP实现,强加了更为严格的限制。在2MSL等待期间,socket中使用的本地端口在默认情况下不能再被使用。若A 10.234.5.5:1234和B 10.55.55.60:6666建立了连接,A主动关闭,那么在A端只要port为1234,无论对方的port和ip是什么,都不允许再起服务。显而易见这是比RFC更为严格的限制,RFC仅仅是要求socket pair不一致,而实现当中只要这个port处于TIME_WAIT,就不允许起连接。这个限制对主动打开方来说是无所谓的,因为一般用的是临时端口;但对于被动打开方,一般是server,就悲剧了,因为server一般是熟知端口。比如http,一般端口是80,不可能允许这个服务在2MSL内不能起来。解决方案是给服务器的socket设置SO_REUSEADDR选项,这样的话就算熟知端口处于TIME_WAIT状态,在这个端口上依旧可以将服务启动。当然,虽然有了SO_REUSEADDR选项,但sockt pair这个限制依旧存在。比如上面的例子,A通过SO_REUSEADDR选项依旧在1234端口上起了监听,但这时我们若是从B通过6666端口去连它,TCP协议会告诉我们连接失败,原因为Address already in use.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本博文涉及众多细节知识点，基本梳理了邢文鹏老师的Linux教学资料中的内容，也相当于自己对这些方面的知识点的回顾。有了之前网络的基础知识，以及这些TCP、UDP通讯细节，对下面的Socket变成来说就能够更加游刃有余。</p><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;本章节集中分析网络编程必备知识TCP，UDP数据报格式，熟悉其原理，优缺点以及通信细节，尤其是对于TCP下的三次握手，状态转换，滑动窗口等复杂机制的理解，为后续Socket编程打下牢牢的基础。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="网络 TCP UDP" scheme="https://891904833.gitee.io/FuckCode/tags/%E7%BD%91%E7%BB%9C-TCP-UDP/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之网络基础</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/16/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/16/Linux系统编程之网络基础/</id>
    <published>2019-01-16T03:47:04.000Z</published>
    <updated>2019-01-17T06:37:50.462Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本章节主要介绍网络基础知识，包括OSI模型，TCP/IP模型，以太网帧、ARP、IP等协议数据包，TCP/IP三次握手等，集中介绍网络编程中遇到的专业名词，为后续Socket编程以及高并发服务器模型大下基础。</strong><br><a id="more"></a></p><h1 id="网络基础"><a href="#网络基础" class="headerlink" title="网络基础"></a>网络基础</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="OSI七层模型"><a href="#OSI七层模型" class="headerlink" title="OSI七层模型"></a>OSI七层模型</h3><p>OSI七层模型，有上及下分别为应用层，表示层，会话层，传输层，网络层，数据链路层，物理层。如下图所示：</p><img src="/FuckCode/2019/01/16/Linux系统编程之网络基础/OSI模型.png" class="OSI模型"><p>图中已经将OSI模型对应的部分，以TCP/IP模型的方式进行了归类，具体每个层的含义如下：</p><ol><li>物理层: 主要定义物理设备标准，如网线的接口类型、光纤的接口类型、各种传输介质的传输速率等。它的主要作用是传输比特流(就是由1、0转化为电流强弱来进行传输，到达目的地后再转化为1、0，也就是我们常说的数模转换与模数转换)。这一层的数据叫做比特，最原始的数据。</li><li>数据链路层: 定义了如何让格式化数据以进行传输，以及如何让控制对物理介质的访问。这一层通常还提供错误检测和纠正，以确保数据的可靠传输。</li><li>网络层: 在位于不同地理位置的网络中的两个主机系统之间提供连接和路径选择。Internet的发展使得从世界各站点访问信息的用户数大大增加，而网络层正是管理这种连接的层。</li><li>传输层: 定义了一些传输数据的协议和端口号(WWW端口80等)，如:TCP(传输控制协议，传输效率低，可靠性强，用于传输可靠性要求高，数据量大的数据)，UDP(用户数据报协议，与TCP特性恰恰相反，用于传输可靠性要求不高，数据量小的数据，如QQ聊天数据就是通过这种方式传输的)。 主要是将从下层接收的数据进行分段和传输，到达目的地址后再进行重组。常常把这一层数据叫做段。</li><li>会话层: 通过传输层(端口号:传输端口与接收端口)建立数据传输的通路。主要在你的系统之间发起会话或者接受会话请求(设备之间需要互相认识可以是IP也可以是MAC或者是主机名)。</li><li>表示层: 可确保一个系统的应用层所发送的信息可以被另一个系统的应用层读取。例如，PC程序与另一台计算机进行通信，其中一台计算机使用扩展二一十进制交换码 (EBCDIC)，而另一台则使用美国信息交换标准码(ASCII)来表示相同的字符。如有必要，表示层会通过使用一种通格式来实现多种数据格式之间的转换。</li><li>应用层: 是最靠近用户的OSI层。这一层为用户的应用程序(例如电子邮件、文件传输和终端仿真)提供网络服务。</li></ol><h3 id="TCP-IP模型"><a href="#TCP-IP模型" class="headerlink" title="TCP/IP模型"></a>TCP/IP模型</h3><p>单纯将OSI模型拿出来讲，就已经很复杂了。然而对于我们开发人员来说，TCP/IP模型就显得更亲切了，如下图：</p><img src="/FuckCode/2019/01/16/Linux系统编程之网络基础/TCPIP模型.png" class="TCPIP模型"><p>从图中可以看出，理解TCP/IP模型相对OSI模型来说，就相对简单了，其适当对OSI模型进行抽象，分为四层：</p><ol><li>应用层，和用户直接打交道的一层，如Telnet，FTP和e-mail等</li><li>传输层，和程序员打交道最多的一层，设计Socket编程，如TCP，UDP</li><li>网络层，此层应该是网络中的设备，例如路由器，交换机的部分工作</li><li>链路层，此层接近于硬件设备，驱动模块以及接口卡</li></ol><h2 id="通信过程"><a href="#通信过程" class="headerlink" title="通信过程"></a>通信过程</h2><p>如果两台计算机在不同的网段中，那么数据从一台计算机到另一台计算机传输过程中要经过一个或多个路由器，如下图所示：</p><img src="/FuckCode/2019/01/16/Linux系统编程之网络基础/TCPIP通信过程.png" class="TCPIP通信过程"><p>其实在链路层之下还有物理层，指的是电信号的传递方式，比如现在以太网通用的网线 (双绞线)、早期以太网采用的的同轴电缆(现在主要用于有线电视)、光纤等都属于物理层的概念。物理层的能力决定了最大传输速率、传输距离、抗干扰性等。集线器(Hub)是工作在物理层的网络设备，用于双绞线的连接和信号中继(将已衰减的信号再次放大使之传得更远)。</p><img src="/FuckCode/2019/01/16/Linux系统编程之网络基础/跨路由通信过程.png" class="跨路由通信过程"><p>链路层有以太网、令牌环网等标准，链路层负责网卡设备的驱动、帧同步(就是说从网线上检测到什么信号算作新帧的开始)、冲突检测(如果检测到冲突就自动重发)、数据差错校验等工作。交换机是工作在链路层的网络设备，可以在不同的链路层网络之间转发数据帧(比如十兆以太网和百兆以太网之间、以太网和令牌环网之间)，由于不同链路层的帧格式不同，交换机要将进来的数据包拆掉链路层首部重新封装之后再转发。</p><p>网络层的IP协议是构成Internet的基础。Internet上的主机通过IP地址来标识，Internet上有大量路由器负责根据IP地址选择合适的路径转发数据包，数据包从Internet上的源主机到目的主机往往要经过十多个路由器。路由器是工作在第三层的网络设备，同时兼有交换机的功能，可以在不同的链路层接口之间转发数据包，因此路由器需要将进来的数据包拆掉网络层和链路层两层首部并重新封装。IP协议不保证传输的可靠性，数据包在传输过程中可能丢失，可靠性可以在上层协议或应用程序中提供支持。</p><p>网络层负责点到点(point-to-point)的传输(这里的“点”指主机或路由器)，而传输层负责端到端(end-to-end)的传输(这里的“端”指源主机和目的主机)。传输层可选择TCP或UDP协议。</p><p>TCP是一种面向连接的、可靠的协议，有点像打电话，双方拿起电话互通身份之后就建立了连接，然后说话就行了，这边说的话那边保证听得到，并且是按说话的顺序听到的，说完话挂机断开连接。也就是说TCP传输的双方需要首先建立连接，之后由TCP协议保证数据收发的可靠性，丢失的数据包自动重发，上层应用程序收到的总是可靠的数据流，通讯之后关闭连接。</p><p>UDP协议不面向连接，也不保证可靠性，有点像寄信，写好信放到邮筒里，既不能保证信件在邮递过程中不会丢失，也不能保证信件是按顺序寄到目的地的。 使用UDP协议的应用程序需要自己完成丢包重发、消息排序等工作。</p><p>目的主机收到数据包后，如何经过各层协议栈最后到达应用程序呢?整个过程如下图所示：</p><img src="/FuckCode/2019/01/16/Linux系统编程之网络基础/Multiplexing过程.png" class="Multiplexing过程"><p>以太网驱动程序首先根据以太网首部中的“上层协议”字段确定该数据帧的有效载荷 (payload，指除去协议首部之外实际传输的数据)是IP、ARP还是RARP协议的数据报，然后交给相应的协议处理。假如是IP数据报，IP协议再根据IP首部中的“上层协议”字段确定该数据报的有效载荷是TCP、UDP、ICMP还是IGMP，然后交给相应的协议处理。假如是TCP段或UDP段，TCP或UDP协议再根据TCP首部或UDP首部的“端口号”字段确定应该将应用层数据交给哪个用户进程。IP地址是标识网络中不同主机的地址，而端口号就是同一台主机上标识不同进程的地址，IP地址和端口号合起来标识网络中唯一的进程。</p><p>注意，虽然IP、ARP和RARP数据报都需要以太网驱动程序来封装成帧，但是从功能上划分，ARP和RARP属于链路层，IP属于网络层。虽然ICMP、IGMP、TCP、UDP的数据都需要IP协议来封装成数据报，但是从功能上划分，ICMP、IGMP与IP同属于网络层，TCP和UDP属于传输层。</p><h2 id="协议格式"><a href="#协议格式" class="headerlink" title="协议格式"></a>协议格式</h2><h3 id="数据包封装"><a href="#数据包封装" class="headerlink" title="数据包封装"></a>数据包封装</h3><p>传输层及其以下的机制由内核提供，应用层由用户进程提供，应用程序对通讯数据的含义进行解释，而传输层及其以下处理通讯的细节，将数据从一台计算机通过一定的路径发送到另一台计算机。应用层数据通过协议栈发到网络上时，每层协议都要加上一个数据首部(header)，称为封装 (Encapsulation)，如下图所示</p><p>不同的协议层对数据包有不同的称谓，在传输层叫做段(segment)，在网络层叫做数据报(datagram)，在链路层叫做帧(frame)。</p><p>数据封装成帧后发到传输介质上，到达目的主机后每层协议再剥掉相应的首部，最后将应用层数据交给应用程序处理，可以将其过程理解为发送快递，快递过程中的打包、投递以及拆包过程对应到数据的各个阶段。</p><h3 id="以太网帧格式"><a href="#以太网帧格式" class="headerlink" title="以太网帧格式"></a>以太网帧格式</h3><p>以太网帧从名字可以看出其处于数据链路层，其中的源地址和目的地址是指网卡的硬件地址(也叫MAC地址)，长度是48位，是在网卡出厂时固化的，全球唯一标识。用ifconfig命令看一下，“HWaddr 00:15:F2:14:9E:3F”部分就是硬件地址。协议字段有三种值，分别对应IP、ARP、RARP。帧末尾是CRC校验码。</p><img src="/FuckCode/2019/01/16/Linux系统编程之网络基础/以太网帧.png" class="以太网帧"><p>以太网帧中的数据长度（注意是数据长度，而不是全部长度）规定最小46字节，最大1500字节，由于ARP和RARP数据包的长度不够46字节，要在后面补填充位。例如类型为0806的ARP协议，其数据区必须达到最小规定字节数，28+18=46字节。</p><p>最大值1500称为以太网的最大传输单元(MTU)，不同的网络类型有不同的MTU，如果一个数据包从以太网路由到拨号链路上，数据包长度大于拨号链路的MTU了，则需要对数据包进行分片(fragmentation)。ifconfig命令的输出中也有“MTU: 1500”。注意，MTU这个概念指数据帧中有效载荷的最大长度，不包括帧首部的长度。</p><h3 id="ARP数据报格式"><a href="#ARP数据报格式" class="headerlink" title="ARP数据报格式"></a>ARP数据报格式</h3><p>在网络通讯时，源主机的应用程序知道目的主机的IP地址和端口号，却不知道目的主机的硬件地址，而数据包首先是被网卡接收到再去处理上层协议的，如果接收到的数据包的硬件地址与本机不符，则直接丢弃。因此在通讯前必须获得目的主机的硬件地址。ARP协议就起到这个作用。</p><p>源主机发出ARP请求，询问“IP地址是192.168.0.1的主机的硬件地址是多少”，并将这个请求广播到本地网段(以太网帧首部的硬件地址填FF:FF:FF:FF:FF:FF表示广播)，目的主机接收到广播的ARP请求，发现其中的IP地址与本机相符，则发送一个ARP应答数据包给源主机，将自己的硬件地址填写在应答包中。</p><p>每台主机都维护一个ARP缓存表，可以用 arp -a 命令查看。缓存表中的表项有过期时间 (一般为20分钟)，如果20分钟内没有再次使用某个表项，则该表项失效，下次还要发 ARP 请求来获得目的主机的硬件地址。</p><p>ARP数据报的格式如下所示</p><img src="/FuckCode/2019/01/16/Linux系统编程之网络基础/ARP数据报格式.png" class="ARP数据报格式"><p>注意到源 MAC 地址、目的 MAC 地址在以太网首部和 ARP 请求中各出现一次，对于链路层为以太网的情况是多余的，但如果链路层是其它类型的网络则有可能是必要的。硬件类型指链路层网络类型，1为以太网，协议类型指要转换的地址类型，0x0800为IP地址，后面两个地址长度对于以太网地址和IP地址分别为6和4(字节)，op字段为1表示ARP请求，op字段为2表示ARP应答。</p><p>具体通信流程这里不做过多解释。</p><h3 id="IP段格式"><a href="#IP段格式" class="headerlink" title="IP段格式"></a>IP段格式</h3><p>直接看图：</p><img src="/FuckCode/2019/01/16/Linux系统编程之网络基础/IP段格式.png" class="IP段格式"><p>IP数据报的首部长度和数据长度都是可变长的，但总是4字节的整数倍。对于IPv4，4位版本字段是4。4位首部长度的数值是以4字节（4*8=32位）为单位的，最小值为5（5列数据），也就是说首部长度最小是4x5=20字节，也就是不带任何选项的IP首部，4位能表示的最大值是15，也就是说首部长度最大是60字节。</p><p>8位TOS字段有3个位用来指定IP数据报的优先级(目前已经废弃不用)，还有4个位表示可选的服务类型(最小延迟、最大吞吐量、最大可靠性、最小成本)， 还有一个位总是0。总长度是整个数据报(包括IP首部和IP层payload)的字节数。每传一个IP数据报，16位的标识加1，可用于分片和重新组装数据报。3位标志和13位片偏移用于分片。</p><p>TTL(Time to live)是这样用的:源主机为数据包设定一个生存时间，比如64，每过一个路由器就把该值减1，如果减到0就表示路由已经太长了仍然找不到目的主机的网络，就丢弃该包，因此这个生存时间的单位不是秒，而是跳(hop)。协议字段指示上层协议是TCP、UDP、ICMP还是IGMP。然后是校验和，只校验IP首部，数据的校验由更高层协议负责。IPv4的IP地址长度为32位。选项字段的解释从略。</p><h3 id="UDP数据报格式"><a href="#UDP数据报格式" class="headerlink" title="UDP数据报格式"></a>UDP数据报格式</h3><p>这里忽略，后续博文详细分析。</p><h3 id="TCP数据包格式"><a href="#TCP数据包格式" class="headerlink" title="TCP数据包格式"></a>TCP数据包格式</h3><p>这里忽略，后续博文详细分析。</p><h2 id="网络术语"><a href="#网络术语" class="headerlink" title="网络术语"></a>网络术语</h2><h3 id="路由route"><a href="#路由route" class="headerlink" title="路由route"></a>路由route</h3><ol><li>网络信息从信源到信宿的路径.路由是指路由器从一个接口上收到数据包，根据数据包的目的地址进行定向并转发到另一个接口的过程。</li><li>路由通常与桥接来对比，在粗心的人看来，它们似乎完成的是同样的事。它们的主要区别在于桥接发生在OSI参考模型的第二层(数据链路层)，而路由发生在第三层(网络层)。这一区别使二者在传递信息的过程中使用不同的信息，从而以不同的方式来完成其任务。</li><li>确定最佳路径,通过网络传输信息。</li></ol><h3 id="路由器"><a href="#路由器" class="headerlink" title="路由器"></a>路由器</h3><p>路由器(Router)是连接因特网中各局域网、广域网的设备，它会根据信道的情况自动选择和设定路由，以最佳路径，按前后顺序发送信号的设备。</p><p>传统地，路由器工作于OSI七层协议中的第三层，其主要任务是接收来自一个网络接口的数据包，根据其中所含的目的地址，决定转发到下一个目的地址。因此，路由器首先得在转发路由表中查找它的目的地址，若找到了目的地址，就在数据包的帧格前添加下一个 MAC 地址，同时IP数据包头的 TTL(Time To Live)域也开始减数，并重新计算校验和。当数据包被送到输出端口时，它需要按顺序等待，以便被传送到输出链路上。</p><p>路由器在工作时能够按照某种路由通信协议查找设备中的路由表。如果到某一特定节点有一条以上的路径，则基本预先确定的路由准则是选择最优(或最经济)的传输路径。由于各种网络段和其相互连接情况可能会因环境变化而变化，因此路由情况的信息一般也按所使用的路由信息协议的规定而定时更新。</p><p>网络中，每个路由器的基本功能都是按照一定的规则来动态地更新它所保持的路由表，以便保持路由信息的有效性。为了便于在网络间传送报文，路由器总是先按照预定的规则把较大的数据分解成适当大小的数据包，再将这些数据包分别通过相同或不同路径发送出去。当这些数据包按先后秩序到达目的地后，再把分解的数据包按照一定顺序包装成原有的报文形式。</p><p>路由器的分层寻址功能是路由器的重要功能之一，该功能可以帮助具有很多节点站的网络来存储寻址信息，同时还能在网络间截获发送到远地网段的报文，起转发作用;选择最合理的路由，引导通信也是路由器基本功能;多协议路由器还可以连接使用不同通信协议的网络段，成为不同通信协议网络段之间的通信平台。</p><p>路由和交换之间的主要区别就是交换发生在OSI参考模型第二层(数据链路层)，而路由发生在第三层，即网络层。这一区别决定了路由和交换在移动信息的过程中需使用不同的控制信息，所以两者实现各自功能的方式是不同的。</p><h3 id="路由表"><a href="#路由表" class="headerlink" title="路由表"></a>路由表</h3><p>在计算机网络中，路由表或称路由择域信息库(RIB)是一个存储在路由器或者联网计 算机中的电子表格(文件)或类数据库。路由表存储着指向特定网络地址的路径</p><h3 id="以太网交换机"><a href="#以太网交换机" class="headerlink" title="以太网交换机"></a>以太网交换机</h3><p>以太网交换机是基于以太网传输数据的交换机，以太网采用共享总线型传输媒体方式的局域网。以太网交换机的结构是每个端口都直接与主机相连，并且一般都工作在全双工方式。交换机能同时连通许多对端口，使每一对相互通信的主机都能像独占通信媒体那样，进行无冲突地传输数据。</p><p>以太网交换机工作于OSI网络参考模型的第二层(即数据链路层)，是一种基于 MAC(Media Access Control，介质访问控制)地址识别、完成以太网数据帧转发的网络设备。</p><h3 id="DNS服务器"><a href="#DNS服务器" class="headerlink" title="DNS服务器"></a>DNS服务器</h3><p>DNS是域名系统 (Domain Name System) 的缩写，是因特网的一项核心服务，它作为可以将域名和IP地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的IP地址串。</p><p>它是由解析器以及域名服务器组成的。域名服务器是指保存有该网络中所有主机的域名和对应IP地址，并具有将域名转换为IP地址功能的服务器。</p><h3 id="局域网"><a href="#局域网" class="headerlink" title="局域网"></a>局域网</h3><p>一种覆盖一座或几座大楼、一个校园或者一个厂区等地理区域的小范围的计算机网</p><ol><li>覆盖的地理范围较小，只在一个相对独立的局部范围内联，如一座或集中的建筑群内。</li><li>使用专门铺设的传输介质进行联网，数据传输速率高(10Mb/s~10Gb/s) </li><li>通信延迟时间短，可靠性较高</li><li>局域网可以支持多种传输介质</li></ol><h3 id="广域网"><a href="#广域网" class="headerlink" title="广域网"></a>广域网</h3><p>一种用来实现不同地区的局域网或城域网的互连，可提供不同地区、城市和国家之间的计算机通信的远程计算机网。</p><p>覆盖的范围比局域网(LAN)和城域网(MAN)都广。广域网的通信子网主要使用分组交换技术。广域网的通信子网可以利用公用分组交换网、卫星通信网和无线分组交换网，它将分布在不同地区的局域网或计算机系统互连起来，达到资源共享的目的。如互联网是世界范围内最大的广域网。</p><ol><li>适应大容量与突发性通信的要求; </li><li>适应综合业务服务的要求; </li><li>开放的设备接口与规范化的协议; </li><li>完善的通信服务与网络管理。</li></ol><h3 id="端口"><a href="#端口" class="headerlink" title="端口"></a>端口</h3><p>逻辑意义上的端口，一般是指TCP/IP协议中的端口，端口号的范围从0到65535，比如用于浏览网页服务的80端口，用于FTP服务的21端口等等。</p><ol><li>端口号小于256的定义为常用端口，服务器一般都是通过常用端口号来识别的。</li><li>客户端只需保证该端口号在本机上是惟一的就可以了。客户端口号因存在时间很短暂又称临时端口号;</li><li>大多数TCP/IP实现给临时端口号分配1024—5000之间的端口号。大于5000的端口<br>号是为其他服务器预留的。</li></ol><h3 id="MTU"><a href="#MTU" class="headerlink" title="MTU"></a>MTU</h3><p>MTU:通信术语，最大传输单元(Maximum Transmission Unit，MTU)</p><p>是指一种通信协议的某一层上面所能通过的最大数据包大小(以字节为单位)。最大传输单元这个参数通常与通信接口有关(网络接口卡、串口等)。以下是一些协议的MTU</p><pre><code>FDDI协议：4352字节以太网（Ethernet）协议：1500字节PPPoE（ADSL）协议：1492字节X.25协议（Dial Up/Modem）：576字节Point-to—Point：4470字节</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本章节集中介绍了网络中涉及的基础内容，包括OSI模型，TCP/IP模型，各层次之间数据包组织形式，以及网络中各种专业名次的相关解释，基本对网络能有有一个简单的认识，至于对我们最重要的TCP，UDP协议数据报的相关内容，后续会专门展开说明。</p><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;本章节主要介绍网络基础知识，包括OSI模型，TCP/IP模型，以太网帧、ARP、IP等协议数据包，TCP/IP三次握手等，集中介绍网络编程中遇到的专业名词，为后续Socket编程以及高并发服务器模型大下基础。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="网络" scheme="https://891904833.gitee.io/FuckCode/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之线程同步</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/15/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/15/Linux系统编程之线程同步/</id>
    <published>2019-01-15T01:54:26.000Z</published>
    <updated>2019-01-15T09:58:17.678Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本章节着重介绍多线程下资源同步的问题，之前章节介绍了Linux多线程编程的有关知识，由于多线程下容易产生资源互抢情况，所以必须对共享资源进行统一规划，其涉及到的概念有互斥量，条件变量，信号量，文件锁以及进程间锁。</strong><br><a id="more"></a></p><h3 id="线程同步"><a href="#线程同步" class="headerlink" title="线程同步"></a>线程同步</h3><p>多线程下共享资源的争夺很是普遍的，对于为什么要进行线程同步，有如下几点：</p><ol><li>共享资源，多个线程都可对共享资源操作 </li><li>线程操作共享资源的先后顺序不确定 </li><li>处理器对存储器的操作一般不是原子操作</li></ol><h4 id="互斥量-pthread-mutex-t"><a href="#互斥量-pthread-mutex-t" class="headerlink" title="互斥量 pthread_mutex_t"></a>互斥量 pthread_mutex_t</h4><p>在多线程中，当多个线程访问同一个共享资源时候，这时候系统提供一把锁和一个钥匙，当线程A请求访问公共资源，那么线程A就申请系统拿到钥匙并上锁，此时如果另外一个线程B也想访问这个共享资源的话，也要向系统申请钥匙开锁，此时系统发现锁已经被线程A持有，就告诉线程B等待线程A访问完成后，锁被归还再通知线程B可以拿钥匙上锁，继而访问公共资源。</p><p>针对以上模型，互斥量就是这么个概念。可以把互斥变量值置为常量PTHREAD_MUTEX_INITIALIZER(针对静态分配的互斥量)，或调用pthread_mutex_init函数进行初始化。如果动态的分配互斥量（如调用malloc函数），那么在释放内存前需要调用pthread_mutex_destory。</p><h5 id="操作原语"><a href="#操作原语" class="headerlink" title="操作原语"></a>操作原语</h5><pre><code>// 初始化，动态初始化int pthread_mutex_init(pthread_mutex_t *restrict mutex, const pthread_mutexattr_t *restrict attr)// 动态初始化后，手动回收int pthread_mutex_destroy(pthread_mutex_t *mutex)// 拿钥匙上锁，如果拿到钥匙，锁就被持有，外部再调用拿钥匙访问就会阻塞等待int pthread_mutex_tlock(pthread_mutex_t *mutex)// 尝试拿钥匙上锁，如果已经被锁，直接返回EBUSY，不阻塞int pthread_mutex_trylock(pthread_mutex_t *mutex)// 释放锁资源int pthread_mutex_unlock(pthreadd_mutex_t *mutex)</code></pre><p>对于锁的初始化有两种方式，静态和动态，如下：</p><ol><li><p>静态，直接使用常量 PTHREAD_MUTEX_INITIALIZER 初始化</p><pre><code>// 锁静态声明定义pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;pthread_mutex_lock(&amp;mutex);//do sometingpthread_mutex_unlock(&amp;mutex);</code></pre></li><li><p>动态初始化</p><pre><code>pthread_mutex_t mutex;// 默认方式初始化 mutex，属性值 NULLpthread_mutex_init(&amp;mutex, NULL);pthread_mutex_lock(&amp;mutex);//do sometingpthread_mutex_unlock(&amp;mutex);// 释放锁资源pthread_mutex_destroy(&amp;mutex)</code></pre></li></ol><h5 id="临界区"><a href="#临界区" class="headerlink" title="临界区"></a>临界区</h5><p>保证在某一时刻只有一个线程能访问数据的简便办法。在任意时刻只允许一个线程对共享资源进行访问。如果有多个线程试图同时访问临界区，那么在有一个线程进入后其他所有试图访问此临界区的线程将被挂起，并一直持续到进入临界区的线程离开。临界区在被释放后，其他线程可以继续抢占，并以此达到用原子方式操作共享资源的目的。</p><p>临界区的选定</p><pre><code>临界区的选定因尽可能小，如果选定太大会影响程序的并行处理性能。</code></pre><h5 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h5><p>在使用互斥量时候，要避免死锁的发生，死锁产生原因：</p><ol><li>同一个线程在拥有A锁的情况下再次请求获得A锁 </li><li>线程一拥有A锁，请求获得B锁;线程二拥有B锁，请求获得A锁</li></ol><p>避免发生死锁的方法： </p><ol><li>按顺序加锁，例如加锁 lock1, lock2,lock3，在加锁 lock3 之前，必须先加 lock2 ,以此类推。释放 lock 时必须按倒序来</li><li>如果无法确定锁的顺序，尽量用 pthread_mutex_trylock 代替 pthread_mutex_lock 以避免死锁</li><li>用串行代替并行</li></ol><h5 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h5><pre><code>#include &lt;pthread.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;/*    互斥量    同一时间为 1 或者 0    保持共享数据的完整正确性*/#define NLOOP 5000// 共享数据int counter;// 锁静态声明定义pthread_mutex_t counter_mutex = PTHREAD_MUTEX_INITIALIZER;void *pthread_do(void * arg){    int i, val;    for (int i = 0; i &lt; NLOOP; ++i)    {        // 加锁，否则数据值丢失一半        pthread_mutex_lock(&amp;counter_mutex);        val = counter;        printf(&quot;%x: %d\n&quot;, (unsigned int)pthread_self(), val + 1);        counter = val + 1;        // 解除锁        pthread_mutex_unlock(&amp;counter_mutex);    }    return NULL;}int main(int argc, int *argv[]){    pthread_t tidA,tidB;    // 创建两个线程，争抢共享资源，通过互斥量保持数据的正确性    pthread_create(&amp;tidA,NULL,pthread_do,NULL);    pthread_create(&amp;tidB,NULL,pthread_do,NULL);    // 回收线程数据    pthread_join(tidA, NULL);    pthread_join(tidB, NULL);    return 0;}</code></pre><h4 id="读写锁-pthread-rwlock-t"><a href="#读写锁-pthread-rwlock-t" class="headerlink" title="读写锁 pthread_rwlock_t"></a>读写锁 pthread_rwlock_t</h4><p>百度百科如下解释读写锁：</p><pre><code>读写锁实际是一种特殊的自旋锁，它把对共享资源的访问者划分成读者和写者，读者只对共享资源进行读访问，写者则需要对共享资源进行写操作。这种锁相对于自旋锁而言，能提高并发性，因为在多处理器系统中，它允许同时有多个读者来访问共享资源，最大可能的读者数为实际的逻辑CPU数。写者是排他性的，一个读写锁同时只能有一个写者或多个读者（与CPU数相关），但不能同时既有读者又有写者。</code></pre><h5 id="读写锁原语"><a href="#读写锁原语" class="headerlink" title="读写锁原语"></a>读写锁原语</h5><pre><code>// 初始化读写锁int pthread_rwlock_init(pthread_rwlock_t *restrict rwlock, const pthread_rwlockattr_t *restrict attr)// 销毁读写锁int pthread_rwlock_destroy(pthread_rwlock_t *rwlock)// 获取读锁int pthread_rwlock_rdlock(pthread_rwlock_t *rwlock)// 获取写锁int pthread_rwlock_wrlock(pthread_rwlock_t *rwlock)// 解锁（读锁后调用解读锁，写锁同样）int pthread_rwlock_unlock(pthread_rwlock_t *rwlock)// 尝试拿读锁，非阻塞int pthread_rwlock_tryrdlock(pthread_rwlock_t *rwlock)// 尝试拿写锁，非阻塞int pthread_rwlock_trywrlock(pthread_rwlock_t *rwlock)</code></pre><p>读写锁机制：读共享，写独占</p><h5 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h5><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;/*    读写锁    读时共享    写时独占*/int counter;// 定义读写锁pthread_rwlock_t rwlock;// 读线程，读共享void *th_read(void *arg){    int i = 100;    while(i--){        // 获取读锁        pthread_rwlock_rdlock(&amp;rwlock);        printf(&quot;read: %x, %d \n&quot;,pthread_self(),counter);        // 解除读锁        pthread_rwlock_unlock(&amp;rwlock);        usleep(100);    }}// 写线程，写独占void *th_write(void *arg){    int t;    int i = 100;    while(i--){        // 获取写锁        pthread_rwlock_wrlock(&amp;rwlock);        t = counter;        // usleep(100);        printf(&quot;write: %x, counter=%d, ++counter=%d \n&quot;,pthread_self(), t,++counter );        // 解除写锁        pthread_rwlock_unlock(&amp;rwlock);        usleep(100);    }}int main(int argc, char *argv[]){    pthread_t tid[8];    // 初始化读写锁    pthread_rwlock_init(&amp;rwlock,NULL);    // 3线程写    for (int i = 0; i &lt; 3; ++i)    {        pthread_create(&amp;tid[i],NULL,th_write,NULL);    }    // 5线程读    for (int i = 0; i &lt; 5; ++i)    {        pthread_create(&amp;tid[i+3],NULL,th_read,NULL);    }    // 销毁读写锁    pthread_rwlock_destroy(&amp;rwlock);    for (int i = 0; i &lt; 8; ++i)    {        /* code */        // 回收线程资源        pthread_join(tid[i],NULL);    }    return 0;}</code></pre><h4 id="条件变量-pthread-cond-t"><a href="#条件变量-pthread-cond-t" class="headerlink" title="条件变量 pthread_cond_t"></a>条件变量 pthread_cond_t</h4><p>条件变量是利用线程间共享全局变量进行同步的一种机制。条件变量上的基本操作有：触发条件(当条件变为 true 时)；等待条件，挂起线程直到其他线程触发条件。</p><p>条件变量一般和互斥量同步使用，其给多个线程提供了一个汇合的场所。例如典型的生产者和消费者，条件变量作为有产品的先决条件，而产品本身就可以看作互斥量，因为消费者也可能是多个线程同时争抢生产者生产出来的产品。</p><h5 id="条件变量原语"><a href="#条件变量原语" class="headerlink" title="条件变量原语"></a>条件变量原语</h5><p>初始化条件变量<br>　　<br>        int pthread_cond_init(pthread_cond_t <em>cond,pthread_condattr_t </em>cond_attr)</p><pre><code>尽管POSIX标准中为条件变量定义了属性，但在Linux中没有实现，因此cond_attr值通常为NULL，且被忽略。</code></pre><p>有两个等待函数 </p><pre><code>// 无条件等待int pthread_cond_wait(pthread_cond_t *cond,pthread_mutex_t *mutex)// 超时等待int pthread_cond_timewait(pthread_cond_t *cond,pthread_mutex *mutex,const timespec *abstime);如果在给定时刻前条件没有满足，则返回ETIMEOUT，结束等待，其中abstime以与time()系统调用相同意义的绝对时间形式出现，0表示格林尼治时间1970年1月1日0时0分0秒。</code></pre><p>　<br>无论哪种等待方式，都必须和一个互斥锁配合，以防止多个线程同时请求。mutex互斥锁在调用pthread_cond_wait()前必须由本线程加锁（pthread_mutex_lock()），而在更新条件等待队列以前，mutex保持锁定状态，并在线程挂起进入等待前解锁。在条件满足从而离开pthread_cond_wait()之前，mutex将被重新加锁，以与进入pthread_cond_wait()前的加锁动作对应。</p><p>激发条件</p><pre><code>// 激活一个等待该条件的线程（存在多个等待线程时按入队顺序激活其中一个）　　int pthread_cond_signal(pthread_cond_t *cond);// 激活所有等待线程int pthread_cond_broadcast(pthread_cond_t *cond); </code></pre><p>销毁条件变量</p><pre><code>int pthread_cond_destroy(pthread_cond_t *cond);只有在没有线程在该条件变量上等待的时候才能销毁这个条件变量，否则返回EBUSY</code></pre><p>说明：</p><ol><li><p>pthread_cond_wait 自动解锁互斥量(如同执行了pthread_unlock_mutex)，并等待条件变量触发。这时线程挂起，不占用CPU时间，直到条件变量被触发（变量为ture）。在调用 pthread_cond_wait之前，应用程序必须加锁互斥量。pthread_cond_wait函数返回前，自动重新对互斥量加锁(如同执行了pthread_lock_mutex)。</p></li><li><p>互斥量的解锁和在条件变量上挂起都是自动进行的。因此，在条件变量被触发前，如果所有的线程都要对互斥量加锁，这种机制可保证在线程加锁互斥量和进入等待条件变量期间，条件变量不被触发。条件变量要和互斥量相联结，以避免出现条件竞争——个线程预备等待一个条件变量，当它在真正进入等待之前，另一个线程恰好触发了该条件（条件满足信号有可能在测试条件和调用pthread_cond_wait函数（block）之间被发出，从而造成无限制的等待）。</p></li><li><p>条件变量函数不是异步信号安全的，不应当在信号处理程序中进行调用。特别要注意，如果在信号处理程序中调用 pthread_cond_signal 或 pthread_cond_boardcast 函数，可能导致调用线程死锁</p></li></ol><p><a href="https://blog.csdn.net/jkx01whg/article/details/78119189" target="_blank" rel="noopener">参考链接</a></p><h5 id="代码实例-1"><a href="#代码实例-1" class="headerlink" title="代码实例"></a>代码实例</h5><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;/*    条件变量    条件变量给多个线程提供了一个汇合的场所    生产与消费模式*/// 产品信息struct msg{    struct msg *next;    int num;};struct msg *head;// 存在生产条件变量pthread_cond_t has_product = PTHREAD_COND_INITIALIZER;// 互斥量pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;;// 消费者void *consumer(void *arg){    struct msg *msg;    for(;;){        // 先加锁，访问共享资源head，如果为空，则进入等待条件变量        pthread_mutex_lock(&amp;lock);        // 阻塞等待生产条件变量存在，此时还是加锁状态        while(head == NULL)            // 进入阻塞等待条件变量，此时线程挂起进入等待，锁被解开，生产者可以获取并生产            pthread_cond_wait(&amp;has_product,&amp;lock);        msg = head;        head = msg-&gt;next;        // 获取产品信息后，解锁以避免别的线程争抢资源        pthread_mutex_unlock(&amp;lock);        printf(&quot;Consume %d \n&quot;,msg-&gt;num);        free(msg);        sleep(rand()%5);    }}// 生产者void *producer(void *arg){    struct msg *msg;    for (;;)    {        // 新建产品内容        msg = malloc(sizeof(struct msg));        msg-&gt;num = rand() %1000 + 1;        printf(&quot;Produce %d \n&quot;,msg-&gt;num);        // 上锁，生产产品        pthread_mutex_lock(&amp;lock);        msg-&gt;next = head;        head = msg;        // 解锁，生产完成        pthread_mutex_unlock(&amp;lock);        // 传递生产条件变量到消费者，进行条件变量通知        pthread_cond_signal(&amp;has_product);        sleep(rand() % 5);    }}int main(int argc, char *argv[]){    pthread_t tidA,tidB;    // 利用系统时间产生随机数种子值，生产随机数    srand(time(NULL));    // 创建两个线程，测试消费和生产之间资源传递关系    pthread_create(&amp;tidA,NULL,producer,NULL);    pthread_create(&amp;tidB,NULL,consumer,NULL);    // 回收线程资源    pthread_join(tidA,NULL);    pthread_join(tidB,NULL);    return 0;}</code></pre><p>在消费者中，需要注意一下情况：</p><ol><li>消费者在消费之前，需要上锁获取共享资源生产的产品，此时产品在消费者线程中处于被锁状态，生产者此时无法获取，只能阻塞。当消费者监测无生产产品是，死循环等待条件变量 pthread_cond_wait 的产生。</li><li>pthread_cond_wait 进入之前，原语保证必须本线程处于锁状态中，此时线程进入挂起等待时刻，锁被解开，其他线程可以访问生产产品。</li><li>当生产者拿到锁生产处产品后，在调用 pthread_cond_signal 通知条件变量变化时，必须解锁共享产品资源，因为一旦通知后，消费者线程中就立即对生产产品的锁状态进行恢复，以确保代码往下继续进行。</li></ol><h4 id="信号量-sem-t"><a href="#信号量-sem-t" class="headerlink" title="信号量 sem_t"></a>信号量 sem_t</h4><p>信号量原理上等同于多个互斥量，同时提供多把锁和钥匙，丰富多线程下共享资源访问的并发行。</p><h5 id="信号量原语"><a href="#信号量原语" class="headerlink" title="信号量原语"></a>信号量原语</h5><p>头文件</p><pre><code>#include &lt;semaphore.h&gt;</code></pre><p>初始化信号量</p><pre><code>int sem_init (sem_t *sem , int pshared, unsigned int value)sem - 指定要初始化的信号量pshared - 信号量 sem 的共享选项，linux只支持0，表示它是当前进程的局部信号量value - 信号量 sem 的初始值</code></pre><p>信号量值加1</p><pre><code>int sem_post(sem_t *sem)给参数sem指定的信号量值加1</code></pre><p>信号量值减1</p><pre><code>int sem_wait(sem_t *sem)给参数sem指定的信号量值减1，如果sem所指的信号量的数值为0，函数将会阻塞等待直到有其它线程使它不再是0为止</code></pre><p>销毁信号量</p><pre><code>int sem_destroy(sem_t *sem)销毁指定的信号量</code></pre><h5 id="代码实例-2"><a href="#代码实例-2" class="headerlink" title="代码实例"></a>代码实例</h5><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;#include &lt;semaphore.h&gt;/*    信号量，互斥量的升级版，同时多把锁*/// 数据内容大小#define NUM 5// 队列　５int queue[NUM];// 信号量sem_t blank_number, product_number;// 生产者void *producer(void *arg){    int n = 0;    while(1){        //　取锁，blank_number内锁自减一；为０则阻塞等待        sem_wait(&amp;blank_number);        queue[n] = rand() % 1000 + 1;        printf(&quot;Produce %d num:%d \n&quot;,n,queue[n]);        // 释放锁，blank_number内锁自加一        sem_post(&amp;product_number);        // 队列数据补足，形成环形队列        n = (n+1) % NUM;        sleep(rand() % 5);    }}// 消费者void *consumer(void *arg){    int n = 0;    while(1){        sem_wait(&amp;product_number);        printf(&quot;Consume %d num: %d\n&quot;,n,queue[n]);        queue[n] = 0;        sem_post(&amp;blank_number);        n = (n+1) % NUM;        sleep(rand() % 5);    }}int main(int argc, char *argv[]){    pthread_t tidA, tidB;    // 初始化信号量，    // blank_number: 信号量数值５, 0等同于互斥量（存在一个锁）    // 中间参数: ０ 代表的是局部线程间共享信号量    sem_init(&amp;blank_number, 0, NUM);    sem_init(&amp;product_number, 0, 0);    // 两个线程间调度生产与消费内容    pthread_create(&amp;tidA, NULL, producer, NULL);    pthread_create(&amp;tidB, NULL, consumer, NULL);    // 回收线程资源    pthread_join(tidA, NULL);    pthread_join(tidB, NULL);    // 销毁信号量    sem_destroy(&amp;blank_number);    sem_destroy(&amp;product_number);    return 0;}</code></pre><p>相关解释：</p><ol><li>生产者持有的信号量5个，容器数组正好有5个缓冲区。每次生产调用sem_wait，信号量值自减少1（为0阻塞等待），生产完成调用sem_post释放。也就是同时可以5个生产者并发生产。</li><li>消费者对于数据的读取串行处理，其信号量值为0，等同于互斥量。数组内容一次读取，通过取余数进行矫正位。</li><li>消费者中对于信号量（初始值0，相当于互斥量，只能进入一次）首次调用sem_wait进入处理，此时信号量为0，其他线程再次获取及阻塞等待，获取数组内数据后，调用sem_post进行释放。</li></ol><h4 id="进程间锁-pthread-mutex-attr-t"><a href="#进程间锁-pthread-mutex-attr-t" class="headerlink" title="进程间锁 pthread_mutex_attr_t"></a>进程间锁 pthread_mutex_attr_t</h4><p>进程间共享内存互斥量，可以通过 pthread_mutex_attr_t 来配置。通过 pthread_mutex_attr_t 可以让进程之间访问共享的互斥量，从而进行资源的有序调度。</p><h5 id="共享互斥量操作原语"><a href="#共享互斥量操作原语" class="headerlink" title="共享互斥量操作原语"></a>共享互斥量操作原语</h5><p>初始化共享互斥量</p><pre><code>pthread_mutexattr_init(pthread_mutex_attr_t *attr)</code></pre><p>操作共享互斥量属性</p><pre><code>// 获得共享互斥量属性，由shared带出intpthread_mutexattr_getpshared(const pthread_mutexattr_t *restrictattr, int *restrictshared );// 设置共享互斥属性，有shard决定，默认为PTHREAD_PROCESS_PRIVATE，即线程共享intpthread_mutexattrattr_ setpshared (  constpthread_mutexattr_t *restrict attr,int pshared);若成功返回0，若失败返回错误编号。注意：shared的取值可以是        PTHREAD_PROCESS_SHARED  存在共享内存中，可以被多个进程中的线程共享        PTHREAD_PROCESS_PRIVATE 只有和创建这个互斥锁的线程在同一个进程中的线程才能访问这个互斥锁</code></pre><p>销毁共享互斥量</p><pre><code>pthread_mutex_attr_t_destroy(pthread_mutex_attr_t *attr)</code></pre><h5 id="代码实例-3"><a href="#代码实例-3" class="headerlink" title="代码实例"></a>代码实例</h5><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/mman.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;string.h&gt;#include &lt;fcntl.h&gt;/*    自定义进程间锁结构体*/struct mutex_lock{    int num;    // 互斥量    pthread_mutex_t mutex;    // 共享互斥量属性    pthread_mutexattr_t mutexattr;};int main(void){    int fd ,i;    struct mutex_lock *mlock;    pid_t pid;    fd = open(&quot;temp&quot;,O_CREAT | O_RDWR, 0777);    // 拓展文件大小    ftruncate(fd,sizeof(*mlock));    // 内存映射，进程间读写数据    mlock = mmap(NULL,sizeof(*mlock), PROT_READ | PROT_WRITE, MAP_SHARED, fd,0);    close(fd);    // 清空内存    memset(mlock,0,sizeof(*mutex_lock));    // 初始化进程间锁属性    pthread_mutexattr_init(&amp;mlock-&gt;mutexattr);    // 设置进程间锁为进程间共享（默认为线程共享）    pthread_mutexattr_setpshared(&amp;mlock-&gt;mutexattr, PTHREAD_PROCESS_SHARED);    // 初始化锁    pthread_mutex_init(&amp;mlock-&gt;mutex,&amp;mlock-&gt;mutexattr);    pid = fork();    // 通过父子不同进程，共同访问共享互斥量中 num 数据，对其进行修改    if(pid == 0){        // 子进程，临界区内执行10次 +1 操作        for (int i = 0; i &lt; 10; ++i)        {            // 取锁            pthread_mutex_lock(&amp;mlock-&gt;mutex);            (mlock-&gt;num)++;            printf(&quot;Child process num: %d\n&quot;,mlock-&gt;num);            // 释放锁            pthread_mutex_unlock(&amp;mlock-&gt;mutex);            sleep(1);        }    }    else {        for (int i = 0; i &lt; 10; ++i)        {            // 父进程，临界区内执行10次 +2 操作            pthread_mutex_lock(&amp;mlock-&gt;mutex);            mlock-&gt;num += 2;            printf(&quot;Parent process num: %d\n&quot;,mlock-&gt;num);            pthread_mutex_unlock(&amp;mlock-&gt;mutex);            sleep(1);        }        // 主线程阻塞等待子线程处理完毕        wait(NULL);    }    // 释放锁资源    pthread_mutexattr_destroy(&amp;mlock-&gt;mutexattr);    pthread_mutex_destroy(&amp;mlock-&gt;mutex);    // 解除内存文件映射    munmap(mlock,sizeof(*mlock));    // 删除临时文件    unlink(&quot;temp&quot;);    return 0;}</code></pre><p>代码说明：</p><ol><li>自定义结构体 mutex_lock 提供不同进程访问的数据基础。</li><li>初始化共享互斥量时候，通过内存映射将 mutex_lock 结构体置位进程共享（MAP_SHARED），同时其内部互斥量属性设置为进程共享（PTHREAD_PROCESS_SHARED）</li><li>父子进程访问结构体 mutex_lock 中的 num 时，都进行上锁，修改数据，解锁等操作，其互斥量能够进程间访问，依赖于之前内存映射以及互斥量属性中的都进行了进程共享属性设置。</li></ol><h4 id="文件锁-fcntl"><a href="#文件锁-fcntl" class="headerlink" title="文件锁 fcntl"></a>文件锁 fcntl</h4><p>文件锁是用于解决资源的共享使用的一种机制：当多个用户需要共享一个文件时，Linux通常采用的方法是给文件上锁，来避免共享的资源产生竞争的状态。</p><p>之前在文件I/O章节中介绍过这个函数，当时只是提到了如何使用 fcntl 进行对一大文件描述符进行权限位的更改，现在我们来了解如何通过 fcntl 对文件进行加锁访问。</p><p>文件锁机制中同样采用的是读共享，写排斥。</p><p>再了解 fcntl 函数之前，先来了解一下结构体 flock</p><pre><code>struct flock {... short l_type; // 文件锁操作类型: F_RDLCK（读锁）,F_WRLCK（写锁）, F_UNLCK（解锁）short l_whence; // 上锁内容的规定: SEEK_SET（开始）, SEEK_CUR（游标当前）, SEEK_END（结尾 off_t l_start; // 文件锁内容开始位置off_t l_len; // 文件锁区域长度，0代表全部 pid_t l_pid; //获取上锁进程pid（仅对F_GETLK有效）...        }; </code></pre><h5 id="fcntl-原语"><a href="#fcntl-原语" class="headerlink" title="fcntl 原语"></a>fcntl 原语</h5><pre><code>#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;int fcntl(int fd, int cmd, ... /* arg */ )例如： int ret = fcntl(fd, F_SETLKW, &amp;lock);</code></pre><p>cmd命令解释： </p><ol><li>F_SETLK：设置锁（读锁F_RDLCK，写锁F_WRLCK）或者释放所（F_UNLCK），如果无法获取，直接返回error。</li><li>F_SETLKW：功能和F_SETLK一样，区别是阻塞等待</li><li>F_GETLK：这个接口是获取锁的相关信息：这个接口会修改我们传入的struct flock</li></ol><p>通过函数参数功能可以看出 fcntl(相对于lockf)是功能最强大的，它既支持共享锁又支持排他锁，即可以锁住整个文件，又能只锁文件的某一部分。</p><p>操作方式，通过结构体指定锁文件内容区域以及锁方式，然后通过命令 fcntl 将结构体锁属性提交到指定文件中。</p><h5 id="实例代码-1"><a href="#实例代码-1" class="headerlink" title="实例代码"></a>实例代码</h5><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;/*    文件锁    通过结构体 struct flock 来设置文件锁的具体属性，通过fcntl提交文件锁到文件    读锁共享，写锁互斥*/void sys_err(char *err){    perror(err);    exit(1);}int main(int argc, char *argv[]){    int fd;    struct flock f_lock;    if(argc &lt; 2){        printf(&quot; ./app must need filename\n&quot;);        exit(1);    }    if((fd = open(argv[1],O_RDWR))&lt;0){        sys_err(&quot;open&quot;);    }    // 写入文件锁，互斥    f_lock.l_type = F_WRLCK;    // 读文件锁，共享    // f_lock.l_type = F_RDLCK;    // 文件锁的游标指针    f_lock.l_whence = SEEK_SET;    // 文件锁的锁区开始位置    f_lock.l_start = 0;    // 文件锁锁区的大小，0代表全部锁住    f_lock.l_len = 0;    // 提交文件锁属性到文件    fcntl(fd,F_SETLKW, &amp;f_lock);    printf(&quot;lock file! \n&quot;);    sleep(30);    // 解除文件锁    f_lock.l_type = F_UNLCK;    fcntl(fd,F_SETLKW, &amp;f_lock);    printf(&quot;unlock flile! \n&quot;);    close(fd);    return 0;}</code></pre><p>以上代码可以采用以下方式测试：</p><ol><li><p>读共享</p><p> 注释 f_lock.l_type = F_WRLCK;<br> 打开 f_lock.l_type = F_RDLCK;<br> 新建txt文件到程序目录下，其内容自定义<br> 运行程序 ./app <em>.txt<br> 程序运行期间（30s），新建并运行程序对 </em>.txt 进行内容读取，测试是否成功</p></li><li><p>写排斥</p><p> 打开 f_lock.l_type = F_WRLCK;<br> 注释 f_lock.l_type = F_RDLCK;<br> 新建txt文件到程序目录下，其内容自定义<br> 运行程序 ./app <em>.txt<br> 程序运行期间（30s），新建并运行程序对 </em>.txt 进行内容修改，测试是否成功</p></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本博文主要介绍了如何对多线程下产生各种资源同步问题应对方式，主要集中介绍了互斥量，条件变量，信号量，文件锁以及进程间共享锁集中处理方式。</p><p>从上面的分析中，我们可以得出：</p><ol><li>单一的线程同步可以产用互斥量，条件变量和互斥量一同使用，可以控制共享资源的访问序列。</li><li>信号量机制提供了多把锁，进一步提升多线程下共享资源访问的并发性。</li><li>对于进程间同步，可以采用共享互斥量方式，通过设置互斥量属性实现，其机制上仍需要共享内存的帮助。</li><li>文件锁可以对文件进行锁同步，机制上依旧是读共享，写排斥方式。</li></ol><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;本章节着重介绍多线程下资源同步的问题，之前章节介绍了Linux多线程编程的有关知识，由于多线程下容易产生资源互抢情况，所以必须对共享资源进行统一规划，其涉及到的概念有互斥量，条件变量，信号量，文件锁以及进程间锁。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="线程同步 锁机制" scheme="https://891904833.gitee.io/FuckCode/tags/%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5-%E9%94%81%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之线程</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/14/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/14/Linux系统编程之线程/</id>
    <published>2019-01-14T08:57:52.000Z</published>
    <updated>2019-01-14T09:58:05.221Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本章节介绍Linux下多线程编程方面的知识，涉及到线程和进程的关系，优缺点，线程基础背景，线程原语以及线程属性等方面内容，后续还会进阶的介绍多线程而引申出来的线程同步问题。让我们更加清晰的了解到多线程编程带来的魅力。</strong><br><a id="more"></a></p><h3 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h3><h4 id="线程与进程"><a href="#线程与进程" class="headerlink" title="线程与进程"></a>线程与进程</h4><p>典型的UNIX/Linux进程可以看成只有一个控制线程：一个进程在同一时刻只做一件事情。有了多个控制线程后，在程序设计时可以把进程设计成在同一时刻做不止一件事，每个线程各自处理独立的任务。　　</p><p>进程是程序执行时的一个实例，是担当分配系统资源（CPU时间、内存等）的基本单位。在面向线程设计的系统中，进程本身不是基本运行单位，而是线程的容器。程序本身只是指令、数据及其组织形式的描述，进程才是程序（那些指令和数据）的真正运行实例。</p><p>线程是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。线程包含了表示进程内执行环境必须的信息，其中包括进程中表示线程的线程ID、一组寄存器值、栈、调度优先级和策略、信号屏蔽字、errno常量以及线程私有数据。进程的所有信息对该进程的所有线程都是共享的，包括可执行的程序文本、程序的全局内存和堆内存、栈以及文件描述符。在Unix和类Unix操作系统中线程也被称为轻量级进程<br>（lightweight processes），但轻量级进程更多指的是内核线程（kernel thread），而把用户线程（user thread）称为线程。</p><p>进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。</p><h5 id="线程间共享资源"><a href="#线程间共享资源" class="headerlink" title="线程间共享资源"></a>线程间共享资源</h5><p>线程几乎共享进程中的任何资源，因为都是同属一个容器内。具体罗列如下：</p><ol><li>文件描述符表 </li><li>每种信号的处理方式 </li><li>当前工作目录 </li><li>用户ID和组ID </li><li>内存地址空间</li><li>代码段中的 Text data bss 堆 共享库</li></ol><img src="/FuckCode/2019/01/14/Linux系统编程之线程/线程共享资源.png" class="线程共享资源"><h5 id="线程间非共享资源"><a href="#线程间非共享资源" class="headerlink" title="线程间非共享资源"></a>线程间非共享资源</h5><p>但是线程间也有不共享的资源，如：</p><ol><li>线程id </li><li>处理器现场和栈指针(内核栈) </li><li>独立的栈空间(用户空间栈) </li><li>errno变量</li><li>信号屏蔽字</li><li>调度优先级</li></ol><h5 id="线程与进程有缺点"><a href="#线程与进程有缺点" class="headerlink" title="线程与进程有缺点"></a>线程与进程有缺点</h5><p>使用多线程的理由之一是和进程相比，它是一种非常”节俭”的多任务操作方式。我们知道，在Linux系统下，启动一个新的进程必须分配给它独立的地址空间，建立众多的数据表来维护它的代码段、堆栈段和数据段，这是一种”昂贵”的多任务工作方式。而运行于一个进程中的多个线程，它们彼此之间使用相同的地址空间，共享大部分数据，启动一个线程所花费的空间远远小于启动一个进程所花费的空间，而且，线程间彼此切换所需的时间也远远小于进程间切换所需要的时间。据统计，总的说来，一个进程的开销大约是一个线程开销的30倍左右，当然，在具体的系统上，这个数据可能会有较大的区别。</p><p>使用多线程的理由之二是线程间方便的通信机制。对不同进程来说，它们具有独立的数据空间，要进行数据的传递只能通过通信的方式进行，这种方式不仅费时，而且很不方便。线程则不然，由于同一进程下的线程之间共享数据空间，所以一个线程的数据可以直接为其它线程所用，这不仅快捷，而且方便。当然，数据的共享也带来其他一些问题，有的变量不能同时被两个线程所修改，有的子程序中声明为static的数据更有可能给多线程程序带来灾难性的打击，这些正是编写多线程程序时最需要注意的地方。</p><p>优点：</p><pre><code>提高程序的并发性，提高应用程序响应开销小，不用重新分配内存通信、共享数据方便使多CPU系统更加有效，操作系统会保证当线程数不大于CPU数目时，不同的线程运行于不同的CPU上改善程序结构。一个既长又复杂的进程可以考虑分为多个线程，成为几个独立或半独立的运行部分</code></pre><p>缺点：</p><pre><code>线程不如进程来的稳定（库实现）多进程下调试困难（gdb支持不好）无法使用unix经典事件，信号（线程下信号处理共享）</code></pre><h5 id="线程库安装"><a href="#线程库安装" class="headerlink" title="线程库安装"></a>线程库安装</h5><p>Linux下的线程实现基于库的方式，因此使用编译过程中都需要额外指定编译库 -lpthread</p><ol><li><p>查看 pthread 函数 Manpage</p><p> man -k pthread</p></li><li><p>安装 pthread 相关的 Manpage</p><p> sudo apt-get install manpages-posix manpages-posix-dev</p></li></ol><h4 id="线程原语"><a href="#线程原语" class="headerlink" title="线程原语"></a>线程原语</h4><p>一下介绍多线程编程中涉及的线程原语。</p><h5 id="pthread-create"><a href="#pthread-create" class="headerlink" title="pthread_create"></a>pthread_create</h5><pre><code>#include &lt;pthread.h&gt;int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine)(void *), void *arg)pthread_t *thread： 传递一个pthread_t变量地址，用于保存新线程的tidpthread_attr_t *attr： 线程属性设置，默认为 NULLvoid *(*start_routine)(void *)： 函数指针，新线程加载后执行的函数模块void *arg：  加载函数调用的参数返回值： 成功返回0，失败返回错误号；（注意其他函数失败返回 -1， 设置errno，这里的pthread时库函数实现，返回值返回错误号，其线程中存在errno是为了兼容其他函接口而提供，pthread并不使用他）</code></pre><p>编译时，添加控制项 -lpthread</p><p>在一个线程中调用pthread_create()创建新的线程后，当前线程从pthread_create() 返回继续往下执行，而新的线程所执行的代码由我们传给pthread_create的函数指针 start_routine 决定。start_routine 函数接收一个参数，是通过 pthread_create 的 arg 参数传递给它的，该参数的类型为void <em>，这个指针按什么类型解释由调用者自己定义。start_routine的返回值类型也是void </em>，这个指针的含义同样由调用者自己定义。start_routine 返回时，这个线程就退出了，其它线程可以调用pthread_join得到start_routine的返回值，类似于父进程调用wait(2)得到子进程的退出状态。</p><p>pthread_create成功返回后，新创建的线程的id被填写到thread参数所指向的内存单元。我们知道进程id的类型是pid_t，每个进程的id在整个系统中是唯一的，调用getpid(2) 可以获得当前进程的id，是一个正整数值。线程id的类型是thread_t，它只在当前进程中保证是唯一的，在不同的系统中thread_t这个类型有不同的实现，它可能是一个整数值， 也可能是一个结构体，也可能是一个地址，所以不能简单地当成整数用printf打印，调用 pthread_self(3)可以获得当前线程的id。</p><p>代码实例：<br>测试进程可以创建多少个线程</p><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;void *pthread_fun(void *arg){    while(1){        sleep(1);    }}int main(void){    pthread_t tid;    int i =1,err;    while(1){        err = pthread_create(&amp;tid, NULL,pthread_fun,NULL);        if(err != 0){            printf(&quot;%s \n&quot;, strerror(err));            exit(1);        }        printf(&quot;%x   %d\n&quot;,tid,i++);    }    return 0;}</code></pre><h5 id="pthread-self"><a href="#pthread-self" class="headerlink" title="pthread_self"></a>pthread_self</h5><p>获取调用线程tid</p><pre><code>pthread_t pthread_self(void)</code></pre><p>实例测试：</p><pre><code>#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;// 线程家在函数，必须规定：返回值 void *， 参数 void *void* pthread_fun(void* arg){    printf(&quot;pthread pid: %d\n&quot;, getpid());    printf(&quot;pthread id: %x\n&quot;, (unsigned int)pthread_self());    printf(&quot;pthread arg: %d\n&quot;,(int *)arg);}int main(void){    pthread_t ptid;    int n = 10;    int err;    if((err = pthread_create(&amp;ptid, NULL, pthread_fun, (void *)n))!=0){        fprintf(stderr,&quot;can not create thread: %s\n&quot;,strerror(err));        exit(1);    }    printf(&quot;main return ptid: %x\n&quot;, (unsigned int)ptid);    printf(&quot;main pid: %d\n&quot;, getpid());    printf(&quot;main thread id: %x\n&quot;, pthread_self());    // 等待1s，子线程处理    sleep(1);}</code></pre><p>由于pthread_create 的错误码不保存在 errno 中，因此不能直接用 perror(3) 打印错误信 息，可以先用 strerror(3) 把错误码转换成错误信息再打印。</p><p>如果任意一个线程调用了 exit 或 _exit ，则整个进程的所有线程都终止，由于从 main 函数 return 也相当于调用 exit，为了防止新创建的线程还没有得到执行就终止，我们在main函数return之前延时1秒，这只是一种权宜之计，即使主线程等待1秒，内核也不一定会调度新创建的线程执行。</p><h5 id="pthread-exit"><a href="#pthread-exit" class="headerlink" title="pthread_exit"></a>pthread_exit</h5><p>调用线程退出函数，注意和 exit 函数的区别，任何线程里 exit 导致进程退出，其他线程未工作结束，主控线程退出时不能 return 或 exit 。</p><pre><code>void pthread_exit(void *value_ptr)void *value_ptr： 线程退出时传出的参数，可以指值或者地址，地址不可以是线程内部申请的局部地址</code></pre><p>需要注意，pthread_exit 或者 return 返回的指针所指向的内存单元必须是全局的或者是用malloc分配的，不能在线程函数的栈上分配，因为当其它线程得到这个返回指针时线程函数已经退出了。</p><h5 id="pthread-join"><a href="#pthread-join" class="headerlink" title="pthread_join"></a>pthread_join</h5><p>将线程挂起，等待处理结束。</p><pre><code>int pthread_join(pthread_t thread, void **value_ptr)pthread_t thread： 需要挂起监测的线程void **value_ptr： 存放线程中止状态值</code></pre><p>调用该函数的线程将挂起等待，直到 id 为 thread 的线程终止。thread 线程以不同的方法终止，通过 pthread_join 得到的终止状态是不同的，总结如下:</p><ol><li>如果 thread 线程通过 return 返回， retval 所指向的单元里存放的是 thread 线程函数的返回值。</li><li>如果 thread 线程被别的线程调用 pthread_cancel 异常终止掉，retval 所指向的单元里存放的是常数 PTHREAD_CANCELED。</li><li>如果 thread 线程是自己调用 pthread_exit 终止的，retval 所指向的单元存放的是传给 pthread_exit 的参数。</li><li>如果对 thread 线程的终止状态不感兴趣，可以传 NULL 给 retval 参数。</li></ol><h5 id="pthread-cancel"><a href="#pthread-cancel" class="headerlink" title="pthread_cancel"></a>pthread_cancel</h5><p>在进程内某个线程可以取消另一个线程。</p><pre><code>int pthread_cancel(pthread_t thread)pthread_t thread： 被取消的线程退出值，定义在Linux的pthread库中，常数 PTHREAD_CANCELED 的值是 -1</code></pre><p>代码实例：<br>通过 return，pthread_exit，pthread_cancel分别控制线程结束</p><pre><code>#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;// 正常return方式返回void* pthread_fun1(void* arg){    printf(&quot;thread 1 return \n&quot;);    return (void*)1;}// 通过pthread_exit方式返回void* pthread_fun2(void* arg){    printf(&quot;thread 2 pthread_exit \n&quot;);    pthread_exit((void*)2);}// 通过其他线程调用pthread_cancel返回void* pthread_fun3(void* arg){    while(1){        printf(&quot;thread 3 do while - sleep\n&quot;);        sleep(1);    }}    int main(void){    pthread_t tid;    void* tret;    pthread_create(&amp;tid, NULL, pthread_fun1, NULL);    pthread_join(tid,&amp;tret);    printf(&quot;thread 1 exit code with return %d\n&quot;,(int)tret);    pthread_create(&amp;tid, NULL, pthread_fun2, NULL);    pthread_join(tid,&amp;tret);    printf(&quot;thread 2 exit code with pthread_exit %d\n&quot;,(int)tret);    pthread_create(&amp;tid, NULL, pthread_fun3, NULL);    sleep(3);    pthread_cancel(tid);    pthread_join(tid,&amp;tret);    printf(&quot;thread 3 exit code with pthread_cancel %d\n&quot;,(int)tret);    return 0;}</code></pre><h5 id="pthread-detach"><a href="#pthread-detach" class="headerlink" title="pthread_detach"></a>pthread_detach</h5><p>一般情况下，线程终止后，其终止状态一直保留到其它线程调用 pthread_join 获取它的状态为止。但是线程也可以被置为detach状态，这样的线程一旦终止就立刻回收它占用的所有资源，而不保留终止状态。</p><pre><code>int pthread_detach(pthread_t thread)pthread_t thread： 需要分离态的线程id</code></pre><p>不能对一个已经处于detach状态的线程调用 pthread_join，这样的调用将返回 EINVAL。如果已经对一个线程调用了 pthread_detach 就不能再调用 pthread_join（互斥） 了。</p><p>如果子线程的资源需要主线程来回收的话，那么主线程就一定要等子线程结束，因为子线程结束，你就不能去回收；但是如果子线程资源要自动回收的话，那么主线程就不必等了。</p><p>代码实例：</p><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;#include &lt;string.h&gt;void *pthread_fun(void *arg){    int i = 3;    while(i--){        printf(&quot;thread count %d\n&quot;, i);        sleep(1);    }    return (void *)1;}int main(void){    pthread_t tid;    void *tret;    int err;    pthread_create(&amp;tid, NULL, pthread_fun, NULL);    // 分两次运行，一次打开以下代码编译运行，一次注释代码运行，查看结果    pthread_detach(tid);    // 已分离态，禁止调用 pthread_join    // pthread_join(tid,NULL);    while(1){        err = pthread_join(tid, &amp;tret);        if(err != 0){            fprintf(stderr,&quot;thread %s \n&quot;, strerror(err));        }else{            fprintf(stderr, &quot;thread exit code %d\n&quot;,(int)tret);        }        sleep(1);    }    return 0;}</code></pre><p>上述代码，第一次使用 pthread_detach 将线程置于分离态，此时 pthread_join 失败，走打印 strerror(err)；不使用 pthread_detach，等待子线程运行结束，打印输出正常运行结束返回值。</p><h5 id="pthread-equal"><a href="#pthread-equal" class="headerlink" title="pthread_equal"></a>pthread_equal</h5><p>比较两个线程是否相等</p><pre><code>int pthread_equal(pthread_t t1, pthread_t t2)pthread_t t1： 比较线程id 1pthread_t t2： 比较线程id 2 </code></pre><h4 id="线程终止方式"><a href="#线程终止方式" class="headerlink" title="线程终止方式"></a>线程终止方式</h4><p>如果需要只终止某个线程而不终止整个进程，可以有三种方法:</p><ol><li>从线程主函数 return。这种方法对主控线程不适用，从 main 函数 return 相当于调用 exit。</li><li>一个线程可以调用 pthread_cancel 终止同一进程中的另一个线程。 </li><li>线程可以调用 pthread_exit 终止自己。</li></ol><p>注意：同一个进程中，pthread_cancel 向另一个线程发送终止信号，系统并不会马上关闭被终止的线程，只有在终止线程下次系统调用时，才会真正的结束。或者调用 pthread_testcancel，让内核去检测是否需要取消当前线程。</p><h4 id="线程属性"><a href="#线程属性" class="headerlink" title="线程属性"></a>线程属性</h4><p>linux下线程的属性是可以根据实际项目需要进行设置，之前我们讨论的线程都是采用线程的默认属性，默认属性已经可以解决绝大多数开发时遇到的问题。如我们对程序的性能提出更高的要求那么需要设置线程属性，比如可以通过设置线程栈的大小来降低内存的使用，增加最大线程个数。</p><pre><code>typedef struct{    int                           detachstate;     线程的分离状态    int                          schedpolicy;   线程调度策略    struct sched_param      schedparam;   线程的调度参数    int                          inheritsched;    线程的继承性    int                          scope;          线程的作用域    size_t                      guardsize; 线程栈末尾的警戒缓冲区大小    int                          stackaddr_set;    void *                     stackaddr;      线程栈的位置    size_t                      stacksize;       线程栈的大小}pthread_attr_t;</code></pre><p>属性值不能直接设置，须使用相关函数进行操作，初始化的函数为 pthread_attr_init。这个函数必须在 pthread_create 函数之前调用。之后须用 pthread_attr_destroy 函数来释放资源。</p><p>线程属性主要包括如下属性:作用域(scope)、栈尺寸(stack size)、栈地址 (stack address)、优先级(priority)、分离的状态(detached state)、调度策略和参数(scheduling policy and parameters)。</p><p>默认的属性为非绑定、非分离、缺省M的堆栈、与父进程同样级别的优先级。</p><h5 id="线程属性初始化"><a href="#线程属性初始化" class="headerlink" title="线程属性初始化"></a>线程属性初始化</h5><p>先初始化线程属性，再 pthread_create 创建线程，最后 pthread_attr_destroy 回收属性。</p><pre><code>int pthread_attr_init(pthread_attr_t *attr)int pthread_attr_destroy(pthread_attr_t *attr)pthread_attr_t *attr： 线程属性结构体地址</code></pre><h5 id="线程的分离状态-detached-state"><a href="#线程的分离状态-detached-state" class="headerlink" title="线程的分离状态(detached state)"></a>线程的分离状态(detached state)</h5><p>线程的分离状态决定一个线程以什么样的方式来终止自己。</p><p>非分离状态: 线程的默认属性是非分离状态，这种情况下，原有的线程等待创建的线程结束。只有当 pthread_join 函数返回时，创建的线程才算终止，才能释放自己占用的系统资源。</p><p>分离状态: 分离线程没有被其他的线程所等待，自己运行结束了，线程也就终止了，马上释放系统资源。应该根据自己的需要，选择适当的分离状态。</p><p>int pthread_attr_setdetachstate(pthread_attr_t <em>attr, int detachstate)<br>int pthread_attr_getdetachstate(const pthread_attr_t </em>attr, int *detachstate)</p><p>pthread_attr_t *attr： 线程属性结构体地址<br>int detachstate： 分离 PTHREAD_CREATE_DETACHED，非分离状态 THREAD_CREATE_JOINABLE</p><p>这里要注意的一点是，如果设置一个线程为分离线程，而这个线程运行又非常快，它很可能在 pthread_create 函数返回之前就终止了，它终止以后就可能将线程号和系统资源移交给其他的线程使用，这样调用 pthread_create 的线程就得到了错误的线程号。要避免这种情况可以采取一定的同步措施，最简单的方法之一是可以在被创建的线程里调用 pthread_cond_timedwait 函数，让这个线程等待一会儿，留出足够的时间让函数 pthread_create 返回。设置一段等待时间，是在多线程编程里常用的方法。但是注意不要使用诸如wait 之类的函数，它们是使整个进程睡眠，并不能解决线程同步的问题。</p><p>代码实例：</p><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;#include &lt;string.h&gt;void *pthread_fun(void *arg){    int i = 30;    while(i--){        printf(&quot;%x   %d\n&quot;,pthread_self(),i);        sleep(1);    }    return (void *)1;}int main(void){    pthread_t tid;    pthread_attr_t attr; // attr里面保存的是垃圾值    int err;    pthread_attr_init(&amp;attr); // 设置线程属性    // int detachstate:     // PTHREAD_CREATE_DETACHED, PTHREAD_CREATE_JOINABLE    // 设置线程为分离状态    pthread_attr_setdetachstate(&amp;attr,PTHREAD_CREATE_DETACHED);    pthread_create(&amp;tid, &amp;attr,pthread_fun,NULL);    err = pthread_join(tid,NULL);    while(1){        if(err != 0){            printf(&quot;%s \n&quot;, strerror(err));            sleep(15);            pthread_exit((void *)1);        }    }    return 0;}</code></pre><h5 id="线程的栈地址-stack-address"><a href="#线程的栈地址-stack-address" class="headerlink" title="线程的栈地址(stack address)"></a>线程的栈地址(stack address)</h5><p>POSIX.1定义了两个常量 _POSIX_THREAD_ATTR_STACKADDR 和 _POSIX_THREAD_ATTR_STACKSIZE 检测系统是否支持栈属性。也可以给sysconf函数传递 _SC_THREAD_ATTR_STACKADDR或 _SC_THREAD_ATTR_STACKSIZE 来进行检测。</p><p>当进程栈地址空间不够用时，指定新建线程使用由 malloc 分配的空间作为自己的栈空间。通过 pthread_attr_setstackaddr 和 pthread_attr_getstackaddr 两个函数分别设置和获取线程的栈地址。传给 pthread_attr_setstackaddr 函数的地址是缓冲区的低地址(不一定是栈的开始地址，栈可能从高地址往低地址增长)。</p><pre><code>int pthread_attr_getstackaddr(const pthread_attr_t *attr, void **stackaddr)int pthread_attr_setstackaddr(pthread_attr_t *attr, void *stackaddr)pthread_attr_t *attr： 线程属性结构体*stackaddr： 获取的栈地址成功返回0，错误返回错误号</code></pre><p>此函数已过时，使用 pthread_attr_getstacksize 替换</p><h5 id="线程的栈大小-stack-size"><a href="#线程的栈大小-stack-size" class="headerlink" title="线程的栈大小(stack size)"></a>线程的栈大小(stack size)</h5><p>当系统中有很多线程时，可能需要减小每个线程栈的默认大小，防止进程的地址空间不够用,当线程调用的函数会分配很大的局部变量或者函数调用层次很深时，可能需要增大线程栈的默认大小。</p><p>函数 pthread_attr_getstacksize 和 pthread_attr_setstacksize 提供设置。</p><pre><code>int pthread_attr_getstacksize(const pthread_attr_t *attr, size_t *stacksize)int pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize)pthread_attr_t *attr： 指向一个线程属性的指针size_t *stacksize： 返回线程的堆栈大小返回值： 成功返回0，错误返回错误号</code></pre><p>除上述对栈设置的函数外，还有以下两个函数可以获取和设置线程栈属性</p><pre><code>int pthread_attr_getstack(const pthread_attr_t *attr, void **stackaddr, size_t *stacksize)int pthread_attr_setstacksize(pthread_attr_t *attr, void *stackaddr, size_t stacksize)pthread_attr_t *attr： 指向一个线程属性的指针void *stackaddr： 返回获取的栈地址size_t *stacksize： 返回获取的栈大小返回值： 成功返回0，错误返回错误号</code></pre><h5 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h5><p>使用线程属性，指定线程栈大小，测试系统可以创建线程数量。</p><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;#include &lt;string.h&gt;// 指定栈大小#define SIZE 0x1000int print_ntimes(char *str){    sleep(1);    printf(&quot;%s\n&quot;,str);    return 0;}// 线程载入实体函数void* pthread_fun(void *arg){    int n = 3;    while(n--){        print_ntimes(&quot;hello allies \n&quot;);    }}int main(void){    pthread_t tid;    int err, detachstate, i =1;    // 线程属性结构体    pthread_attr_t attr;    // 栈大小    size_t stacksize;    // 栈地址    void *stackaddr;    // 初始化线程属性    pthread_attr_init(&amp;attr);    // 获取并打印默认的栈信息    pthread_attr_getstack(&amp;attr, &amp;stackaddr,&amp;stacksize);    printf(&quot;stackaddr = %p\n&quot;,stackaddr );    printf(&quot;stacksize = %x\n&quot;, (int)stacksize);    // 获取分离状态    pthread_attr_getdetachstate(&amp;attr, &amp;detachstate);    if(detachstate == PTHREAD_CREATE_DETACHED)        printf(&quot;thread detached \n&quot;);    else if(detachstate == PTHREAD_CREATE_JOINABLE)        printf(&quot;thread join\n&quot;);    else        printf(&quot;thread uknown\n&quot;);    // 设置分离态属性    pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_DETACHED);    while(1){        // 手动分配栈内存        stackaddr = malloc(SIZE);        if(stackaddr == NULL){            perror(&quot;malloc error!&quot;);            exit(1);        }        stacksize = SIZE;        // 设置栈属性        pthread_attr_setstack(&amp;attr,stackaddr,stacksize);        // 线程属性设置完成，在创建线程，此时传入属性参数 attr        err = pthread_create(&amp;tid, &amp;attr, pthread_fun, NULL);        if(err != 0){            printf(&quot;%s\n&quot;,strerror(err));            exit(1);        }        printf(&quot;%d\n&quot;,i++);    }    // 回收线程属性    pthread_attr_destory(&amp;attr);    return 0;}</code></pre><p>输出打印：</p><pre><code>Allies:xiancheng rememberme$ ./pthread_attr_stackstackaddr = 0x0stacksize = 80000thread join123...40944095Resource temporarily unavailable</code></pre><h4 id="若干注意"><a href="#若干注意" class="headerlink" title="若干注意"></a>若干注意</h4><p>多线程虽然给我提供很多便利，但是还是要注意一下几点：</p><ol><li>主线程退出其他线程不退出，主线程应调用ptrhed_exit </li><li>避免僵线程，使用join，分离态等手动回收或者自动释放资源</li><li>malloc 和 mmap 申请的内存可以被其他线程释放 </li><li>如果线程终止时没有释放加锁的互斥量，则该互斥量不能再被使用 </li><li>应避免在多线程模型中调用 fork，除非马上 exec，子进程中只有调用 fork 的线程存在，其他线程在子进程中均 pthread_exit </li><li>信号的复杂语义很难和多线程共存，应避免在多线程引入信号机制</li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本博客主要集中的介绍了 Linux 下多线程编程的内容，设计线程进程之间的比较，线程创建销毁原语以及更高级的线程属性原语。对于线程原语也仅仅是停留在理论入门阶段，更高级的方法需要在以后实际工作中切实落实。最后，毕竟 Linux 下的多线程是基于库实现的，使用时需要自己主机安装响应的线程库，之后在编译过程中添加 -lpthread 条件。</p><p>对于多线程编程带来的资源抢夺问题，而产生的同步问题，再下一篇博客中集中说明 Linux 下各种同步锁机制，让我们拭目以待。</p><p><a href="https://www.cnblogs.com/xiehongfeng100/p/4620852.html" target="_blank" rel="noopener">更多参考</a></p><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;本章节介绍Linux下多线程编程方面的知识，涉及到线程和进程的关系，优缺点，线程基础背景，线程原语以及线程属性等方面内容，后续还会进阶的介绍多线程而引申出来的线程同步问题。让我们更加清晰的了解到多线程编程带来的魅力。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="线程" scheme="https://891904833.gitee.io/FuckCode/tags/%E7%BA%BF%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之守护进程</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/14/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/14/Linux系统编程之守护进程/</id>
    <published>2019-01-14T01:20:40.000Z</published>
    <updated>2019-01-14T03:53:10.836Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本章节介绍Linux系统中特殊的进程-守护进程（Daemon），其类似于安卓中后台服务的存在，生存周期足够长，能够长期运行于后台，周期的执行某些任务或事件。其次，在介绍守护进程模型之前，我们还需要了解进程间几个重要的概念，例如控制终端、进程组和会话等。</strong></p><a id="more"></a><h3 id="守护进程"><a href="#守护进程" class="headerlink" title="守护进程"></a>守护进程</h3><p>Daemon(精灵)进程,是Linux中的后台服务进程,生存期较长的进程，通常独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。</p><p>它不需要用户输入就能运行而且提供某种服务，不是对整个系统就是对某个用户程序提供服务。Linux系统的大多数服务器就是通过守护进程实现的。常见的守护进程包括系统日志进程syslogd、 web服务器httpd、邮件服务器sendmail和数据库服务器mysqld等。</p><p>守护进程一般在系统启动时开始运行，除非强行终止，否则直到系统关机都保持运行。守护进程经常以超级用户（root）权限运行，因为它们要使用特殊的端口（1-1024）或访问某些特殊的资源。</p><p>守护进程的父进程是init进程，因为它真正的父进程在fork出子进程后就先于子进程exit退出了，所以它是一个由init继承的孤儿进程。守护进程是非交互式程序，没有控制终端，所以任何输出，无论是向标准输出设备stdout还是标准出错设备stderr的输出都需要特殊处理。</p><p>守护进程的名称通常以d结尾，比如sshd、xinetd、crond等。</p><p>对于守护进程的创建过程，需要了解进程中几个重要概念，控制终端、进程组和会话。</p><h4 id="控制终端"><a href="#控制终端" class="headerlink" title="控制终端"></a>控制终端</h4><p>在UNIX系统中，用户通过终端登录系统后得到一个Shell进程，这个终端成为Shell进程的控制终端(Controlling Terminal)。</p><p>控制终端是保存在PCB中的信息，而我们知道fork会复制PCB中的信息，因此由Shell进程启动的其它进程的控制终端也是这个终端。</p><p>默认情况下(没有重定向)，每个进程的标准输入、标准输出和标准错误输出都指向控制终端，进程从标准输入读也就是读用户的键盘输入，进程往标准输出或标准错误输 出写也就是输出到显示器上。</p><p>文件与I/O中讲过，每个进程都可以通过一个特殊的设备文件/dev/tty访问它的控制终端。事实上每个终端设备都对应一个不同的设备文件，/dev/tty提供了一个通用的接口，一个进程要访问它的控制终端既可以通过/dev/tty也可以通过该终端设备所对应的设备文件来访问。ttyname函数可以由文件描述符查出对应的文件名，该文件描述符必须指向一个终端设备而不能是任意文件。</p><h5 id="终端数据执行流程"><a href="#终端数据执行流程" class="headerlink" title="终端数据执行流程"></a>终端数据执行流程</h5><p>数据流在终端中执行流程如下图：</p><img src="/FuckCode/2019/01/14/Linux系统编程之守护进程/终端设备模块.png" class="终端设备模块"><p>硬件驱动程序负责读写实际的硬件设备，比如从键盘读入字符和把字符输出到显示器， 线路规程像一个过滤器，对于某些特殊字符并不是让它直接通过，而是做特殊处理，比如在 键盘上按下Ctrl-Z，对应的字符并不会被用户程序的read读到，而是被线路规程截获，解释成SIGTSTP信号发给前台进程，通常会使该进程停止。线路规程应该过滤哪些字符和做哪些<br>特殊处理是可以配置的。</p><h5 id="网络终端"><a href="#网络终端" class="headerlink" title="网络终端"></a>网络终端</h5><p>虚拟终端或串口终端的数目是有限的，虚拟终端(字符控制终端)一般就是/dev/tty1 ∼ /dev/tty6六个，串口终端的数目也不超过串口的数目。然而网络终端或图形终端窗口的数目却是不受限制的，这是通过伪终端(Pseudo TTY)实现的。一套伪终端由一个主设备(PTY Master)和一个从设备(PTY Slave)组成。主设备在概念上相当于键盘和显示器，只不过它不是真正的硬件而是一个内核模块，操作它的也不是用户而是另外一个进程。从设备和上面介绍的/dev/tty1这样的终端设备模块类似，只不过它的底层驱动程序不是访问硬件而是访问主设备。网络终端或图形终端窗口的Shell进程以及它启动的其它进程都会认为自己的控制终端是伪终端从设备，例如/dev/pts/0、/dev/pts/1等。下面以telnet为例说明网络登录和使用伪终端的过程。</p><p>如果telnet客户端和服务器之间的网络延迟较大，我们会观察到按下一个键之后要过几秒钟才能回显到屏幕上。这说明我们每按一个键telnet客户端都会立刻把该字符发送给服务器，然后这个字符经过伪终端主设备和从设备之后被Shell进程读取，同时回显到伪终端从设备，回显的字符再经过伪终端主设备、telnetd服务器和网络发回给telnet客户端，显示给用户看。也许你会觉得吃惊，但真的是这样:每按一个键都要在网络上走个来回!</p><p>其网络终端中数据流导向如下图所示：</p><img src="/FuckCode/2019/01/14/Linux系统编程之守护进程/网络终端.png" class="网络终端"><h4 id="进程组"><a href="#进程组" class="headerlink" title="进程组"></a>进程组</h4><p>进程组： 一个或多个进程的集合,进程组ID是一个正整数。</p><h5 id="获取进程组id"><a href="#获取进程组id" class="headerlink" title="获取进程组id"></a>获取进程组id</h5><p>用来获得当前进程进程组ID的函数</p><pre><code>pid_t getpgid(pid_t pid)pid_t getpgrp(void)</code></pre><h5 id="获取进程组实例"><a href="#获取进程组实例" class="headerlink" title="获取进程组实例"></a>获取进程组实例</h5><p>获取父子进程的进程组</p><pre><code>#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main(void){    pid_t pid;    if((pid = fork()) &lt; 0){        perror(&quot;fork&quot;);        exit(1);    }else if(pid == 0){        printf(&quot;child process PID is %d\n&quot;, getpid());        printf(&quot;Group ID is %d\n&quot;, getpgrp());        printf(&quot;Group ID is %d\n&quot;, getpgid(0));        printf(&quot;Group ID is %d\n&quot;, getpgid(getpid()));        exit(0);    }    sleep(3);    printf(&quot;parent process PID is %d\n&quot;,getpid());    printf(&quot;Group ID is %d\n&quot;, getpgrp());    return 0;}</code></pre><p>注意：</p><ol><li>组长进程标识:其进程组ID == 其进程ID</li><li>组长进程可以创建一个进程组，创建该进程组中的进程，然后终止,只要进程组中有一个进程存在，进程组就存在，与组长进程是否终止无关</li><li>进程组生存期:进程组创建到最后一个进程离开(终止或转移到另一个进程组)</li><li>一个进程可以为自己或子进程设置进程组ID</li></ol><h5 id="创建或设置进程组"><a href="#创建或设置进程组" class="headerlink" title="创建或设置进程组"></a>创建或设置进程组</h5><p>setpgid()加入一个现有的进程组或创建一个新进程组,如改变父子进程为新的组</p><pre><code>int setpgit(pid_t pid, pid_t pgid)如改变子进程为新的组，应在fork后，exec之前非root进程只能改变自己创建的子进程，或者有权限操作的进程</code></pre><h5 id="设置进程组实例"><a href="#设置进程组实例" class="headerlink" title="设置进程组实例"></a>设置进程组实例</h5><pre><code>#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main(void){    pid_t pid;    if((pid = fork()) &lt; 0){        perror(&quot;fork&quot;);        exit(1);    }else if(pid == 0){        printf(&quot;child process PID is %d\n&quot;, getpid());        // 返回当前的组id        printf(&quot;Group ID is %d\n&quot;, getpgid(0));        sleep(5);        // 返回改变之后的组id        printf(&quot;Group ID of child is changed to %d\n&quot;, getpgid(0));         exit(0);    }    sleep(1);    printf(&quot;parent process PID is %d\n&quot;,getpid());    printf(&quot;Group ID is %d\n&quot;, getpgrp());    setpgid(pid, pid);    sleep(5);    printf(&quot;parent process PID is %d\n&quot;,getpid());    printf(&quot;parent of parent process PID is %d\n&quot;,getppid());    printf(&quot;Group ID of parent is %d\n&quot;, getpgid(0));    // 改变父进程的组id为父进程的父进程    setpgid(getpid(), getpgid());    printf(&quot;Group ID of parent is changed to %d\n&quot;, getpgid(0));    return 0;}</code></pre><h4 id="会话session"><a href="#会话session" class="headerlink" title="会话session"></a>会话session</h4><p>其实叫做会话期(session)，它包括了期间所有的进程组，一般一个会话期开始于用户login，一般login的是shell终端，所以shell终端又是此次会话期的首进程，会话一般结束于logout。对于非进程组长，它可以调用setsid()创建一个新的会话。</p><pre><code>pid_t setsid(void)</code></pre><p>注意：</p><ol><li>调用进程不能是进程组组长,该进程变成新会话首进程(session header) </li><li>该进程成为一个新进程组的组长进程。 </li><li>需有root权限(ubuntu不需要) </li><li>新会话丢弃原有的控制终端,该会话没有控制终端 </li><li>该调用进程是组长进程，则出错返回 </li><li>建立新会话时，先调用 fork, 父进程终止，子进程调用</li></ol><p>获取会话id</p><pre><code>pid_t getsid(pid_t pid)</code></pre><p>pid 为 0 表示察看当前进程session ID</p><p>ps ajx命令查看系统中的进程。</p><pre><code>参数a表示不仅列当前用户的进程，也列出所有其他用户的进程，参数x表示不仅列有控制终端的进程，也列出所有无控制终端的进程，参数j表示 列出与作业控制相关的信息。</code></pre><p>组长进程不能成为新会话首进程，新会话首进程必定会成为组长进程。</p><h5 id="会话实例"><a href="#会话实例" class="headerlink" title="会话实例"></a>会话实例</h5><pre><code>#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main(void){    pid_t pid;    if((pid = fork()) &lt; 0){        perror(&quot;fork&quot;);        exit(1);    }else if(pid == 0){        printf(&quot;child process PID is %d\n&quot;, getpid());        printf(&quot;Group ID is %d\n&quot;, getpgid(0));        printf(&quot;Session ID of child is %d\n&quot;, getsgid(0));        sleep(10);        // 子进程非组长进程，故其成为新会话首进程，且为组长进程。        // 该进程组id极为会话进程。        setsid();        printf(&quot;Changed: \n&quot;);        printf(&quot;parent process PID is %d\n&quot;,getpid());        printf(&quot;Group ID is %d\n&quot;, getpgrp());        printf(&quot;Session ID of child is %d\n&quot;,getsid(0));        sleep(20);        exit(0);    }    return 0;}</code></pre><h4 id="守护进程模型"><a href="#守护进程模型" class="headerlink" title="守护进程模型"></a>守护进程模型</h4><p>守护进程的创建有一定的规律，如下总结：</p><ol><li><p>创建子进程，父进程退出<br> 任务工作于子进程中，形式上脱离控制终端</p></li><li><p>子进程创建新会话，独立出来<br> setsid()</p></li><li><p>改变当前工作目录为根目录（也可以其他路径），防止占用可卸载的文件系统<br> chdir()</p></li><li><p>重设文件权限掩码值，防止继承的文件创建屏蔽字拒绝某些权限，增减守护进程的灵活性<br> umask()</p></li><li><p>关闭文件描述符，减少系统资源浪费<br> 由于子进程继承父进程的文件描述符，关闭他们</p></li><li><p>开始执行守护进程的核心工作<br> while()死循环处理</p></li><li><p>守护进程退出处理（非必要）<br> 释放必要的资源</p></li></ol><h4 id="守护进程代码模型"><a href="#守护进程代码模型" class="headerlink" title="守护进程代码模型"></a>守护进程代码模型</h4><p>通过以上模型介绍，我们可以代码实现Daemon守护进程的模型。</p><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;/*Daemon 精灵 守护进程编程实现步骤*/void daemonize(void){    pid_t pid;    //1. 创建子进程，父进程退出    if((pid = fork()) &lt; 0){        perror(&quot;fork error!&quot;);        exit(1);    }    else if(pid !=0){        exit(0);    }    // 子进程，创建新会话，独立终端，父进程成为init    setsid();    // 更改当期那路径为根路径，或者其他路径，避免占用可卸载文件系统    if(chdir(&quot;/&quot;) &lt; 0){        perror(&quot;chdir error!&quot;);        exit(1);    }    // 设置umask，防止继承文件系统创建屏蔽字拒绝某些权限    umask(0);    // 关闭文件描述符，较少系统资源浪费    close(0);    open(&quot;/dev/null&quot;, O_RDWR);    dup2(0,1);    dup2(0,2);}int main(void){    daemonize();    while(1){        /* do something here!!!*/    }}</code></pre><p>运行这个程序，它变成一个守护进程，不再和当前终端关联。用ps命令看不到，必须运行带x参数的ps命令才能看到。</p><p>另外还可以看到，用户关闭终端窗口或注销也不会影响守护进程的运行。</p><h4 id="守护进程参考实例"><a href="#守护进程参考实例" class="headerlink" title="守护进程参考实例"></a>守护进程参考实例</h4><p>创建一个守护进程，定时的记录时间到文件 /tmp/daemon.log 中。</p><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#include &lt;unistd.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;  #include &lt;string.h&gt;#define FILEPATH &quot;/tmp/daemon.log&quot;#define NEWLINE &quot;\n&quot;/*Daemon 精灵 守护进程编程实现步骤*/void daemonize(void){    pid_t pid;    //1. 创建子进程，父进程退出    if((pid = fork()) &lt; 0){        perror(&quot;fork error!&quot;);        exit(1);    }else if(pid !=0){        exit(0);    }    // 子进程，创建新会话，独立终端，父进程成为init    setsid();    // 更改当期那路径为根路径，或者其他路径，避免占用可卸载文件系统    if(chdir(&quot;/&quot;) &lt; 0){        perror(&quot;chdir error!&quot;);        exit(1);    }    // 设置umask，防止继承文件系统创建屏蔽字拒绝某些权限    umask(0);    // 关闭文件描述符，较少系统资源浪费    close(0);    open(&quot;/dev/null&quot;, O_RDWR);    dup2(0,1);    dup2(0,2);}int testFile(){    return access(FILEPATH,F_OK);}int createFile(){    int fd;    if((fd = open(FILEPATH, O_CREAT, 0644)) &lt; 0){        perror(&quot;open file error!&quot;);        exit(1);    }    close(fd);    return 0;}int getTime(char *buf){    time_t t;    if(time(&amp;t) != -1)        return 0;    else         retrun -1;}int saveTime2file(char *buf){    int fd;    if((fd = open(FILEPATH,O_RDWR | O_APPEND)) &gt; 0){        // strcat(buf,NEWLINE);        write(fd,buf,strlen(buf));        close(fd);        printf(&quot;write time to file %s\n&quot;,buf);    }}int main(void){    char buf[1024] = {0};    // 文件不存在，创建    if(testFile() == -1){        createFile();    }    daemonize();    while(1){        /* do something here!!!*/        // 每10s记录一次        sleep(10);        if(getTime(buf)==0){            saveTime2file(buf);            memset(buf,0,1024);        }    }}</code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本博文主要介绍了Linux下进程组、会话以及守护进程方面的内容，对于守护进程，其有一套约定俗成的编程模版，不必死记，只需要了解起工作原理，脱离终端，孤儿进程，新会话，长期运行等特性，其特殊性也就决定了他和其他进程的不同。在以后用到的时候，回过头来看一下模版，即可迅速的实现Daemon守护进程的。</p><p><a href="https://blog.csdn.net/a511244213/article/details/79625801" target="_blank" rel="noopener">参考链接</a></p><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;本章节介绍Linux系统中特殊的进程-守护进程（Daemon），其类似于安卓中后台服务的存在，生存周期足够长，能够长期运行于后台，周期的执行某些任务或事件。其次，在介绍守护进程模型之前，我们还需要了解进程间几个重要的概念，例如控制终端、进程组和会话等。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="守护进程" scheme="https://891904833.gitee.io/FuckCode/tags/%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之信号后记</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/10/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E4%BF%A1%E5%8F%B7%E5%90%8E%E8%AE%B0/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/10/Linux系统编程之信号后记/</id>
    <published>2019-01-10T02:37:52.000Z</published>
    <updated>2019-01-10T03:41:25.713Z</updated>
    
    <content type="html"><![CDATA[<p><strong>前面两章节基本将Linux系统下信号知识都介绍了一遍，这里对信号机制进行补充。虽然信号给我们编程带来了极大的方便，但是其原则还是异步操作，内核在调度进程上不可能像我们预期想象的那样运行，因此会额外带来其他问题，这里就进行统一的阐述。</strong><br><a id="more"></a></p><h3 id="信号引起的竞态"><a href="#信号引起的竞态" class="headerlink" title="信号引起的竞态"></a>信号引起的竞态</h3><h4 id="普通版sleep"><a href="#普通版sleep" class="headerlink" title="普通版sleep"></a>普通版sleep</h4><p>先来看一下信号机制中普通版本的sleep函数是如何实现的：</p><pre><code>#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;#include &lt;stdlib.h&gt;void sig_alrm(int i){}unsigned int mysleep(unsigned int nsecs){    struct sigaction newact, oldact;    unsigned int unslept;    // 注册捕捉函数    newact.sa_handler = sig_alrm;    newact.sa_flags = 0;    sigemptyset(&amp;newact.sa_mask);    // 注册阻塞捕捉信号 SIGALRM    sigaction(SIGALRM, &amp;newact, &amp;oldact);    // alarm定时开启    alarm(nsecs);    // 挂起等待，知道被唤醒    pause();    unslept = alarm(0);    // 恢复原来信号屏蔽字，否则 SIGALRM 信号一直被程序阻塞    sigaction(SIGALRM, &amp;oldact, NULL);    return unslept;}int main(void){    while(1){        mysleep(2);        printf(&quot;Two seconds passed\n&quot;);    }    return 0; }</code></pre><p>通过之前的捕捉函数处理流程可以得知，程序最大的缺点是：系统运行的时序并不像我们写程序时所设想的那样，虽然alarm紧接着下一步就是pause，但是无法保证pause一定会在调用alarm之后的指定秒之内被调用，因为捕捉函数的调用是由内核返回用户态监测时后调用的。因此这种情况下就导致了时序竞态的产生。</p><p>竞态条件： 由于异步事件在任何时候都有可能会发生（异步事件在这里指的是更高优先级的进程），如果我们写程序时考虑不周密，就可能由于时序问题而导致错误，这就是竞态条件。</p><h4 id="避免时序竞态sleep"><a href="#避免时序竞态sleep" class="headerlink" title="避免时序竞态sleep"></a>避免时序竞态sleep</h4><p>设想如何将解除信号屏蔽与挂起等待信号合并成一个原子操作就可以避免因时序问题导致的错误，因此引入sigsuspend函数。它不仅用于pause函数的挂起等待功能，而且解决了竞态条件产生的时序问题。</p><p>sugsuspeng 原理： 用于在接收到某个信号之前，临时用mask替换进程的信号掩码，并暂停进程执行，直到收到信号为止。调用sigsuspend后，进程就挂在那里，等待着开放的信号的唤醒。系统在接收到信号后，马上就把现在的信号集还原为原来的，然后调用处理函数。</p><pre><code>int sigsuspend(const sigset_t *mask);mask：指定进程的信号屏蔽字，可以临时解除对某一个信号的屏蔽，然后挂起等待。当suspend返回时，进程的信号屏蔽字恢复原先的值，如果原先对信号是屏蔽的，返回后仍然屏蔽。返回值：返回值与pause一致，永远返回-1，errno设置为EINTR。</code></pre><p>sigsuspend的整个原子操作过程为：</p><ol><li>设置新的mask阻塞当前进程；</li><li>挂起等待，收到信号，恢复原先mask；</li><li>调用该进程设置的信号处理函数；</li><li>待信号处理函数返回后，sigsuspend返回。</li></ol><p>使用sigsuspend解决时序竞态sleep函数代码</p><pre><code>#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;#include &lt;stdlib.h&gt;void sig_alrm(int i){}unsigned int mysleep(unsigned int nsecs){    struct sigaction newact, oldact;    sigset_t newmask, oldmask, suspmask;    unsigned int unslept;    newact.sa_handler = sig_alrm;    newact.sa_flags = 0;    sigemptyset(&amp;newact.sa_mask);    // 注册信号捕追    sigaction(SIGALRM, &amp;newact, &amp;oldact);    // 信号捕追时，启动信号屏蔽字    sigemptyset(&amp;newmask);    // 添加指定信号屏蔽字 SIGALRM    sigaddset(&amp;newmask,SIGALRM);    // 更改信号屏蔽字，已阻的方式开启定时    sigprocmask(SIG_BLOCK, &amp;newmask, &amp;oldmask);    alarm(nsecs);    // 开始定时后，删除原有 mask 中 SIGALRM，只有 SIGALRM 才能通过    suspmask = oldmask;    sigdelset(&amp;suspmask,SIGALRM);    // 使用sigsuspend 替换 pause，使用新的 mask     // 此时信号屏蔽字更改临时的suspmask，非阻塞信号SIGALRM，挂起等待    // 一旦有信号SIGALRM立即唤醒，恢复原来信号屏蔽字oldmask（阻塞SIGALRM），便立即进入捕捉函数处理    sigsuspend(&amp;suspmask);    // 检查定时剩余时间    unslept = alarm(0);    sigaction(SIGALRM, &amp;oldact, NULL);    // 信号捕追完毕，恢复信号原有 mask    sigprocmask(SIG_SETMASK, &amp;oldmask, NULL);    return unslept;}int main(void){    while(1){        mysleep(2);        printf(&quot;Two seconds passed\n&quot;);    }    return 0; }</code></pre><p>以上通过函数 sigsuspend 避免了时序竞态引起的问题，其主要流程是：能够在信号 SIGALRM 到达时立即响应处理，通过实现不阻塞 SIGALRM 唤醒进程，然后恢复原来设置的信号屏蔽字后（阻塞捕捉 SIGALRM），直接进入信号捕捉函数，然后返回。</p><h3 id="可重入函数与不可重入函数"><a href="#可重入函数与不可重入函数" class="headerlink" title="可重入函数与不可重入函数"></a>可重入函数与不可重入函数</h3><p>函数是一段载入到内存的代码。函数的代码可长可短，执行时间长度也不确定。在多线程中，线程之间是可以进行切换的。函数是一段写好的代码，属于程序公有的代码段。一个进程中有多个线程，每一个线程都可以调用这段函数代码执行。而在多线程环境中，线程的切换是无法预料的，你不知道下一秒是哪个线程在执行，每时每刻的运行环境都不一样，因为线程切换也是变化莫测的。这是操作系统调度进程线程的范围，不是我们能够掌控的。</p><p>信号作为一种软中断，能够被进程给捕获，因而也就中断进程的正常执行，转而去执行信号处理程序，最后再返回到原进程继续正常执行。因此就会涉及到可重入函数问题了。</p><h4 id="可重入函数"><a href="#可重入函数" class="headerlink" title="可重入函数"></a>可重入函数</h4><p>一个函数在执行的过程中被打断，然后会再被再重头执行一次，执行完后，再回来把刚才没执行完的部分执行完。这就相当于嵌套的执行了。函数是公共代码，这样的执行是允许的。函数的执行可以被打断，打断之后还可以再重头执行，执行完后接着执行刚才没有执行的代码，然后第一次执行的代码（被打断的函数）执行结果还是正确的。也就是说，这个函数执行，无论中间把这个函数再嵌入执行多少遍，怎么嵌入，最终执行完，执行的结果都是正确的，这样的函数就是可重入函数。</p><p>常用的可重入函数的方法有：</p><ol><li>不要使用全局变量，防止别的代码覆盖这些变量的值。</li><li>调用这类函数之前先关掉中断，调用完之后马上打开中断。防止函数执行期间被中断进入别的任务执行。</li><li>使用信号量（互斥条件）。</li></ol><p>总而言之：要保证中断是安全的。<br>使用 man 7 signal 查看</p><h4 id="不可重入函数"><a href="#不可重入函数" class="headerlink" title="不可重入函数"></a>不可重入函数</h4><p>在函数执行期间被中断，从头执行这个函数，执行完毕后再返回刚才的中断点继续执行，此时由于刚才的中断导致了现在从新在中断点执行时发生了不可预料的错误。那么这类函数就是不可重入函数。</p><p>常见的不可重入函数：</p><ol><li>使用了静态数据结构</li><li>调用了malloc和free等</li><li>调用了标准I/O函数</li><li>进行了浮点运算</li></ol><h4 id="如何避免写出不可重入函数"><a href="#如何避免写出不可重入函数" class="headerlink" title="如何避免写出不可重入函数"></a>如何避免写出不可重入函数</h4><ol><li>不使用或者互斥使用全局变量</li><li>不使用静态局部变量，只是用局部变量，</li><li>在函数中动态分配的内存只在本函数中使用，不会传递函数外使用，</li></ol><p>只要保证局部特性，函数中使用的所有东西都只有局部性，对外不公开，用完即释放，就可以保证可重入。这样，不管怎么重叠，反正本层的函数的东西只有本层能够使用，其他层的函数无法使用。</p><h4 id="信号中的可重入函数"><a href="#信号中的可重入函数" class="headerlink" title="信号中的可重入函数"></a>信号中的可重入函数</h4><p>信号捕捉函数内部，禁止调用不可重入函数。</p><p>strtok就是一个不可重入函数，因为strtok内部维护了一个内部静态指针，保存上一次切割到的位置，如果信号的捕捉函数中也去调用strtok函数，则会造成切割字符串混乱， 应用strtok_r版本，r表示可重入。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过三篇信号博文，信号的部分就告一段落了。从信号的基础认识，到信号相关函数的介绍，以及后续对信号机制带来的其他问题等方面，系统的阐述了Linux下信号的内容，相信读完三篇内容的你，应该会对信号有更深层次的了解。</p><p>我们知道信号机制可以在多进程中进行通信，当然在多线程中使用也是可以的。但是这对开发者来说并非好事，使用时必须要严格控制管理，否则会造成不可预估的后果。</p><p><a href="https://blog.csdn.net/liuchenxia8/article/details/79961851" target="_blank" rel="noopener">参考链接</a></p><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;前面两章节基本将Linux系统下信号知识都介绍了一遍，这里对信号机制进行补充。虽然信号给我们编程带来了极大的方便，但是其原则还是异步操作，内核在调度进程上不可能像我们预期想象的那样运行，因此会额外带来其他问题，这里就进行统一的阐述。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="信号" scheme="https://891904833.gitee.io/FuckCode/tags/%E4%BF%A1%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之信号进阶</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/08/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E4%BF%A1%E5%8F%B7%E8%BF%9B%E9%98%B6/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/08/Linux系统编程之信号进阶/</id>
    <published>2019-01-08T08:26:37.000Z</published>
    <updated>2019-01-10T02:39:37.753Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本章节继续介绍关于Linux系统下信号相关的知识点，介于之前介绍了信号的基础，了解了信号的产生、缘由以及工作流程，这里就对信号的相关函数进行统一的分析，之后在用实例进行说明。</strong></p><a id="more"></a><h3 id="信号产生函数"><a href="#信号产生函数" class="headerlink" title="信号产生函数"></a>信号产生函数</h3><h4 id="kill"><a href="#kill" class="headerlink" title="kill"></a>kill</h4><p>kill 函数可以给一个特定的进程发送指定的信号</p><pre><code>#include &lt;signal.h&gt;int kill(pid_t pid, int sig) pid &gt; 0     sig发送给ID为pid的进程 pid == 0    sig发送给与发送进程同组的所有进程 pid &lt; 0     sig发送给组ID为|-pid|的进程，并且发送进程具有向其发送信号的权限 pid == -1   sig发送给发送进程有权限向他们发送信号的系统上的所有进程 sig为发送的信号值，当sig为0时，用于检测特定为pid进程是否存在，如不存在，返回-1。</code></pre><h4 id="raise"><a href="#raise" class="headerlink" title="raise"></a>raise</h4><p>raise函数可以给当前进程发送指定的信号（自己也可以给自己发送信号）</p><pre><code>#include &lt;signal.h&gt;int raise(int sig);</code></pre><h4 id="abort"><a href="#abort" class="headerlink" title="abort"></a>abort</h4><p>abort函数通过向当前进程发送信号值 SIGABRT，使当前进程接收到信号而异常终止，但是abort会认为进程不安全。</p><pre><code>#include &lt;stdlib.h&gt; void abort(void);</code></pre><p>类似于exit函数一样，abort函数总是成功的，因此没有返回值。</p><h4 id="alarm"><a href="#alarm" class="headerlink" title="alarm"></a>alarm</h4><p>由软件条件产生信号，进程可以通过调用alarm向它自己发送SIGALRM信号</p><pre><code>#include &lt;unistd.h&gt;unsigned int alarm(unsigned int seconds);参数seconds：alarm函数安排内核在seconds秒内发送一个SIGALRM信号给调用进程，如果soconds等于0，那么不会调度新的闹钟（alarm）返回值：前一次闹钟剩余的秒数，若以前没有设定闹钟，则为0</code></pre><p>在使用时，alarm只设定为发送一次信号，如果要多次发送，就要多次使用alarm调用。</p><h4 id="setitimer"><a href="#setitimer" class="headerlink" title="setitimer"></a>setitimer</h4><p>现在的系统中很多程序不再使用alarm调用，而是使用setitimer调用来设置定时器，用getitimer来得到定时器的状态，这两个调用的声明格式如下：</p><pre><code>#include &lt;sys/time.h&gt;int getitimer(int which, struct itimerval *value);int setitimer(int which, const struct itimerval *value, struct itimerval *ovalue);</code></pre><p>该系统调用给进程提供了三个定时器，它们各自有其独有的计时域，当其中任何一个到达，就发送一个相应的信号给进程，并使得计时器重新开始。三个计时器由参数which指定，如下所示：</p><pre><code>ITIMER_REAL：按实际时间计时，计时到达将给进程发送SIGALRM信号。ITIMER_VIRTUAL：仅当进程执行时才进行计时。计时到达将发送SIGVTALRM信号给进程。ITIMER_PROF：当进程执行时和系统为该进程执行动作时都计时。与ITIMER_VIRTUAL是一对，该定时器经常用来统计进程在用户态和内核态花费的时间。计时到达将发送SIGPROF信号给进程。</code></pre><p>定时器中的参数value用来指明定时器的时间，其结构如下：</p><pre><code>struct itimerval {        struct timeval it_interval; /* 下一次的取值 */        struct timeval it_value; /* 本次的设定值 */};</code></pre><p>该结构中timeval结构定义如下：</p><pre><code>struct timeval {        long tv_sec; /* 秒 */        long tv_usec; /* 微秒，1秒 = 1000000 微秒*/};</code></pre><p>在setitimer 调用中，参数ovalue如果不为空，则其中保留的是上次调用设定的值。定时器将it_value递减到0时，产生一个信号，并将it_value的值设定为it_interval的值，然后重新开始计时，如此往复。当it_value设定为0时，计时器停止，或者当它计时到期，而it_interval 为0时停止。调用成功时，返回0；错误时，返回-1，并设置相应的错误代码errno：</p><pre><code>EFAULT：参数value或ovalue是无效的指针。EINVAL：参数which不是ITIMER_REAL、ITIMER_VIRT或ITIMER_PROF中的一个。</code></pre><p>下面是关于setitimer调用的一个简单示范，在该例子中，每隔一秒发出一个SIGALRM，每隔0.5秒发出一个SIGVTALRM信号：</p><pre><code>#include &lt;signal.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/time.h&gt;int sec;void sigroutine(int signo) {    switch (signo) {        case SIGALRM:            printf(&quot;Catch a signal -- SIGALRM\n&quot;);            break;        case SIGVTALRM:            printf(&quot;Catch a signal -- SIGVTALRM\n&quot;);            break;    }    return;}int main(){    struct itimerval value,ovalue,value2;    sec = 5;    printf(&quot;process id is %d\n&quot;,getpid());    signal(SIGALRM, sigroutine);    signal(SIGVTALRM, sigroutine);    value.it_value.tv_sec = 1;    value.it_value.tv_usec = 0;    value.it_interval.tv_sec = 1;    value.it_interval.tv_usec = 0;    setitimer(ITIMER_REAL, &amp;value, &amp;ovalue);    value2.it_value.tv_sec = 0;    value2.it_value.tv_usec = 500000;    value2.it_interval.tv_sec = 0;    value2.it_interval.tv_usec = 500000;    setitimer(ITIMER_VIRTUAL, &amp;value2, &amp;ovalue);    for (;;) ;}</code></pre><p>程序运行结果如下：</p><pre><code>$ ./app process id is 104501 Catch a signal -- SIGVTALRMCatch a signal -- SIGALRMCatch a signal -- SIGVTALRMCatch a signal -- SIGVTALRMCatch a signal -- SIGALRMCatch a signal -- SIGVTALRMCatch a signal -- SIGVTALRM...</code></pre><h4 id="sigqueue"><a href="#sigqueue" class="headerlink" title="sigqueue"></a>sigqueue</h4><p>sigqueue是比较新的发送信号系统调用，主要是针对实时信号提出的（当然也支持前32种），支持信号带有参数，与函数sigaction配合使用，后续详细介绍。</p><h3 id="信号集"><a href="#信号集" class="headerlink" title="信号集"></a>信号集</h3><p>信号集被定义为一种数据类型：</p><pre><code>typedef struct {    unsigned long sig[_NSIG_WORDS]；} sigset_t</code></pre><p>信号集用来描述信号的集合，每个信号占用一位。Linux所支持的所有信号可以全部或部分的出现在信号集中，主要与信号阻塞相关函数配合使用。</p><h4 id="sigset-t-信号集"><a href="#sigset-t-信号集" class="headerlink" title="sigset_t 信号集"></a>sigset_t 信号集</h4><p>下面是为信号集 sigset_t 操作定义的相关函数：</p><pre><code>int sigemptyset(sigset_t *set)  初始化由set指定的信号集，信号集里面的所有信号被清空；int sigfillset(sigset_t *set)   调用该函数后，set指向的信号集中将包含linux支持的64种信号；int sigaddset(sigset_t *set, int signo) 在set指向的信号集中加入signum信号；int sigdelset(sigset_t *set, int signo) 在set指向的信号集中删除signum信号；int sigismember(const sigset_t *set, int signo) 判定信号signum是否在set指向的信号集中。 </code></pre><h4 id="信号集模型"><a href="#信号集模型" class="headerlink" title="信号集模型"></a>信号集模型</h4><p>如果在进程解除对某信号的阻塞之前这种信号产生过多次，将如何处理?POSIX.1允许系统递送该信号一次或多次。Linux是这样实现的:常规信号在递达之前产生多次只计一次，而实时信号在递达之前产生多次可以依次放在一个队列里。本章不讨论实时信号。每个信号只有一个bit的未决标志，非0即1，不记录该信号产生了多少次，阻塞标志也是这样表示的。因此，未决和阻塞标志可以用相同的数据类型 sigset_t 来存储，sigset_t 称为信号集，这个类型可以表示每个信号的“有效”或“无效”状态， 在阻塞信号集中“有效”和“无效”的含义是该信号是否被阻塞，而在未决信号集中“有效”和“无效”的含义是该信号是否处于未决状态。</p><p>阻塞信号集也叫做当前进程的信号屏蔽字(Signal Mask)，这里的“屏蔽”应该理解 为阻塞而不是忽略。下图展示了PCB中信号集模型：</p><img src="/FuckCode/2019/01/08/Linux系统编程之信号进阶/pcb信号集模型.png" class="pcb信号集模型"><ol><li>PCB进程控制块中有信号屏蔽状态字(block)，信号未决状态字(pending)还有是否忽略标识</li><li>信号屏蔽状态字(block)：1代表阻塞，0代表不阻塞；信号未决状态字(pending)：1代表未决，0代表信号递达</li><li>向进程发送SIGINT，内核首先判断信号屏蔽状态字是否阻塞，如果信号屏蔽状态字阻塞，信号未决状态字(pengding)相应位置1；<br>若阻塞解除，信号未决状态字(pending)相应位置0，表示信号可以递达了。</li><li>block状态字，pending状态都是64bit，分别代表Linux系统中的64个信号。例如SIGINT是2号信号，对应block状态字中的第二位</li><li>block状态字用户可以读写，pending状态字用户只能读，这是新号的设计机制。</li></ol><p>内核将信号传递到PEND集合中，置位相应的未决态1，之后，经过用户设置过的阻塞信号集屏蔽字处理，如果信号被阻塞，那么未决集内对应位还是1，如果没有阻塞，信号成为递达，经过handler处理，默认、忽略以及捕捉，此时未决集中对应的标志位转为0.</p><h3 id="信号集操作函数"><a href="#信号集操作函数" class="headerlink" title="信号集操作函数"></a>信号集操作函数</h3><p>在信号集中，用户可通过sigprocmask设置阻塞信号集，通过sigpending获取未决态信号集</p><h4 id="sigprocmask"><a href="#sigprocmask" class="headerlink" title="sigprocmask"></a>sigprocmask</h4><p>调用函数sigprocmask可以读取或更改进程的信号屏蔽字。</p><pre><code>#include &lt;signal.h&gt;int sigprocmask(int how,const sigset_t *set,sigset * oset);成功返回0，出错返回-1</code></pre><p>如果oset是非空指针，则读取进程的当前信号屏蔽状态字通过oset参数传出，如果set是非空指针，则更改进程的信号屏蔽状态字，参数how控制如何更改。</p><p>如果oset和set都是非空指针，则先将原来的信号屏蔽字备份到oset里，然后根据set和how参数更改信号屏蔽字。</p><p>参数 how 的含义</p><pre><code>--SIG_BLOCK    set包含了我们希望添加到当前信号屏蔽字的信号，相当于mask=mask|set(位或运算)--SIG_UNBLOCK    set包含了我们希望从当前信号屏蔽字中解除阻塞的信号，相当于mask=mask^set(位异或运算)--SIG_SETMASK    设置当前信号屏蔽字为set所指向的值，相当于mask=set</code></pre><p>如果调用sigprocmask解除了对当前若干个未决信号的阻塞，则在sigprocmask返回前， 至少将其中一个信号递达。</p><h4 id="sigpending"><a href="#sigpending" class="headerlink" title="sigpending"></a>sigpending</h4><p>sigpending函数用来读取当前进程的信号未决集，通过set参数传出。</p><pre><code>#include &lt;signal.h&gt;int sigpending(sigset_t *set);成功返回0，出错返回-1。</code></pre><p>sigpending读取当前进程的未决信号集，通过set参数传出，调用成功返回0，失败返回-1。</p><h4 id="信号集代码示例"><a href="#信号集代码示例" class="headerlink" title="信号集代码示例"></a>信号集代码示例</h4><pre><code>#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;void printsigset(const sigset_t *set){    int i;    for(i =1; i&lt;32;i++)    {        if(sigismember(set, i)==1)            putchar(&apos;1&apos;);        else            putchar(&apos;0&apos;);    }    puts(&quot;&quot;);}int main(void){    sigset_t s,p;    int i = 0;    //  清空信号集    sigemptyset(&amp;s);    // 捕捉信号屏蔽字    sigaddset(&amp;s, SIGINT); // ctrl + c    sigaddset(&amp;s, SIGQUIT); // ctrl + z    sigaddset(&amp;s, SIGTSTP); // ctrl + \    // 补充设置部分信号屏蔽字    sigprocmask(SIG_BLOCK, &amp;s, NULL);    while(1)    {        // 获取未决信号集        sigpending(&amp;p);        printsigset(&amp;p);        if (i==10)        {            // 解除信号屏蔽字 SIGQUIT            sigdelset(&amp;s, SIGQUIT);            sigprocmask(SIG_UNBLOCK, &amp;s, NULL);        }        sleep(1);        i++;    }    return 0;}</code></pre><p>注意，SIGKUILL 和 SIGSTOP 两个信号屏蔽字 无法捕捉、阻塞以及忽略。<br><br>程序执行结果：</p><pre><code>Allies:signal rememberme$ ./sigprocmask  0000000000000000000000000000000^Z0000000000000000010000000000000  0000000000000000010000000000000^C0100000000000000010000000000000^\0110000000000000010000000000000  0110000000000000010000000000000  ...</code></pre><h3 id="信号捕捉"><a href="#信号捕捉" class="headerlink" title="信号捕捉"></a>信号捕捉</h3><h4 id="sigaction"><a href="#sigaction" class="headerlink" title="sigaction"></a>sigaction</h4><p>注册信号捕捉函数sigaction，POSIX标准定义了sigaction函数，它允许像Linux和Solaris这样与POSIX兼容的系统上的用户，明确地指出它们想要的信号处理语义。</p><p>sigaction函数可以读取或者指定信号相关联的处理动作，signal与其功能类似，但signal是标准C的信号接口，对不同的操作系统有不同的行为，所以一般尽量不使用signal，取而代之的是sigaction函数。函数原型如下：</p><pre><code>#include &lt;signal.h&gt;int sigaction(int signum, const struct sigaction *act, struct sigaction *oldact);signum： 指定信号的编号（利用kill -l命令可以查看）；*act： 若act指针非空，则根据act修改该信号的处理动作；*oldact： 若oldact指针非空，则通过oldact传出该信号原来的处理动作；返回值：成功返回0，失败返回-1；</code></pre><p>对于其中的结构体 struct sigaction 的定义如下: </p><pre><code>struct sigaction {    void void sigset_t int    void    (*sa_handler)(int);    (*sa_sigaction)(int, siginfo_t *, void *);    sa_mask;    sa_flags; (*sa_restorer)(void);};sa_handler : 早期的捕捉函数，SIG_DEF（默认）,SIG_IGN（忽略）,自定义函数指针sa_sigaction : 新添加的捕捉函数，可以传参 , 和sa_handler互斥，两者通过sa_flags选择采用哪种捕捉函数 sa_mask : 在执行捕捉函数时，设置阻塞其它信号，sa_mask | 进程阻塞信号集，退出捕捉函数后，还原回原有的 阻塞信号集sa_flags : SA_SIGINFO 或者 0sa_restorer : 保留，已过时</code></pre><h4 id="捕捉代码示例"><a href="#捕捉代码示例" class="headerlink" title="捕捉代码示例"></a>捕捉代码示例</h4><pre><code>#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;// 捕捉函数处理，接受捕捉的信号值void sigHandler(int signum){    printf(&quot;\nsigHandler: %d\n&quot;,signum);}int main(void){    // 声明结构体 sigaction    struct  sigaction sa;    // 使用 sa_handler 函数指针捕捉处理，参数 0    sa.sa_flags = 0;    // 设置函数指针指向函数体    sa.sa_handler = sigHandler;    // sa.sa_handler = SIG_DFL;    // sa.sa_handler = SIG_IGN;    // 清空函数捕捉时的 mask 值    sigemptyset(&amp;sa.sa_mask);    // 注册捕捉函数    sigaction(SIGINT, &amp;sa, NULL);    while(1){        printf(&quot;......\n&quot;);        sleep(1);    }    return 0;}</code></pre><p>程序运行结果：</p><pre><code>Allies:signal rememberme$ ./sigaction..................^CsigHandler: 2............^CsigHandler: 2............</code></pre><p> 注意:子进程继承了父进程的信号屏蔽字和信号处理动作。</p><p>库函数 signal 原型如下：</p><pre><code>typedef void (*sighandler_t)(int)sighandler_t signal(int signum, sighandler_t handler)int system(const char *command) 集合fork，exec，wait一体</code></pre><h4 id="信号捕捉传参"><a href="#信号捕捉传参" class="headerlink" title="信号捕捉传参"></a>信号捕捉传参</h4><p>通常的sigaction结构体中 sa.sa_flags 设置为 时，信号捕捉只能够得到捕捉的信号值，然而向进程本身发送信号，并传递指针参数则需要修改标志位 sa.sa_flags 为 SA_SIGINFO。发送信号时候就用之前提到的函数 sigqueue 添加而外信息。先来回顾一下信号捕捉流程</p><img src="/FuckCode/2019/01/08/Linux系统编程之信号进阶/信号捕捉.png" class="信号捕捉流程"><h5 id="sigqueue-函数原型"><a href="#sigqueue-函数原型" class="headerlink" title="sigqueue 函数原型"></a>sigqueue 函数原型</h5><p>sigqueue 功能和 kill 相同，都可以向制定进程发送信号，其次还可以额外夹杂数据。</p><pre><code>#include &lt;sys/types.h&gt;#include &lt;signal.h&gt;int sigqueue(pid_t pid, int sig, const union sigval val)调用成功返回 0；否则，返回 -1。</code></pre><p>sigqueue 是比较新的发送信号系统调用，主要是针对实时信号提出的（当然也支持前32种），支持信号带有参数，与函数sigaction()配合使用。</p><p>sigqueue的第一个参数是指定接收信号的进程ID，第二个参数确定即将发送的信号，第三个参数是一个联合数据结构union sigval，指定了信号传递的参数，即通常所说的4字节值。</p><pre><code>typedef union sigval {    int  sival_int;    void *sival_ptr;}sigval_t;</code></pre><p>sigqueue 比kill 传递了更多的附加信息，但 sigqueue 只能向一个进程发送信号，而不能发送信号给一个进程组。如果signo=0，将会执行错误检查，但实际上不发送任何信号，0值信号可用于检查pid的有效性以及当前进程是否有权限向目标进程发送信号。</p><p>在调用 sigqueue 时，sigval_t 指定的信息会拷贝到对应 sig 注册的3参数信号处理函数的 siginfo_t 结构中，这样信号处理函数就可以处理这些信息了。由于 sigqueue 系统调用支持发送带参数信号，所以比 kill 系统调用的功能要灵活和强大得多。</p><pre><code>#include &lt;signal.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;void new_op(int,siginfo_t*,void*);int main(int argc,char**argv){    struct sigaction act;      union sigval mysigval;    int i;    int sig;    pid_t pid;             char data[10];    memset(data,0,sizeof(data));    for(i=0;i &lt; 5;i++)            data[i]=&apos;2&apos;;    mysigval.sival_ptr=data;    sig=atoi(argv[1]);    pid=getpid();    sigemptyset(&amp;act.sa_mask);    act.sa_sigaction=new_op;//三参数信号处理函数    act.sa_flags=SA_SIGINFO;//信息传递开关，允许传说参数信息给new_op    if(sigaction(sig,&amp;act,NULL) &lt; 0)    {            printf(&quot;install sigal error\n&quot;);    }    while(1)    {            sleep(2);            printf(&quot;wait for the signal\n&quot;);            sigqueue(pid,sig,mysigval);//向本进程发送信号，并传递附加信息    }}void new_op(int signum,siginfo_t *info,void *myact)//三参数信号处理函数的实现{    int i;    for(i=0;i&lt;10;i++)    {            printf(&quot;%c\n &quot;,(*( (char*)((*info).si_ptr)+i)));    }    printf(&quot;handle signal %d over;&quot;,signum);}</code></pre><p>程序运行结果</p><pre><code>$ ./sigqueue 7wait for the signalSSSSShandle signal 7 over;wait for the signalSSSSShandle signal 7 over;wait for the signalSS...</code></pre><p>sigqueue 也可以夸进程通信，但是其传递的数据一半以整型等具体的基本数据。不同进程间收发信号，不在同一地址空间,不适合传地址。</p><h3 id="SIGCHLD-信号处理"><a href="#SIGCHLD-信号处理" class="headerlink" title="SIGCHLD 信号处理"></a>SIGCHLD 信号处理</h3><h4 id="产生条件"><a href="#产生条件" class="headerlink" title="产生条件"></a>产生条件</h4><p>SIGCHLD的产生条件有一下三点：</p><ol><li>子进程终止时 </li><li>子进程接收到SIGSTOP信号停止时 </li><li>子进程处在停止态，接受到SIGCONT后唤醒时</li></ol><h4 id="处理方式"><a href="#处理方式" class="headerlink" title="处理方式"></a>处理方式</h4><p>父进程回收子进程时，对status的处理方式</p><pre><code>pid_t waitpid(pid_t pid, int *status, int options) options    1. WNOHANG 没有子进程结束，立即返回    2. WUNTRACED   如果子进程由于被停止产生的SIGCHLD， waitpid则立即返回    3. WCONTINUED  如果子进程由于被SIGCONT唤醒而产生的SIGCHLD， waitpid则立即返回获取status     1. WIFEXITED(status) 子进程正常exit终止，返回真 WEXITSTATUS(status)返回子进程正常退出值    2. WIFSIGNALED(status) 子进程被信号终止，返回真    3. WTERMSIG(status)    返回终止子进程的信号值     4. WIFSTOPPED(status)  子进程被停止，返回真 WSTOPSIG(status)返回停止子进程的信号值    5. WIFCONTINUED(status)    子进程由停止态转为就绪态，返回真</code></pre><h4 id="SIGCHLD-代码示例"><a href="#SIGCHLD-代码示例" class="headerlink" title="SIGCHLD 代码示例"></a>SIGCHLD 代码示例</h4><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;signal.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;void do_sig(int n){    int status;    pid_t pid;    // 收到子进程发送的 SIGCHILD 信号，捕捉函数至此    while((pid = waitpid(0,&amp;status,WNOHANG)) &gt; 0){        // 子进程正常退出        if(WIFEXITED(status))            // 打印出正常退出值            printf(&quot;child %d exit %d\n&quot;,pid,WEXITSTATUS(status));        // 子进程被信号终止退出        else if(WIFSIGNALED(status))            // 打印终止子进程的信号值            printf(&quot;child %d received signal %d\n&quot;,pid,WTERMSIG(status));    }}int main(void){    pid_t pid;    int i;    // 父子进程均设置 SIGCHLD 信号屏蔽字，方便捕捉    sigset_t newmask,oldmask;    sigemptyset(&amp;newmask);    sigaddset(&amp;newmask,SIGCHLD);    // 修改信号屏蔽字    sigprocmask(SIG_BLOCK,&amp;newmask,&amp;oldmask);    // fork子进程5个，全部捕捉 SIGCHLD 信号值    for (i = 0; i &lt; 5; i++)    {        if((pid = fork() )== 0){            break;        }        else if(pid &lt; 0){            perror(&quot;fork error!&quot;);            exit(1);        }    }    // 子进程 10 次循环打印， 10s 后结束    if(pid == 0){        int n = 10;        while(n--){            printf(&quot;child ID %d\n&quot;, getpid());            sleep(1);        }        return i;    } else if(pid &gt; 0){        // 父进程添加捕追函数        struct sigaction act;        act.sa_handler = do_sig;        act.sa_flags = 0;        sigemptyset(&amp;act.sa_mask);        // 注册信号捕捉函数        sigaction(SIGCHLD,&amp;act, NULL);        // 恢复初始信号屏蔽字的状态        sigprocmask(SIG_SETMASK,&amp;oldmask,NULL);        while(1){            printf(&quot;Parent ID %d\n&quot;,getpid());            sleep(5);        }    }    return 0;}</code></pre><p>程序运行结果：</p><pre><code>...child ID 1421child ID 1422child ID 1423child ID 1420Parent ID 1418child ID 1421child ID 1422child ID 1419child 1423 exit 4child 1422 exit 3child 1421 exit 2child 1420 exit 1child 1419 exit 0Parent ID 1418...</code></pre><p>此时通过 kill -9 ？ 向指定的子进程（？）发信号杀死子进程，那么打印log便会将指定的子进程退出信号值打印出来（9）。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本章节继前面章节，系统从函数方面阐述了信号的发送，拦截以及捕捉。对于父子进程而言，信号屏蔽字，信号集等在父子进程中都会的到继承，因此再处理父子进程间信号时，需要优先设置相应的信号屏蔽字后，再fork子进程，然后在父进程中进行监听。</p><p><a href="https://blog.csdn.net/ypt523/article/details/80365108" target="_blank" rel="noopener">参考链接1</a><br><a href="https://www.cnblogs.com/subo_peng/p/5325326.html" target="_blank" rel="noopener">参考链接2</a></p><blockquote><p>邢文鹏Linux系统教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;本章节继续介绍关于Linux系统下信号相关的知识点，介于之前介绍了信号的基础，了解了信号的产生、缘由以及工作流程，这里就对信号的相关函数进行统一的分析，之后在用实例进行说明。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="信号" scheme="https://891904833.gitee.io/FuckCode/tags/%E4%BF%A1%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之信号基础</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/08/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E4%BF%A1%E5%8F%B7%E5%9F%BA%E7%A1%80/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/08/Linux系统编程之信号基础/</id>
    <published>2019-01-08T03:39:45.000Z</published>
    <updated>2019-01-14T03:42:49.411Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Linux下信号涉及知识点非常多，本章节先做一个铺垫，简单介绍一下信号种类及其含义，信号的几种默认处理方式等，后续再来介绍具体的信号处理函数以及注意事项也就方便很多了。工欲善其事，必先利其器。Let us get it！</strong><br><a id="more"></a></p><h3 id="信号"><a href="#信号" class="headerlink" title="信号"></a>信号</h3><p>信号，通俗来讲就是给予一种提示，告知一方发生了啥。Linux下的信号指的是操作系统给进程发送信号，告知进程在合适的时间内去处理所接受的信号。其本质给发送信号的进程内的PCB中写入数据，修改相应的PCB字段，内核在返回处理时候得知信号记录再予以操作。</p><p>如下模拟场景：</p><ol><li><p>用户输入一个命令，在shell下启动一个前台进程；</p></li><li><p>用户按下Ctrl+c，通过键盘输入产生一个硬件中断；</p></li><li><p>如果CPU当前正在运行此进程的代码，则该进程的用户空间的代码将暂停执行，CPU从用户态切换至内核态处理中断；</p></li><li><p>终端驱动程序将Ctrl+c解释为一个SIGINT信号，记在该进程的PCB中；</p></li><li><p>当某个时刻从内核返回至该进程的用户空间代码继续执行之前，首先处理PCB中记录的信号；SIGINT信号的默认处理动作为终止信号，所以直接终止进程而不再返回到它的用户空间代码；</p></li></ol><p><a href="https://blog.csdn.net/ypt523/article/details/80290208" target="_blank" rel="noopener">示例参考</a></p><h4 id="信号种类"><a href="#信号种类" class="headerlink" title="信号种类"></a>信号种类</h4><p>信号可以按照可靠性和时间关系划分，如下表：</p><table><thead><tr><th style="text-align:center">划分</th><th style="text-align:center">Item1</th><th style="text-align:center">Item2</th></tr></thead><tbody><tr><td style="text-align:center">可靠性</td><td style="text-align:center">可靠信号</td><td style="text-align:center">不可靠信号</td></tr><tr><td style="text-align:center">时间关系</td><td style="text-align:center">普通信号</td><td style="text-align:center">实时信号</td></tr></tbody></table><h5 id="可靠信号"><a href="#可靠信号" class="headerlink" title="可靠信号"></a>可靠信号</h5><p>信号值位于SIGRTMIN和SIGRTMAX之间的信号都是可靠信号，信号的可靠与不可靠只与信号值有关，与信号的发送及安装函数无关。</p><h5 id="实时信号"><a href="#实时信号" class="headerlink" title="实时信号"></a>实时信号</h5><p>通过命令查看信号种类</p><pre><code>kill -l1) SIGHUP       2) SIGINT       3) SIGQUIT      4) SIGILL5) SIGTRAP      6) SIGABRT      7) SIGBUS       8) SIGFPE9) SIGKILL     10) SIGUSR1     11) SIGSEGV     12) SIGUSR213) SIGPIPE     14) SIGALRM     15) SIGTERM     17) SIGCHLD18) SIGCONT     19) SIGSTOP     20) SIGTSTP     21) SIGTTIN22) SIGTTOU     23) SIGURG      24) SIGXCPU     25) SIGXFSZ26) SIGVTALRM   27) SIGPROF     28) SIGWINCH    29) SIGIO30) SIGPWR      31) SIGSYS      33) SIGRTMIN    34) SIGRTMIN+135) SIGRTMIN+2  36) SIGRTMIN+3  37) SIGRTMIN+4  38) SIGRTMIN+539) SIGRTMIN+6  40) SIGRTMIN+7  41) SIGRTMIN+8  42) SIGRTMIN+943) SIGRTMIN+10 44) SIGRTMIN+11 45) SIGRTMIN+12 46) SIGRTMIN+1347) SIGRTMIN+14 48) SIGRTMIN+15 49) SIGRTMAX-14 50) SIGRTMAX-1351) SIGRTMAX-12 52) SIGRTMAX-11 53) SIGRTMAX-10 54) SIGRTMAX-955) SIGRTMAX-8  56) SIGRTMAX-7  57) SIGRTMAX-6  58) SIGRTMAX-559) SIGRTMAX-4  60) SIGRTMAX-3  61) SIGRTMAX-2  62) SIGRTMAX-163) SIGRTMAX </code></pre><p>前32种信号已经有了预定义值，每个信号有了确定的用途及含义，并且每种信号都有各自的缺省动作，为普通信号。后32个信号表示实时信号，等同于前面阐述的可靠信号。这保证了发送的多个实时信号都被接收。</p><p>非实时信号都不支持排队，都是不可靠信号；实时信号都支持排队，都是可靠信号。</p><h4 id="信号产生"><a href="#信号产生" class="headerlink" title="信号产生"></a>信号产生</h4><h5 id="产生来源"><a href="#产生来源" class="headerlink" title="产生来源"></a>产生来源</h5><p>信号事件的发生有两个来源：</p><ol><li><p>终端特殊按键</p><pre><code>ctl+c SIGINT    暂停ctl+z SIGTSTP   停止ctl+\ SIGQUIT   退出</code></pre></li><li><p>软件来源,包括发送信号相关函数和硬件异常产生信号</p><pre><code>函数：killraisealarmsetitimersigqueue硬件异常除0操作 SIGFPE访问非法内存    SIGSEGV</code></pre></li></ol><h5 id="信号产生缘由"><a href="#信号产生缘由" class="headerlink" title="信号产生缘由"></a>信号产生缘由</h5><p>以下罗列了各种信号产生的缘由</p><ol><li>SIGHUP:当用户退出shell时，由该shell启动的所有进程将收到这个信号，默认动作为终止进程 </li><li>SIGINT:当用户按下了&lt;Ctrl+C&gt;组合键时，用户终端向正在运行中的由该终端启动的程序发出此信号。默认动作为终止里程。</li><li>SIGQUIT:当用户按下&lt;ctrl+>组合键时产生该信号，用户终端向正在运行中的由该终端启动的程序发出些信号。默认动作为终止进程。</li><li>SIGILL:CPU检测到某进程执行了非法指令。默认动作为终止进程并产生core文件 </li><li>SIGTRAP:该信号由断点指令或其他trap指令产生。默认动作为终止里程并产生core文件。</li><li>SIGABRT:调用abort函数时产生该信号。默认动作为终止进程并产生core文件。 </li><li>SIGBUS:非法访问内存地址，包括内存对齐出错，默认动作为终止进程并产生core文件。 8. SIGFPE:在发生致命的运算错误时发出。不仅包括浮点运算错误，还包括溢出及除数为0等所有的算法错误。默认动作为终止进程并产生core文件。 </li><li>SIGKILL:无条件终止进程。本信号不能被忽略，处理和阻塞。默认动作为终止进程。它向系统管理员提供了可以杀死任何进程的方法。</li><li>SIGUSE1:用户定义的信号。即程序员可以在程序中定义并使用该信号。默认动作为终止进程。 </li><li>SIGSEGV:指示进程进行了无效内存访问。默认动作为终止进程并产生core文件。 </li><li>SIGUSR2:这是另外一个用户自定义信号，程序员可以在程序中定义并使用该信号。默认动作为终止进程。</li><li>SIGPIPE:Broken pipe向一个没有读端的管道写数据。默认动作为终止进程。</li><li>SIGALRM:定时器超时，超时的时间由系统调用alarm设置。默认动作为终止进程。 </li><li>SIGTERM:程序结束信号，与SIGKILL不同的是，该信号可以被阻塞和终止。通常用来要示程序正常退出。执行 shell命令Kill时，缺省产生这个信号。默认动作为终止进程。 </li><li>SIGCHLD:子进程结束时，父进程会收到这个信号。默认动作为忽略这个信号。 </li><li>SIGCONT:停止进程的执行。信号不能被忽略，处理和阻塞。默认动作为终止进程。 </li><li>SIGTTIN:后台进程读终端控制台。默认动作为暂停进程。 </li><li>SIGTSTP:停止进程的运行。按下&lt;ctrl+z&gt;组合键时发出这个信号。默认动作为暂停进程。 </li><li>SIGTTOU:该信号类似于SIGTTIN，在后台进程要向终端输出数据时发生。默认动作为暂停进程。 </li><li>SIGURG:套接字上有紧急数据时，向当前正在运行的进程发出些信号，报告有紧急数据到达。如网络带外数据到达，默认动作为忽略该信号。</li><li>SIGXFSZ:进程执行时间超过了分配给该进程的CPU时间，系统产生该信号并发送给该进程。默认动作为终止进程。</li><li>SIGXFSZ:超过文件的最大长度设置。默认动作为终止进程。 </li><li>SIGVTALRM:虚拟时钟超时时产生该信号。类似于SIGALRM，但是该信号只计算该进程占用CPU的使用时间。默认动作为终止进程。 </li><li>SGIPROF:类似于SIGVTALRM，它不公包括该进程占用CPU时间还包括执行系统调用时间。默认动作为终止进程。</li><li>SIGWINCH:窗口变化大小时发出。默认动作为忽略该信号。 </li><li>SIGIO:此信号向进程指示发出了一个异步IO事件。默认动作为忽略。 </li><li>SIGPWR:关机。默认动作为终止进程。 </li><li>SIGSYS:无效的系统调用。默认动作为终止进程并产生core文件。 </li><li>SIGRTMIN~(64)SIGRTMAX:LINUX的实时信号，它们没有固定的含义(可以由用户自定义)。所有的实时信号的默认动作都为终止进程。</li></ol><h4 id="信号处理"><a href="#信号处理" class="headerlink" title="信号处理"></a>信号处理</h4><h5 id="信号处理行为"><a href="#信号处理行为" class="headerlink" title="信号处理行为"></a>信号处理行为</h5><p>进程处理信号的行为分为以下几种方式：</p><ol><li><p>SIG_DFL  忽略信号</p></li><li><p>默认操作；由内核预定义的默认操作取决于信号的类型，可以是以下类型之一：</p><p> Treminate：进程被终止（杀死）</p><p> Dump：进程被终止（杀死），如果可能，创建包含进程执行上下文的核心转储文件</p><p> Ignore：信号被忽略</p><p> Stop：进程被停止，即把进程置为TASK_STOPPED状态</p><p> Continue：如果进程被停止，就把它设置为TASK_RUNNING状态</p></li><li><p>通过调用相应的信号处理函数捕捉信号（自定义信号捕捉）</p></li></ol><h5 id="信号处理流程"><a href="#信号处理流程" class="headerlink" title="信号处理流程"></a>信号处理流程</h5><p>信号的处理动作是用户自定义的函数，在信号递达时就调用这个函数，这称为信号捕捉。由于信号处理函数的代码是在用户空间执行的，处理过程较为复杂，我们画图来进行解释：</p><img src="/FuckCode/2019/01/08/Linux系统编程之信号基础/信号捕捉.png" class="信号捕捉"><p>上图很好的说明了信号捕捉时用户态和内核态的切换（用户处理信号最好的时机是程序从内核态切换至用户态的时候），下面就上图的一系列操作作以解释说明：</p><ol><li>用户程序注册了SIGQUIT信号的处理函数sighandler（自定义信号处理函数）。</li><li>当前正在执行main函数，这里发生中断、异常或者系统调用切换至内核态。</li><li>在内核中断处理完毕后要返回用户态的main函数之前，检查进程中有信号SIGQUIT递达。</li><li>内核决定返回用户态后不是恢复main函数的上下文信息继续执行，而是执行sighandler函数，sighandler函数和main函数使用不同的堆栈空间，两者之间不存在调用和被调用的关系，属于两个独立的控制流程。</li><li>sighandler函数返回后自动执行特殊的系统调用，调用4的sigreturn再次进入内核态。</li><li>内核中如果没有新的信号要递达，再返回用户态就是恢复main函数的上下文继续向下执行。</li></ol><p><a href="https://blog.csdn.net/ypt523/article/details/80365108" target="_blank" rel="noopener">参考链接</a></p><p>从上面可以看出，对于信号捕捉函数的调用时机是异步的，简而言之，就是进程收到信号了不一定马上执行，只有调度的进程处于内核状态中，在即将返回用户空间时候，才会监测进程中PCB对应的信号区域是否有待处理信号存在，如果存在，先返回到信号处理函数，处理完成返回内核再次检测，之后才真正返回原来程序被中断的位置。</p><p>由此可以看出，如果进程处于长时间用户空间中，即长时间无中断、异常或系统调用进入内核态，也就是说无法由内核态转到用户空间，这种情况下，即使信号区域有信号记录，也不会调用信号捕捉函数。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本章节大概介绍了信号基本知识，后续会介绍信号编程函数以及信号机制带来的若干问题。</p><p><a href="www...">链接</a></p><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Linux下信号涉及知识点非常多，本章节先做一个铺垫，简单介绍一下信号种类及其含义，信号的几种默认处理方式等，后续再来介绍具体的信号处理函数以及注意事项也就方便很多了。工欲善其事，必先利其器。Let us get it！&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="信号" scheme="https://891904833.gitee.io/FuckCode/tags/%E4%BF%A1%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之静态/动态库生成实战</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/07/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E9%9D%99%E6%80%81-%E5%8A%A8%E6%80%81%E5%BA%93%E7%94%9F%E6%88%90%E5%AE%9E%E6%88%98/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/07/Linux系统编程之静态-动态库生成实战/</id>
    <published>2019-01-07T08:40:22.000Z</published>
    <updated>2019-01-07T10:12:55.963Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本节就如何创建和使用程序库进行论述。所谓” 程序库”，简单说，就 是包含了数据和执行码的文件。其不能单独执行，可以作为其它执行程序的一部分来完成某些功能。库的存在，可以使得程序模块化，可以加快程序的再编译，可以实现代码重用，可以使得程序便于升级。程序库可分静态库 (static library) 和共享库 (shared object)。</strong><br><a id="more"></a></p><h3 id="静态库"><a href="#静态库" class="headerlink" title="静态库"></a>静态库</h3><p>静态库是在可执行程序运行前就已经加入到执行码中，成为执行程序的一部分。静态库可以认为是一些目标代码的集合。按照习惯，一般以”.a” 做为文件后缀名。使用 ar(archiver) 命令可以创建静态库。因为共享库有着更大的优势，静态库已经不经常使用。但静态库使用简单，仍有使用的余地，并会一直存在。有些 Unix 系统，如 Solaris 10，已经基本废弃了静态库。<br>静态库在应用程序生成时，可以不必再编译，节省再编译时间。但在编译器越来越快的今天，这一点似乎已不重要。如果其他开发人员要使用你的程序，而你又不想给其源码，提供静态库是一种选择。从理论上讲，应用程序使用了静态库，要比使用动态加载库速度快 1-5%，但实际上可能并非如此。由此看来，除了使用方便外，静态库可能并非一种好的选择。</p><p>要创建一个静态库，或要将目标代码加入到已经存在的静态库中，可以 使用以下命令:** 静态库</p><pre><code>ar rcs libmylib.a file1.o file2.o </code></pre><p>以上表示要把目标码 file1.o 和 file2.o 加入到静态库 libmylib.a 中 (ar 的参数 r)。若 libmylib.a 不存在，会自动创建 (ar 的参数 c)。然后更新.a 文件的索引，使之包含新加入的.o 文件的内容 (ar 的参数 s)。</p><p>静态库创建成功后，需要链接到应用程序中使用。使用 gcc 的 -l 选项来指定静态库，使用 -L 参数来指定库文件的搜索路径。比如上述例子应指定 -lmylib，所有库文件名都以 lib 开头，开头的 lib 在指定参数时应省略。-l 和 -L 之后都直接带参数而不跟空格。<br>在使用 gcc 时，要注意其参数的顺序。-l 是链接器选项，一定要放在被编译的文件名称之后;若放在文件名称之前则会连接失败，并会出现莫名其妙的错误。下面来实战一下。</p><h4 id="生成实例"><a href="#生成实例" class="headerlink" title="生成实例"></a>生成实例</h4><p>通用步骤：</p><ol><li><p>通过代码生成 *.o 编译后文件 </p><pre><code>gcc -c -fPIC (-Iinclude) *c -I 为包含头文件路径，include 为 *.c 源码包含的头文件路径</code></pre></li><li><p>使用 ar 将 <em>.o 打包成 </em>.a 静态库</p><pre><code>ar rcs lib*.a *.o可以使用 nm -a *.a 查看静态库内容</code></pre></li><li><p>将 *.a 打包到应用程序中直接使用</p><pre><code>gcc main.c (-Iinclude) lib/lib*.a -o app-I 包含头文件路径，lib*.a 为静态库名，前面的 lib 为静态库的路径</code></pre></li></ol><h4 id="代码及路径"><a href="#代码及路径" class="headerlink" title="代码及路径"></a>代码及路径</h4><p>库头文件</p><pre><code>路径： /include/math.h#include &lt;stdio.h&gt;int add(int i,int j);int sub(int i, int j);</code></pre><p>库实现文件</p><pre><code>路径： /src/math.c#include &quot;../include/math.h&quot;int add(int i,int j){    return i+j;}int sub(int i,int j){    return i*j;}</code></pre><p>调用文件</p><pre><code>路径： ./main.c#include &lt;stdio.h&gt;#include &quot;include/math.h&quot;int main(void){    int a = add(5,4);    printf(&quot;%d\n&quot;,a);    int s = sub(5,4);    printf(&quot;%d\n&quot;,s);    return 0;}</code></pre><h4 id="编译流程"><a href="#编译流程" class="headerlink" title="编译流程"></a>编译流程</h4><ol><li><p>将 <em>.c 生成 </em>.o 中间文件</p><pre><code>gcc -c -fPIC -Iinclude ./srcmath.c当前路径位于main.c, 生成 math.o </code></pre></li><li><p>根据 <em>.o 生成对应的 </em>.a 静态库</p><pre><code>ar rcs libmath.a math.o文件下生成 libmath.a 静态库文件查看静态内容    nm -a libmath.a </code></pre></li><li><p>将 *.a 打包进入可执行文件中</p><pre><code>gcc main.c (-Iinclude) lib/libmath.a -o app步骤2将生成静态库放到了主目录下lib下</code></pre></li><li><p>执行查看结果</p><pre><code>执行结果输出Allies:soku rememberme$ ./app920</code></pre></li></ol><h3 id="动态库"><a href="#动态库" class="headerlink" title="动态库"></a>动态库</h3><p>共享库，是在执行程序启动时加载到执行程序中，可以被多个执行程序共享使用。建议库开发人员创建共享库，比较明显的优势在于库是独立的，便于维护和更新;而静态库的更新比较麻烦，一般不做推荐。然而，它们又各有优点，后面会讲到。</p><p>Linux下的库均采用的是动态库形式，便于更新以及各个程序都可无缝调用，而且还节省空间，优点出众，下面我们来看如何生成动态库并实施运行。</p><p>共享库的创建比较简单，基本有两步。首先使用 -fPIC 或 -fpic 创建目标文件，PIC 或 pic 表示位置无关代码，然后就可以使用以下格式创建共享库了:</p><pre><code>gcc -share -Wl,-soname,lib*.so.1 -o lib*.so.1.01 *.o</code></pre><p>注意：</p><ol><li>-Wl,-soname,lib*.so.1之间没有空格</li><li>*.o 后面可以跟 .o 多个文件</li><li><p>对于 so 库的默认名字的解释</p><pre><code>real name:  lib*.so.1.01    真实的so库名 so name:    lib*.so.1       只含有大版本的so库名link name:  lib*.so         编译链接阶段使用，方便Makefile管理link name 使用命令 ln -s lib*.so.1.01 lib.*.so 来创建</code></pre></li></ol><h4 id="生成实例-1"><a href="#生成实例-1" class="headerlink" title="生成实例"></a>生成实例</h4><p>通用步骤：</p><ol><li><p>源库文件 <em>.c 生成 </em>.o 中间文件</p><pre><code>gcc -c -fPIC (-Include) *.c</code></pre></li><li><p>将 <em>.o 生成动态库 </em>.so</p><pre><code>gcc -share -Wl,-soname,lib*.so.1 -o lib*.so.1.01 *.o</code></pre></li><li><p>将动态库链接到可执行主文件中</p><pre><code>gcc main.c lib/lib*.so -o app已建立动态库 real name 的 link name 库</code></pre></li><li><p>设置共享库配置文件</p><pre><code>1. 打开指定配置文件sudo vim /etc/ld.conf2. 添加动态库所在路径3. 更新动态库配置文件sudo ldconfig -v </code></pre></li></ol><h4 id="编译流程-1"><a href="#编译流程-1" class="headerlink" title="编译流程"></a>编译流程</h4><ol><li><p>源库文件 <em>.c 生成 </em>.o 中间文件</p><pre><code>gcc -c -fPIC -Include ./src/math.c</code></pre></li><li><p>将 <em>.o 生成动态库 </em>.so</p><pre><code>gcc -shared -Wl,-soname,libmath.so.1 -o libmath.so.1.01 math.olinux端测试成功，mac端失败，使用如下命令（不含版本号信息）gcc -shared -Wl -o libmath.so math.o</code></pre></li><li><p>将动态库链接到可执行主文件中</p><pre><code>gcc main.c lib/lib*.so -o app1已建立动态库 real name 的 link name 库ldd app1 测试程序可否找到共享库</code></pre></li><li><p>设置共享库配置文件（Linux配置，Mac测试无需配置）</p><pre><code>1. 打开指定配置文件sudo vim /etc/ld.conf2. 添加动态库所在的绝对路径/home/allies/linux/shared/lib3. 更新动态库配置文件sudo ldconfig -v </code></pre></li><li><p>运行查看结果</p><pre><code>Allies:soku rememberme$ ./app1920</code></pre></li></ol><h4 id="Linux下共享库的加载"><a href="#Linux下共享库的加载" class="headerlink" title="Linux下共享库的加载"></a>Linux下共享库的加载</h4><p>在所有基于 GNUglibc 的系统中，在启动一个 ELF 二进制执行程序时， 一个特殊的程序”程序装载器”会被自动装载并运行。在 linux 中，这个程序装载器就是/lib/ld-linux.so.X(X 是版本号)。它会查找并装载应用程序所依赖的所有共享库。被搜索的目录保存在/etc/ld.so.conf 文件中。当然，如果程序的每次启动，都要去搜索一番，势必效率不堪忍受。Linux 系统已经考虑这一点，对共享库采用了缓存管理。ldconfig 就是实现这一功能的工具，其缺省读取/etc/ld.so.conf 文件，对所有共享库按照一定规范建立符号连接，然后将信息写入 /etc/ld.so.cache。/etc/ld.so.cache 的存在大大加快了程序的启动速度。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>共享库下静态库和动态库都已实现，总结一下两者的优缺点：</p><table><thead><tr><th style="text-align:center">共享库</th><th style="text-align:center">优点</th><th style="text-align:center">缺点</th></tr></thead><tbody><tr><td style="text-align:center">静态库</td><td style="text-align:center">库文件直接打入代码中，linux下无需修改动态库配置文件直接使用</td><td style="text-align:center">代码体积比较大，升级迭代麻烦</td></tr><tr><td style="text-align:center">动态库</td><td style="text-align:center">代码中使用相对地址寻址，值友符号记录表和库版本号，升级迭代容易，代码体积小</td><td style="text-align:center">主机必须含有共享库，且版本一致，linux下需要修改动态库搜寻配置文件</td></tr></tbody></table><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;本节就如何创建和使用程序库进行论述。所谓” 程序库”，简单说，就 是包含了数据和执行码的文件。其不能单独执行，可以作为其它执行程序的一部分来完成某些功能。库的存在，可以使得程序模块化，可以加快程序的再编译，可以实现代码重用，可以使得程序便于升级。程序库可分静态库 (static library) 和共享库 (shared object)。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="共享库" scheme="https://891904833.gitee.io/FuckCode/tags/%E5%85%B1%E4%BA%AB%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之进程间通信</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/07/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/07/Linux系统编程之进程间通信/</id>
    <published>2019-01-07T03:42:31.000Z</published>
    <updated>2019-01-07T07:17:02.120Z</updated>
    
    <content type="html"><![CDATA[<p><strong>之前文件介绍了Linux下进程的创建，执行以及回收，此章节就进程间通信进行详细介绍。Linux提供了多种进程间通信机制，本文会分别介绍匿名管道pipe，有名管道fifo，内存共享映射机制以及Socket套接字等。通过其中任意一种机制，都能够方便两个进程间进行通信操作，极大方便开发者的开发效率。</strong><br><a id="more"></a></p><h3 id="进程间通信"><a href="#进程间通信" class="headerlink" title="进程间通信"></a>进程间通信</h3><p>在Linux中，每个进程各自有不同的用户地址空间，任何一个进程的全局变量在另一个进程中都看不 到，所以进程之间要交换数据必须通过内核，在内核中开辟一块缓冲区，进程1把数据从用 户空间拷到内核缓冲区，进程2再从内核缓冲区把数据读走，内核提供的这种机制称为进程 间通信(IPC，InterProcess Communication)。</p><p>Linux多进程间通信有多种方式，最简单的就是父子间的管道通信（父子进程共享文件描述符），然后是升级版的有名管道（指定管道描述符），其次是高效的内存共享映射和基于网络的Socket套接字。</p><h4 id="pipe管道"><a href="#pipe管道" class="headerlink" title="pipe管道"></a>pipe管道</h4><p>管道pipe提供了父子间进行通信的简单方式，使用起来也是相反方便。管道作用于有血缘关系的进程之间,通过fork来传递。</p><h5 id="原型"><a href="#原型" class="headerlink" title="原型"></a>原型</h5><pre><code>#include &lt;unistd.h&gt;int pipe(int filedes[2]);</code></pre><h5 id="参数解释"><a href="#参数解释" class="headerlink" title="参数解释"></a>参数解释</h5><p>filedes[2]:  整型数组2个大小，一个为读端，一个为写端。pipe的单条管道只能实现单工通信，如果要实现双向通信，需要创建两条pipe管道。</p><h5 id="相关说明"><a href="#相关说明" class="headerlink" title="相关说明"></a>相关说明</h5><p>先调用pipe函数时在内核中开辟一块缓冲区(称为管道)用于通信，它有一个读端一个写端，然后通过filedes参数传出给用户程序两个文件描述符，filedes[0]指向管道的读端，filedes[1]指向管道的写端。之后通过fork得到子进程，共享之前创建的文件描述符实现父子血缘进程之间通信。</p><p>编程流程：</p><ol><li>父进程调用pipe开辟管道，得到两个文件描述符指向管道的两端。 </li><li>父进程调用fork创建子进程，那么子进程也有两个文件描述符指向同一管道。 </li><li>父进程关闭管道读端，子进程关闭管道写端。父进程可以往管道里写，子进程可以从管道里读，管道是用环形队列实现的，数据从写端流入从读端流出，这样就实现了进程间通信。</li></ol><p>使用限制：</p><ol><li>只支持单向数据流；</li><li>只能用于具有亲缘关系的进程之间；</li><li>没有名字；</li><li>管道的缓冲区是有限的（管道制存在于内存中，在管道创建时，为缓冲区分配一个页面大小）；</li><li>管道所传送的是无格式字节流，这就要求管道的读出方和写入方必须事先约定好数据的格式，比如多少字节算作一个消息（或命令、或记录）等等；</li></ol><p>4种特殊情况(假设都是阻塞I/O操作，没有设置O_NONBLOCK标志):</p><ol><li>所有指向管道写端的文件描述符都关闭了(管道写端的引用计数等于0)，而仍然有进程从管道的读端读数据，那么管道中剩余的数据都被读取后，再次read会返回0，就像读到文件末尾一样。</li><li>存在指向管道写端的文件描述符没关闭(管道写端的引用计数大于0)，而持有管道写端的进程也没有向管道中写数据，这时有进程从管道读端读数据，那么管道中剩余的数据都被读取后，再次read会阻塞，直到管道中有数据可读了才读取数据并返回。</li><li>所有指向管道读端的文件描述符都关闭了(管道读端的引用计数等于0)，这时有进程向管道的写端write，那么该进程会收到信号SIGPIPE，通常会导致进程异常终止。</li><li>存在指向管道读端的文件描述符没关闭(管道读端的引用计数大于0)，而持有管道读端的进程也没有从管道中读数据，这时有进程向管道写端写数据，那么在管道被写满时再次write会阻塞，直到管道中有空位置了才写入数据并返回。</li></ol><h5 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h5><p>阻塞式通信</p><pre><code>#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;wait.h&gt;#define SIZE 64int main(){    int pipeFd[2];    char str[] = &quot;This is test pipe content!\n&quot;;    char red[SIZE];    // 创建管道    int ret = pipe(pipeFd);    if(ret &lt; 0)    {        perror(&quot;pipe error!&quot;);        exit(1);    }    pid_t pid = fork();    if(pid &gt;0){        // 父进程，父写子读，关闭父读接口        close(pipeFd[0]);        sleep(3);        write(pipeFd[1],str,strlen(str));        // 阻塞等待子进程退出        wait(NULL);    }    else if(pid == 0)    {        // 子进程，父写子读，关闭子写接口        close(pipeFd[1]);        int len = read(pipeFd[0],red,sizeof(red));        write(STDOUT_FILENO,red,len);    }    return 0;}</code></pre><p>非阻塞式通信</p><pre><code>#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;wait.h&gt;#include &lt;fcntl.h&gt;#include &lt;errno.h&gt;#define SIZE 64int main(){    int pipeFd[2];    char str[] = &quot;This is test pipe content!\n&quot;;    char msg[] = &quot;Try again!\n&quot;;    char red[SIZE];    // 创建管道    int ret = pipe(pipeFd);    if(ret &lt; 0)    {        perror(&quot;pipe error!&quot;);        exit(1);    }    pid_t pid = fork();    if(pid &gt;0){        // 父进程，父写子读，关闭父读接口        close(pipeFd[0]);        sleep(5);        write(pipeFd[1],str,strlen(str));        close(pipeFd[1]);        // 阻塞等待子进程退出        wait(NULL);    }    else if(pid == 0)    {        // 子进程，父写子读，关闭子写接口        close(pipeFd[1]);        int flag, len;        // 设置管道为非阻塞状态        flag = fcntl(pipeFd[0],F_GETFL);        flag |= O_NONBLOCK;        fcntl(pipeFd[0],F_SETFL, flag);try_again:         len = read(pipeFd[0],red,sizeof(red));        if(len == -1){            if(errno == EAGAIN){                write(STDOUT_FILENO,msg,sizeof(msg));                sleep(1);                goto try_again;            }            else            {                perror(&quot;read error!&quot;);                exit(1);            }        }        write(STDOUT_FILENO,red,len);        close(pipeFd[0]);    }    return 0;}</code></pre><h4 id="fifo有名管道"><a href="#fifo有名管道" class="headerlink" title="fifo有名管道"></a>fifo有名管道</h4><p>管道应用的一个重大限制是它没有名字，因此，只能用于具有亲缘关系的进程间通信，在有名管道（named pipe或FIFO）提出后，该限制得到了克服。FIFO不同于管道之处在于它提供一个路径名与之关联，以FIFO的文件形式存在于文件系统中。这样，即使与FIFO的创建进程不存在亲缘关系的进程，只要可以访问该路径，就能够彼此通过FIFO相互通信（能够访问该路径的进程以及FIFO的创建进程之间），因此，通过FIFO不相关的进程也能交换数据。值得注意的是，FIFO严格遵循先进先出（first in first out），对管道及FIFO的读总是从开始处返回数据，对它们的写则把数据添加到末尾。它们不支持诸如lseek()等文件定位操作。</p><h5 id="函数原型"><a href="#函数原型" class="headerlink" title="函数原型"></a>函数原型</h5><pre><code>#include &lt;sys/types.h&gt; #include &lt;sys/stat.h&gt;int mkfifo(const char *pathname, mode_t mode);</code></pre><h5 id="参数解释-1"><a href="#参数解释-1" class="headerlink" title="参数解释"></a>参数解释</h5><p>pathname: 事先规定的节点名称，多个进程间知晓<br>mode: 节点工作方式，可读可写等</p><p>注意：</p><ol><li>当只写打开FIFO管道时，如果没有FIFO没有读端打开，则open写打开会阻塞。</li><li>FIFO内核实现时可以支持双向通信。(pipe单向通信，因为父子进程共享同一个file<br>结构体)</li><li>FIFO可以一个读端，多个写端;也可以一个写端，多个读端。</li><li>FIFO读端和写端需要协同工作，同时打开读写，阻塞式通信。</li><li>FIFO内部数据结构采用严格的先进先出规则。</li><li>其由内核在内核中分配空间，再在磁盘中生成相应的节点，此节点内无任何数据（大小为0），只作为节点凭证。</li></ol><h5 id="相关说明-1"><a href="#相关说明-1" class="headerlink" title="相关说明"></a>相关说明</h5><p>FIFO的打开规则：</p><ol><li><p>如果当前打开操作是为读而打开FIFO时，若已经有相应进程为写而打开该FIFO，则当前打开操作将成功返回；否则，可能阻塞直到有相应进程为写而打开该FIFO（当前打开操作设置了阻塞标志）；或者，成功返回（当前打开操作没有设置阻塞标志）。</p></li><li><p>如果当前打开操作是为写而打开FIFO时，如果已经有相应进程为读而打开该FIFO，则当前打开操作将成功返回；否则，可能阻塞直到有相应进程为读而打开该FIFO（当前打开操作设置了阻塞标志）；或者，返回ENXIO错误（当前打开操作没有设置阻塞标志）。</p></li></ol><p>从FIFO中读取数据：</p><p>约定：如果一个进程为了从FIFO中读取数据而阻塞打开FIFO，那么称该进程内的读操作为设置了阻塞标志的读操作。</p><ol><li>如果有进程写打开FIFO，且当前FIFO内没有数据，则对于设置了阻塞标志的读操作来说，将一直阻塞。对于没有设置阻塞标志读操作来说则返回-1，当前errno值为EAGAIN，提醒以后再试。</li><li><p>对于设置了阻塞标志的读操作说，造成阻塞的原因有两种：</p><ul><li><p>当前FIFO内有数据，但有其它进程在读这些数据；</p></li><li><p>另外就是FIFO内没有数据。解阻塞的原因则是FIFO中有新的数据写入，不论信写入数据量的大小，也不论读操作请求多少数据量。</p></li></ul></li><li><p>读打开的阻塞标志只对本进程第一个读操作施加作用，如果本进程内有多个读操作序列，则在第一个读操作被唤醒并完成读操作后，其它将要执行的读操作将不再阻塞，即使在执行读操作时，FIFO中没有数据也一样（此时，读操作返回0）。</p></li><li><p>如果没有进程写打开FIFO，则设置了阻塞标志的读操作会阻塞。</p></li><li>如果FIFO中有数据，则设置了阻塞标志的读操作不会因为FIFO中的字节数小于请求读的字节数而阻塞，此时，读操作会返回FIFO中现有的数据量。</li></ol><p>向FIFO中写入数据：</p><p>约定：如果一个进程为了向FIFO中写入数据而阻塞打开FIFO，那么称该进程内的写操作为设置了阻塞标志的写操作。</p><ul><li><p>对于设置了阻塞标志的写操作：</p><ol><li><p>当要写入的数据量不大于PIPE_BUF时，linux将保证写入的原子性。如果此时管道空闲缓冲区不足以容纳要写入的字节数，则进入睡眠，直到当缓冲区中能够容纳要写入的字节数时，才开始进行一次性写操作。</p></li><li><p>当要写入的数据量大于PIPE_BUF时，linux将不再保证写入的原子性。FIFO缓冲区一有空闲区域，写进程就会试图向管道写入数据，写操作在写完所有请求写的数据后返回。</p></li></ol></li><li><p>对于没有设置阻塞标志的写操作：</p><ol><li><p>当要写入的数据量大于PIPE_BUF时，linux将不再保证写入的原子性。在写满所有FIFO空闲缓冲区后，写操作返回。</p></li><li><p>当要写入的数据量不大于PIPE_BUF时，linux将保证写入的原子性。如果当前FIFO空闲缓冲区能够容纳请求写入的字节数，写完后成功返回；如果当前FIFO空闲缓冲区不能够容纳请求写入的字节数，则返回EAGAIN错误，提醒以后再写。</p></li></ol></li></ul><p><a href="http://blog.chinaunix.net/uid-25365622-id-3059840.html" target="_blank" rel="noopener">参考链接</a></p><h5 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h5><p>头文件</p><pre><code>#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#define FIFO_NAME  &quot;fifo&quot;char str[] = &quot;Test fifo file...\n&quot;;</code></pre><p>写端</p><pre><code>#include &quot;fifo.h&quot;int main(void){    // fifo文件存在检测    if(access(FIFO_NAME, F_OK)){        // 不存在文件，直接创建        int ret = mkfifo(FIFO_NAME,0644);        if (ret != 0)        {            /* code */            perror(&quot;mkfifo error!&quot;);            exit(1);        }    }    // 存在fifo文件，打开    int fd = open(FIFO_NAME,O_RDWR);    if (fd &lt; 0)    {        /* code */        perror(&quot;open fifo error!&quot;);        exit(1);    }    write(fd, str, strlen(str));    printf(&quot;write fifo ok! \n&quot;);    close(fd);    return 0;}</code></pre><p>读端</p><pre><code>#include &quot;fifo.h&quot;int main(void){    char buf[64];    // fifo文件存在检测    if(access(FIFO_NAME, F_OK)){        perror(&quot;fifo file is  not exist!&quot;);        exit(1);    }    // 存在fifo文件，打开    int fd = open(FIFO_NAME,O_RDONLY);    if (fd &lt; 0)    {        /* code */        perror(&quot;open fifo error!&quot;);        exit(1);    }    int len = read(fd, buf, sizeof(buf));    write(STDOUT_FILENO,buf,len);    close(fd);    return 0;}</code></pre><h4 id="内存共享映射"><a href="#内存共享映射" class="headerlink" title="内存共享映射"></a>内存共享映射</h4><p>介于所有的程序都是从磁盘读取到内存，然后由CPU在内存中进行读取修改等。因此，内存共享映射可以提供一个地址即可实现通信功能。mmap可以把磁盘文件的一部分直接映射到内存，这样文件中的位置直接就有对应的内存地址，对文件的读写可以直接用指针来做而不需read/write函数。</p><h5 id="函数原型-1"><a href="#函数原型-1" class="headerlink" title="函数原型"></a>函数原型</h5><pre><code>#include &lt;sys/mman.h&gt;void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset); int munmap(void *addr, size_t length);</code></pre><h5 id="参数解释-2"><a href="#参数解释-2" class="headerlink" title="参数解释"></a>参数解释</h5><p>addr: 如果addr参数为NULL，内核会自己在进程地址空间中选择合适的地址建立映射。如果 addr不是NULL，则给内核一个提示，应该从什么地址开始映射，内核会选择addr之上的某个 合适的地址开始映射。建立映射后，真正的映射首地址通过返回值可以得到。</p><p>length: 需要映射的那一部分文件的长度。</p><p>offset: 从文件的什么位置开始映射，必须是页大小的整数倍(在32位体系统结构上通常是4K)。</p><p>filedes: 代表映射文件的描述符。</p><p>prot: 参数有四种取值</p><pre><code>1. PROT_EXEC表示映射的这一段可执行，例如映射共享库2. PROT_READ表示映射的这一段可读3. PROT_WRITE表示映射的这一段可写4. PROT_NONE表示映射的这一段不可访问</code></pre><p>flag: 参数有很多种取值，这里只讲两种，其它取值可查看mmap(2)</p><pre><code>1. MAP_SHARED多个进程对同一个文件的映射是共享的，一个进程对映射的内存做了修 改，另一个进程也会看到这种变化。2. MAP_PRIVATE多个进程对同一个文件的映射不是共享的，一个进程对映射的内存做了修 改，另一个进程并不会看到这种变化，也不会真的写到文件中去。</code></pre><p>如果mmap成功则返回映射首地址，如果出错则返回常数MAP_FAILED，errno被设置。当进程终止时，该进程 的映射内存会自动解除，也可以调用munmap解除映射。munmap成功返回0，出错返回-1。</p><pre><code>EACCES：访问出错EAGAIN：文件已被锁定，或者太多的内存已被锁定EBADF：fd不是有效的文件描述词EINVAL：一个或者多个参数无效ENFILE：已达到系统对打开文件的限制ENOMEM：内存不足，或者进程已超出最大内存映射数量SIGSEGV：试着向只读区写入具体查看 Man Page</code></pre><h5 id="示例代码-1"><a href="#示例代码-1" class="headerlink" title="示例代码"></a>示例代码</h5><pre><code>#include &lt;sys/mman.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;string.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#define MMAP_FILE &quot;hello&quot;int main(void){    char buf[] = &quot;HELLOWORLD&quot;;    // mmap文件存在检测    if(access(MMAP_FILE, F_OK)){        perror(&quot;mmap file is  not exist!&quot;);        exit(1);    }    // 存在mmap文件，打开    int fd = open(MMAP_FILE,O_RDWR);    if (fd &lt; 0)    {        /* code */        perror(&quot;open mmap file error!&quot;);        exit(1);    }    // 获取文件大小    int length = lseek(fd,0,SEEK_END);    int *addr = mmap(NULL, length, PROT_WRITE, MAP_SHARED, fd, 0);    // 修改内存数据，查看是否同步到文件    memcpy(addr,buf,length &gt;= sizeof(buf)?sizeof(buf):length);    // 关闭文件    close(fd);    // 解除内存映射    munmap(addr,length);    return 0;}</code></pre><p>测试代码中，可以事先向hello文件中写入一部分数据，之后调用该程序进行内存映射修改文件数据，之后打开文件查看内容。如果原数据小于写入buf数据，原文件替换成新数据，数据大小增加，如果写入数据buf小于源文件数据，那么只替换原数据中前面需要替换的字节，文件大小不变。</p><p>在内存映射文件时候，可以使用lseek实现对文件读写指针的修改，控制读取写入数据的位置。后续实现多进程或多线程数据拷贝中可以使用到。</p><h4 id="Socket"><a href="#Socket" class="headerlink" title="Socket"></a>Socket</h4><p>Linux下的Socket套接字需要很大的篇章来介绍，这里险先立个flag，后续会着重介绍。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Linux下进程间通信通用的七种方式：</p><p>第一类：传统的unix通信机制：</p><ol><li>管道( pipe )：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。</li><li>有名管道 (named pipe) ： 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。</li><li>信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。</li></ol><p>第二类：System V IPC： </p><ol start="4"><li>信号量( semophore ) ： 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。</li><li>消息队列( message queue ) ： 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。</li><li>共享内存( shared memory ) ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。</li></ol><p>第三类：BSD 套接字：</p><ol start="7"><li>套接字( socket ) ： 套解字也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同及其间的进程通信。</li></ol><p><a href="http://blog.chinaunix.net/uid-25365622-id-3059840.html" target="_blank" rel="noopener">参考链接</a></p><blockquote><p>邢文鹏Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;之前文件介绍了Linux下进程的创建，执行以及回收，此章节就进程间通信进行详细介绍。Linux提供了多种进程间通信机制，本文会分别介绍匿名管道pipe，有名管道fifo，内存共享映射机制以及Socket套接字等。通过其中任意一种机制，都能够方便两个进程间进行通信操作，极大方便开发者的开发效率。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="跨进程通信" scheme="https://891904833.gitee.io/FuckCode/tags/%E8%B7%A8%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之进程</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/05/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E8%BF%9B%E7%A8%8B/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/05/Linux系统编程之进程/</id>
    <published>2019-01-05T01:33:30.000Z</published>
    <updated>2019-01-07T03:39:14.422Z</updated>
    
    <content type="html"><![CDATA[<p><strong>学习Linux中的进程，主要设计了三个重要的函数：fork（创建子进程），exec（执行命令），wait（回收进程资源）。进程的管理由内核中的task_struct结构体(PCB进程控制块)负责，下面我们来具体了解一下。</strong><br><a id="more"></a></p><h3 id="PCB及进程环境"><a href="#PCB及进程环境" class="headerlink" title="PCB及进程环境"></a>PCB及进程环境</h3><h4 id="进程控制块PCB"><a href="#进程控制块PCB" class="headerlink" title="进程控制块PCB"></a>进程控制块PCB</h4><p>我们知道，每个进程在内核中都有一个进程控制块(PCB)来维护进程相关的信息，Linux内核的进程控制块是task_struct结构体。现在我们全面了解一下其中都有哪些信息。</p><ul><li>进程id。系统中每个进程有唯一的id，在C语言中用pid_t类型表示，其实就是一个非负整数。</li><li>进程的状态，有运行、挂起、停止、僵尸等状态。</li><li>进程切换时需要保存和恢复的一些CPU寄存器。</li><li>描述虚拟地址空间的信息。</li><li>描述控制终端的信息。</li><li>当前工作目录(Current Working Directory)。</li><li>umask掩码。</li><li>文件描述符表，包含很多指向file结构体的指针。</li><li>和信号相关的信息。</li><li>用户id和组id。</li><li>控制终端、Session和进程组。</li><li>进程可以使用的资源上限(Resource Limit)</li></ul><h4 id="进程环境"><a href="#进程环境" class="headerlink" title="进程环境"></a>进程环境</h4><p>每个进程在运行时，都会涉及到默认的环境变量。按照惯例，环境变量字符串都是name=value这样的形式，大多数name由大写字母加下划线组成，一般把name的部分叫做环境变量，value的部分则是环境变量的值。环境变量定义了进程的运行环境，一些比较重要的环境变量的含义如下:</p><p>PATH</p><ul><li>可执行文件的搜索路径。ls命令也是一个程序，执行它不需要提供完整的路径名/bin/ ls，然而通常我们执行当前目录下的程序a.out却需要提供完整的路径名./a.out，这 是因为PATH环境变量的值里面包含了ls命令所在的目录/bin，却不包含a.out所在的目录。PATH环境变量的值可以包含多个目录，用:号隔开。在Shell中用echo命令可以查 看这个环境变量的值:$ echo $PATH </li></ul><p>SHELL</p><ul><li>当前Shell，它的值通常是/bin/bash。 </li></ul><p>TERM</p><ul><li>当前终端类型，在图形界面终端下它的值通常是xterm，终端类型决定了一些程序的输 出显示方式，比如图形界面终端可以显示汉字，而字符终端一般不行。</li></ul><p>LANG</p><ul><li>语言和locale，决定了字符编码以及时间、货币等信息的显示格式。</li></ul><p>HOME</p><ul><li>当前用户主目录的路径，很多程序需要在主目录下保存配置文件，使得每个用户在运 行该程序时都有自己的一套配置。</li></ul><h4 id="环境变量相关函数"><a href="#环境变量相关函数" class="headerlink" title="环境变量相关函数"></a>环境变量相关函数</h4><p>libc中定义的全局变量environ指向环境变量表，environ没有包含在任何头文件中，所<br>以在使用时要用extern声明。例如：</p><pre><code>#include &lt;stdio.h&gt;int main(void) {    extern char **environ;    int i;    for(i=0; environ[i]!=NULL; i++)    printf(&quot;%s\n&quot;, environ[i]); return 0;}</code></pre><p>用environ指针可以查看所有环境变量字符串，但是不够方便，如果给出name要在环境变量 表中查找它对应的value，可以用getenv函数。</p><h5 id="获取环境变量getenv"><a href="#获取环境变量getenv" class="headerlink" title="获取环境变量getenv"></a>获取环境变量getenv</h5><pre><code>#include &lt;stdlib.h&gt;char *getenv(const char *name); getenv的返回值是指向value的指针，若未找到则为NULL。</code></pre><h4 id="设置及取消"><a href="#设置及取消" class="headerlink" title="设置及取消"></a>设置及取消</h4><pre><code>#include &lt;stdlib.h&gt;int setenv(const char *name, const char *value, int rewrite); void unsetenv(const char *name);putenv和setenv函数若成功则返回为0，若出错则返回非0。</code></pre><p>setenv将环境变量name的值设置为value。如果已存在环境变量name，那么若rewrite非0，则覆盖原来的定义; 若rewrite为0，则不覆盖原来的定义，也不返回错误。unsetenv删除name的定义。即使name没有定义也不返回错误。</p><h4 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h4><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main(void){    // 获取    char* path = getenv(&quot;PATH&quot;);    printf(&quot;PATH : %s \n&quot;, path);    // 设置    setenv(&quot;PATH&quot;,&quot;what fuck you!&quot;,1);    printf(&quot;NEW PATH: %s\n&quot;,getenv(&quot;PATH&quot;));    return 0;}</code></pre><h4 id="进程资源限制"><a href="#进程资源限制" class="headerlink" title="进程资源限制"></a>进程资源限制</h4><p>PCB由内核实施管理，其为每个进程分配的资源大小也是固定有限的，不能无限制的增加，一般来说软限制不得大于硬限制，设置进程资源的函数如下（root权限）：</p><pre><code>#include &lt;sys/time.h&gt; #include &lt;sys/resource.h&gt;int getrlimit(int resource, struct rlimit *rlim);int setrlimit(int resource, const struct rlimit *rlim)；</code></pre><p>查看资源限制</p><pre><code>cat /proc/self/limltsulimit -a</code></pre><p>设置进程打开文件上限值</p><pre><code>ulimit -n ?</code></pre><h3 id="进程原语"><a href="#进程原语" class="headerlink" title="进程原语"></a>进程原语</h3><p>linux涉及进程的函数主要包括复制进程fork，执行命令exec以及回收进程资源wait（waitpid）。程序一般的流程是通过fork得到一个子进程，然后再子进程中执行exec命令操作，之后通过wait或者waitpid回收进程执行的结果。</p><h4 id="进程状态"><a href="#进程状态" class="headerlink" title="进程状态"></a>进程状态</h4><p>linux中进程存在多种状态，运行，就绪，睡眠，停止，各种状态存在转换关系，例如运行态可以切换到任意其他三种状态，具体切换状态如下：</p><p>运行： 可以切换到就绪，睡眠，停止任意状态<br>就绪： 可以切换到就绪，停止，无法直接切换到运行<br>就绪： 同运行，可以切换任意其他状态<br>停止： 无法切换状态</p><h5 id="相关命令"><a href="#相关命令" class="headerlink" title="相关命令"></a>相关命令</h5><p>查看系统所有程序： ps aux<br>动态查看进程： top<br>进程组查看： ps ajx</p><h4 id="fork"><a href="#fork" class="headerlink" title="fork"></a>fork</h4><p>fork的作用是根据一个现有的进程复制出一个新进程，原来的进程称为父进程(Parent Process)，新进程称为子进程(Child Process)。系统中同时运行着很多进程，这些进程都是从最初只有一个进程开始一个一个复制出来的。在Shell下输入命令可以运行一个程序，是因为Shell进程在读取用户输入的命令之后会调用fork复制出一个新的Shell进程，然后新的Shell进程调用exec执行新的程序。</p><h5 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h5><pre><code>#include &lt;unistd.h&gt;pid_t fork(void);</code></pre><h5 id="参数介绍"><a href="#参数介绍" class="headerlink" title="参数介绍"></a>参数介绍</h5><p>void</p><h5 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h5><p>fork调用一次，返回两次。调用成功后，父进程中返回0，子进程中返回系统分配的pid（非负整数）。如果调用失败，父进程中返回-1，并且设置errno，子进程不被创建。</p><pre><code>EAGAIN: 当前的进程数已经达到了系统规定的上限，这时errno的值被设置为ENOMEM: 系统内存不足，这时errno的值被设置为</code></pre><h5 id="相关说明"><a href="#相关说明" class="headerlink" title="相关说明"></a>相关说明</h5><ol><li>fork调用一次，返回两次，成功时，父进程中返回0，子进程中返回进程id。</li><li>fork执行后，父子进程的执行代码直接从返回值处pid开始，父子进程的执行的先后顺序不确定，由操作系统调度。父进程先于子进程结束，子进程的父进程转换成init守护进程。</li><li>父子进程的拷贝采用读时共享，写时复制机制（copy on write）。</li></ol><h5 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h5><pre><code>#include &lt;sys/types.h&gt; #include &lt;unistd.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt;int main(void) {    pid_t id = fork();    pid_t pid; char *message; int n;    pid = fork();     if (pid &lt; 0) {        perror(&quot;fork failed&quot;);        exit(1);    }    if (pid == 0) {        message = &quot;This is the child\n&quot;; n = 6;    } else {        message = &quot;This is the parent\n&quot;; n = 3;    }    for(; n &gt; 0; n--) {        printf(message);        sleep(1);     }    return 0; }</code></pre><p>fork调用完成后，此时父子进程都从if判断开始，由于父子进程执行顺序不确定，在各自逻辑处理完成后，睡眠等待 1s 后，在结束父进程，后续可以通过wait函数进行回收进程资源。其代码执行流程见下图：</p><img src="/FuckCode/2019/01/05/Linux系统编程之进程/fork.png" class="fork执行流程"><h4 id="exec"><a href="#exec" class="headerlink" title="exec"></a>exec</h4><p>用fork创建子进程后执行的是和父进程相同的程序(但有可能执行不同的代码分支)，子进程往往要调用一种exec函数以执行另一个程序。当进程调用一种exec函数时，该进程的用户空间代码和数据完全被新程序替换，从新程序的启动例程开始执行。调用exec并不创建新进程，所以调用exec前后该进程的id并未改变。其实有六种以exec开头的函数，统称exec函数:</p><pre><code>#include &lt;unistd.h&gt;int execl(const char *path, const char *arg, ...);int execlp(const char *file, const char *arg, ...);int execle(const char *path, const char *arg, ..., char *const envp[]）; int execv(const char *path, char *const argv[]);int execvp(const char *file, char *const argv[]);int execve(const char *path, char *const argv[], char *const envp[]);</code></pre><p>这些函数如果调用成功则加载新的程序从启动代码开始执行，不再返回，如果调用出错，则返回-1，所以exec函数只有出错的返回值而没有成功的返回值。</p><h5 id="相关说明-1"><a href="#相关说明-1" class="headerlink" title="相关说明"></a>相关说明</h5><p>exec函数族记忆方式：</p><pre><code>l 命令行参数列表p 搜素file时使用path变量v 使用命令行参数数组e 使用环境变量数组,不使用进程原有的环境变量，设置新加载程序运行的环境变量execvp常用，使用数组将命令组织，调用时直接使用系统路径进行</code></pre><p>代码举例:</p><pre><code>char *const ps_argv[] ={&quot;ps&quot;, &quot;-o&quot;, &quot;pid,ppid,pgrp,session,tpgid,comm&quot;, NULL}; char *const ps_envp[] ={&quot;PATH=/bin:/usr/bin&quot;, &quot;TERM=console&quot;, NULL}; execl(&quot;/bin/ps&quot;, &quot;ps&quot;, &quot;-o&quot;, &quot;pid,ppid,pgrp,session,tpgid,comm&quot;, NULL); execv(&quot;/bin/ps&quot;, ps_argv);execle(&quot;/bin/ps&quot;, &quot;ps&quot;, &quot;-o&quot;, &quot;pid,ppid,pgrp,session,tpgid,comm&quot;, NULL, ps_envp); execve(&quot;/bin/ps&quot;, ps_argv, ps_envp);execlp(&quot;ps&quot;, &quot;ps&quot;, &quot;-o&quot;, &quot;pid,ppid,pgrp,session,tpgid,comm&quot;, NULL);execvp(&quot;ps&quot;, ps_argv);</code></pre><p>事实上，只有execve是真正的系统调用，其它五个函数最终都调用execve，所以execve 在man手册第2节，其它函数在man手册第3节。这些函数之间的关系如下图所示:</p><img src="/FuckCode/2019/01/05/Linux系统编程之进程/exec.png" class="fork执行流程"><h5 id="示例代码-1"><a href="#示例代码-1" class="headerlink" title="示例代码"></a>示例代码</h5><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;int main(){    pid_t id = fork();    if(id == 0)    {        // 子进程执行打开网页        execlp(&quot;firefox&quot;,&quot;firefox&quot;,&quot;www.baidu.com&quot;,NULL);    }    else if(id &gt; 0)    {        // 父进程执行 ls        execl(&quot;/bin/ls&quot;,&quot;ls&quot;,&quot;-la&quot;, &quot;./../&quot;,NULL);    }    return 0;}</code></pre><h4 id="wait-waitpid"><a href="#wait-waitpid" class="headerlink" title="wait(waitpid)"></a>wait(waitpid)</h4><p>一个进程在终止时会关闭所有文件描述符，释放在用户空间分配的内存，但它的PCB还保留着，内核在其中保存了一些信息:如果是正常终止则保存着退出状态，如果是异常终止 则保存着导致该进程终止的信号是哪个。这个进程的父进程可以调用wait或waitpid获取这些信息，然后彻底清除掉这个进程。</p><p>我们知道一个进程的退出状态可以在Shell中用特殊变量$?查看，因为Shell是它的父进程，当它终止时Shell调用wait或waitpid得到它的退出状态同时彻底清除掉这个进程。如果一个进程已经终止，但是它的父进程尚未调用wait或waitpid对它进行清理，这时的进程状态称为僵尸(Zombie)进程。任何进程在刚终止时都是僵尸进程，正常情况下，僵尸进程都立刻被父进程清理了。</p><ul><li>僵尸进程： 子进程退出，父进程没有回收子进程资源(PCB)，则子进程变成僵尸进程</li><li>孤儿进程： 父进程先于子进程结束，则子进程成为孤儿进程</li><li>领养孤儿进程： 成为孤儿的子进程，其父进程变成成为1号init进程</li></ul><h5 id="函数原型"><a href="#函数原型" class="headerlink" title="函数原型"></a>函数原型</h5><pre><code>#include &lt;sys/types.h&gt; #include &lt;sys/wait.h&gt;pid_t wait(int *status);pid_t waitpid(pid_t pid, int *status, int options);对于 waitpid 的 pid 而言：&lt; -1 回收指定进程组内的任意子进程-1 回收任意子进程0 回收和当前调用waitpid一个组的所有子进程 &gt; 0 回收指定ID的子进程</code></pre><p>若调用成功则返回清理掉的子进程id，若调用出错则返回-1。父进程调用 wait或waitpid 时可能出现情况:</p><ul><li>阻塞(如果它的所有子进程都还在运行)。</li><li>带子进程的终止信息立即返回(如果一个子进程已终止，正等待父进程读取其终止信息)。</li><li>出错立即返回(如果它没有任何子进程)</li></ul><p>这两个函数的区别是:</p><ul><li>如果父进程的所有子进程都还在运行，调用wait将使父进程阻塞，而调用waitpid时如果在options参数中指定WNOHANG可以使父进程不阻塞而立即返回0。</li><li>wait等待第一个终止的子进程，而waitpid可以通过pid参数指定等待哪一个子进程。</li></ul><p>可见，调用 wait和waitpid 不仅可以获得子进程的终止信息，还可以使父进程阻塞等待子进程终止，起到进程间同步的作用。如果参数 status 不是空指针，则子进程的终止信息通过这个参数传出，如果只是为了同步而不关心子进程的终止信息，可以将 status 参数指定为 NULL。</p><p>对于子进程退出状态的监测，使用宏函数进行监测，具体参考 Man Page。</p><h5 id="示例代码-2"><a href="#示例代码-2" class="headerlink" title="示例代码"></a>示例代码</h5><p>wait阻塞推出监测</p><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;wait.h&gt;int main(){    pid_t id = fork();    int status;    pid_t ret;    if(id == 0)    {        // 子进程中返回，处理子进程逻辑        printf(&quot;Child is running for 5s! The pid is %d\n&quot;,getpid());        // 延时5s，执行退出        sleep(5);        exit(1);    }    else if(id &gt; 0)    {        // 父进程中返回        printf(&quot;This is parent, the parent id is %d\n&quot;,getppid());    }    while(1)    {        // 阻塞函数，将子进程返回状态存到status中        ret = wait(&amp;status);        if(ret == id)            printf(&quot;Child exits, pid id %d\n&quot;,ret);        else if(ret == -1)        {            printf(&quot;No child is exist\n&quot;);            break;        }        else            break;    }    // 退出状态监测status    if (WIFEXITED(status))        printf(&quot;Child exited with code %d\n&quot;, WEXITSTATUS(status));    else if (WIFSIGNALED(status))        printf(&quot;Child terminated abnormally, signal %d\n&quot;, WTERMSIG(status));    return 0;}</code></pre><p>waitpid指定阻塞监听</p><pre><code>#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main(void){    pid_t pid;    pid = fork();    if (pid &lt; 0) {        perror(&quot;fork failed&quot;);        exit(1);    }    if (pid == 0) {        int i;        for (i = 3; i &gt; 0; i--) {            printf(&quot;This is the child\n&quot;);            sleep(1);        }        exit(3);    } else {        int stat_val;        // 回收指定id进程资源        waitpid(pid, &amp;stat_val, 0);            if (WIFEXITED(stat_val))                printf(&quot;Child exited with code %d\n&quot;, WEXITSTATUS(stat_val));            else if (WIFSIGNALED(stat_val))                printf(&quot;Child terminated abnormally, signal %d\n&quot;, WTERMSIG(stat_val));    }    return 0;}</code></pre><p>#### 进程相关函数</p><p>获取进程id</p><pre><code>#include &lt;sys/types.h&gt; #include &lt;unistd.h&gt;pid_t getpid(void); //返回调用进程的PID号pid_t getppid(void); //返回调用进程父进程的PID号</code></pre><p>获取用户id</p><pre><code>#include &lt;unistd.h&gt; #include &lt;sys/types.h&gt;uid_t getuid(void); //返回实际用户ID uid_t geteuid(void); //返回有效用户ID</code></pre><p>获取组id</p><pre><code>#include &lt;unistd.h&gt; #include &lt;sys/types.h&gt;gid_t getgid(void); //返回实际用户组ID gid_t getegid(void); //返回有效用户组ID</code></pre><h5 id="特殊权限位"><a href="#特殊权限位" class="headerlink" title="特殊权限位"></a>特殊权限位</h5><p>s: 这里的特殊权限位s代表的是动态权限设置位（setuid），方便普通用户可以以root用户的角色运行只有root帐号才能运行的程序或命令。</p><pre><code>whereis passwd/usr/bin/passwdls -la /usr/bin/passwd-rwsr-xr-x  1   root    root    ...</code></pre><p>在设置s权限时文件属主、属组必须先设置相应的x权限，否则s权限并不能正真生效（当我们ls -l时看到rwS，大写S说明s权限未生效）</p><pre><code>执行    sudo chmod * 04755  原权限  -rwxr-xr-x 新权限  -rwsr-xr-x </code></pre><p>t: 设置粘着位，一个文件可读写的用户并一定相让他有删除此文件的权限，如果文件设置了t权限则只用属主和root有删除文件的权限，通过chmod +t filename 来设置t权限。</p><p>i: 不可修改权限。例：chattr u+i filename 则filename文件就不可修改，无论任何人，如果需要修改需要先删除i权限，用chattr -i filename就可以了。查看文件是否设置了i权限用lsattr filename。</p><p>a: 只追加权限，对于日志系统很好用，这个权限让目标文件只能追加，不能删除，而且不能通过编辑器追加。可以使用chattr +a设置追加权限。</p><p><a href="https://blog.csdn.net/LYJwonderful/article/details/80220452" target="_blank" rel="noopener">特殊权限详见</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇博文主要介绍了Linux中涉及进程的三大函数fork、exec以及wait，知晓了一般进程的创建，执行以及回收销毁登，为后续进程间通信提供基础。对于Linux中的进程限制，可以通过手动修改提高程序的并发行，但是此方法也不是万能的，后续还会介绍Linux下的多线程编程基础，涉及到多线程，又不得不提线程同步，东西那么多还是要一点一点的啃，此章节就此告一段落吧！</p><blockquote><p>邢文鹏老师Linux教学资料</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;学习Linux中的进程，主要设计了三个重要的函数：fork（创建子进程），exec（执行命令），wait（回收进程资源）。进程的管理由内核中的task_struct结构体(PCB进程控制块)负责，下面我们来具体了解一下。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="进程" scheme="https://891904833.gitee.io/FuckCode/tags/%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之文件IO进阶</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/04/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E6%96%87%E4%BB%B6IO%E8%BF%9B%E9%98%B6/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/04/Linux系统编程之文件IO进阶/</id>
    <published>2019-01-04T02:00:42.000Z</published>
    <updated>2019-01-04T06:51:37.462Z</updated>
    
    <content type="html"><![CDATA[<p><strong>文章概括描述文段</strong><br>上篇文章中介绍了linux中基本的io处理操作，对于一般的文件读取是没有问题的，但是考虑到终端以及网络等可能存在阻塞情况下，这时候就需要我们使用标志位 O_NONBLOCK 。其次，对于文件的阻塞和非阻塞操作，如果是一个已经打开的文件呢又该如何操作，fcntl 便应运而生了。<br><a id="more"></a></p><h3 id="阻塞与非阻塞"><a href="#阻塞与非阻塞" class="headerlink" title="阻塞与非阻塞"></a>阻塞与非阻塞</h3><p>读常规文件是不会阻塞的，不管读多少字节，read一定会在有限的时间内返回。从终端设备或网络读则不一定，如果从终端输入的数据没有换行符，调用read读终端设备就会阻塞，如果网络上没有接收到数据包，调用read从网络读就会阻塞，至于会阻塞多长时间也是不确定的，如果一直没有数据到达就一直阻塞在那里。同样，写常规文件是不会阻塞的，而向终端设备或网络写则不一定。<br>现在明确一下阻塞(Block)这个概念。当进程调用一个阻塞的系统函数时，该进程被置于睡眠(Sleep)状态，这时内核调度其它进程运行，直到该进程等待的事件发生了(比如网络上接收到数据包，或者调用sleep指定的睡眠时间到了)它才有可能继续运行。与睡眠状态相对的是运行(Running)状态，在Linux内核中，处于运行状态的进程分为两种情况:</p><ol><li>正在被调度执行。CPU处于该进程的上下文环境中，程序计数器(eip)里保存着该进程的指令地址，通用寄存器里保存着该进程运算过程的中间结果，正在执行该进程的指令，正在读写该进程的地址空间。</li><li>就绪状态。该进程不需要等待什么事件发生，随时都可以执行，但CPU暂时还在执行另一个进程，所以该进程在一个就绪队列中等待被内核调度。系统中可能同时有多个就绪的进程，那么该调度谁执行呢?内核的调度算法是基于优先级和时间片的，而且会根据每个进程的运行情况动态调整它的优先级和时间片，让每个进程都能比较公平地得到机会执行，同时要兼顾用户体验，不能让和用户交互的进程响应太慢。</li></ol><h4 id="阻塞读终端"><a href="#阻塞读终端" class="headerlink" title="阻塞读终端"></a>阻塞读终端</h4><p>对于终端设备来说，如果终端没有输入，那么read将一直阻塞，知道用户输入enter后才会返回结果。</p><pre><code>#include &lt;unistd.h&gt; #include &lt;stdlib.h&gt;int main(void) {    char buf[10];    int n;    // 阻塞读终端数据    n = read(STDIN_FILENO, buf, 10);     if (n &lt; 0) {        perror(&quot;read STDIN_FILENO&quot;);        exit(1);     }    // 读成功，写到输出终端    write(STDOUT_FILENO, buf, n);    return 0; }</code></pre><h4 id="非阻塞读终端"><a href="#非阻塞读终端" class="headerlink" title="非阻塞读终端"></a>非阻塞读终端</h4><p>如果在open一个设备时指定了O_NONBLOCK标志，read/write就不会阻塞。以read为例，如果设备暂时没有数据可读就返回-1，同时置errno为EWOULDBLOCK(或者EAGAIN，这两个 宏定义的值相同)，表示本来应该阻塞在这里(would block，虚拟语气)，事实上并有阻塞而是直接返回错误，调用者应该试着再读一次(again)。这种行为方式称为轮询 (Poll)，调用者只是查询一下，而不是阻塞在这里死等，这样可以同时监视多个设备。</p><pre><code>#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;fcntl.h&gt;#include &lt;stdlib.h&gt;#define DEV_TTY  &quot;/dev/tty&quot;#define MSG_TRY  &quot;try again \n&quot;int main(void){    char buf[10];    int fd,n;    // 以非阻塞方式打开文件设备    fd = open(DEV_TTY,O_RDONLY | O_NONBLOCK);    if(fd&lt;0)    {        perror(&quot;open device error!&quot;);        exit(1);    }// 轮训标志try_again:    // 非阻塞读数据，立即返回    n = read(fd,buf,sizeof(buf));    if(n &lt; 0){        // 未读取到任何数据，重新读取        if(errno == EAGAIN)        {            // 输出重读标志            write(STDOUT_FILENO,MSG_TRY,strlen(MSG_TRY));            // 睡眠一会            sleep(3);            goto try_again;        }        else        {            perror(&quot;read device error!&quot;);            exit(1);        }    }    // 写入输出终端    write(STDOUT_FILENO,buf,n);    close(fd);    return 0;}</code></pre><p>用非阻塞I/O实现等待超时的例子。既保证了超时退出的逻辑又保证了有数据到<br>达时处理延迟较小。</p><h4 id="非阻塞升级，等待超时"><a href="#非阻塞升级，等待超时" class="headerlink" title="非阻塞升级，等待超时"></a>非阻塞升级，等待超时</h4><p>对于非阻塞读取终端数据，逻辑上是采用轮训的方式，这样处理起来有一点不好就是要一直的sleep睡眠等待。在一定程度上造成性能的浪费，。当然排除select，poll以及epoll等高效的异步IO转接方式，另一种可以替代的方式就是规定在一定的时间范围呢进行轮训，如果超时了就直接推出，给出有效提示。</p><pre><code>#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;fcntl.h&gt;#include &lt;stdio.h&gt;#define DEV_TTY  &quot;/dev/tty&quot;#define MSG_TRY  &quot;try again \n&quot;#define MSG_TIMEOUT &quot;read device time out!\n&quot;int main(void){    char buf[10];    int fd,n;    fd = open(DEV_TTY,O_RDONLY | O_NONBLOCK);    if(fd&lt;0)    {        perror(&quot;open device error!&quot;);        exit(1);    }    // ls * 10 的超时等待    for (int i = 0; i &lt; 10; ++i)    {        /* code */        n = read(fd,buf,sizeof(buf));        if(n &lt; 0){            if(errno == EAGAIN)            {                // 未读取有效数据，睡眠等待轮训                write(STDOUT_FILENO,MSG_TRY,strlen(MSG_TRY));                sleep(1);            }            else            {                /* code */                perror(&quot;read device error!&quot;);                exit(1);            }        }        else        {            // 读取到有效数据，直接打印输出            write(STDOUT_FILENO,buf,n);            close(fd);            return 0;        }    }    // 读取超时，打印输出超时信息    write(STDOUT_FILENO,MSG_TIMEOUT,strlen(MSG_TIMEOUT));    close(fd);    return 0;}</code></pre><h3 id="使用-fcntl-修改文件状态位"><a href="#使用-fcntl-修改文件状态位" class="headerlink" title="使用 fcntl 修改文件状态位"></a>使用 fcntl 修改文件状态位</h3><p>先前我们以read终端设备为例介绍了非阻塞I/O，为什么我们不直接对STDIN_FILENO做非阻塞read，而要重新open一遍/dev/tty呢?因为STDIN_FILENO在程序启动时已经被自动打开了，而我们需要在调用open时指定O_NONBLOCK标志。这里介绍另外一种办法，可以用 fcntl 函数改变一个已打开的文件的属性，可以重新设置读、写、追加、非阻塞等标志(这些标志称为File Status Flag)，而不必重新open文件。</p><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><pre><code>#include &lt;fcntl.h&gt;int fcntl(int fildes, int cmd, ...);</code></pre><h4 id="参数描述"><a href="#参数描述" class="headerlink" title="参数描述"></a>参数描述</h4><p>filsed: 唯一文件描述符<br>cmd: 操作命令</p><p>根据cmd命令，我们可以选择对一打开文件描述符进行属性操作，例如通过命令 F_GETFD 来获取当前文件描述符的属性，通过 F_SETFD 来设置已经打开的文件描述符的属性，从而避免从新打开时添加对应的属性。</p><table><thead><tr><th>cmd</th><th>描述</th></tr></thead><tbody><tr><td>F_DUPFD</td><td>复制文件描述词</td></tr><tr><td>F_GETFD</td><td>读取文件描述词标志</td></tr><tr><td>F_SETFD</td><td>设置文件描述词标志</td></tr><tr><td>F_GETFL</td><td>读取文件状态标志</td></tr><tr><td>F_SETFL</td><td>设置文件状态标志</td></tr><tr><td>…</td><td>…</td></tr></tbody></table><p>更多设置参考 Man Page</p><p>在 F_SETFL 命令中，其中O_RDONLY， O_WRONLY， O_RDWR， O_CREAT，  O_EXCL， O_NOCTTY 和 O_TRUNC不受影响，可以更改的标志有 O_APPEND，O_ASYNC， O_DIRECT， O_NOATIME 和 O_NONBLOCK。 </p><h4 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h4><p>返回值根据命令的不同有不同的返回值，例如 F_DUPFD 会返回一个新的文件描述符，F_GETFL 会返回文件的属性位等，如果出错会反悔 -1， 并且置位 errno，具体参考 Man Page。</p><h4 id="实力代码"><a href="#实力代码" class="headerlink" title="实力代码"></a>实力代码</h4><p>本片主要讲述文件属性值的设置，下面代码展示了如何通过 fctnl 函数对已经打开的文件进行非阻塞读取操作。</p><pre><code>#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;fcntl.h&gt;#include &lt;stdio.h&gt;#define DEV_TTY  &quot;/dev/tty&quot;#define MSG_TRY  &quot;try again \n&quot;#define MSG_TIMEOUT &quot;read device time out!\n&quot;int main(void){    char buf[10];    int fd,n;    fd = open(DEV_TTY,O_RDONLY);    // 通过fcntl的命令 F_GETFL 获取打开文件的访问属性    int flags = fcntl(fd,F_GETFL);    printf(&quot;flags0 is %d\n&quot;,flags);    // 属性添加 O_NONBLOCK    flags |= O_NONBLOCK;    printf(&quot;flags1 is %d\n&quot;,flags);    // 通过命令 F_SETFL 将新的文件属性设置到文件中去    fcntl(fd,F_SETFL, flags);    if(fd&lt;0)    {        perror(&quot;open device error!&quot;);        exit(1);    }    for (int i = 0; i &lt; 5; ++i){        /* code */        n = read(fd,buf,sizeof(buf));        if(n &lt; 0){            if(errno == EAGAIN){                write(STDOUT_FILENO,MSG_TRY,strlen(MSG_TRY));                sleep(1);            } else {                /* code */                perror(&quot;read device error!&quot;);                exit(1);            }        } else {            write(STDOUT_FILENO,buf,n);            close(fd);            return 0;        }    }    write(STDOUT_FILENO,MSG_TIMEOUT,strlen(MSG_TIMEOUT));    close(fd);    return 0;}</code></pre><p>当然 fctnl 的功能还有很多，比如文件锁（读共享，写私有）等，后续篇幅会在锁机制中统一介绍，这里单纯的介绍一下设置文件非阻塞访问属性。</p><h3 id="lseek修改文件访问指针"><a href="#lseek修改文件访问指针" class="headerlink" title="lseek修改文件访问指针"></a>lseek修改文件访问指针</h3><p>lseek函数的作用是用来重新定位文件读写的位移。</p><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><pre><code>#include &lt;unistd.h&gt;off_t   lseek(int fildes, off_t offset, int whence);</code></pre><h4 id="参数解释"><a href="#参数解释" class="headerlink" title="参数解释"></a>参数解释</h4><p>fildes： 文件描述符<br>offset： 为正则向文件末尾移动（向前移），为负数则向文件头部（向后移）<br>whence： 文件便偏移参考值</p><pre><code>SEEK_SET:　　从文件头部开始偏移offset个字节SEEK_CUR：　　从文件当前读写的指针位置开始，增加offset个字节的偏移量SEEK_END：　　文件偏移量设置为文件的大小加上偏移量字节</code></pre><h4 id="返回值-1"><a href="#返回值-1" class="headerlink" title="返回值"></a>返回值</h4><p>如果设备不支持lseek，则lseek返回-1，并将errno设置为ESPIPE。<br>注意 fseek 和 lseek 在返回值上有细微的差别， fseek 成功时返回 0 ，失败时返回 -1 ，要返回当前偏移量需调用 ftell ，而 lseek 成功时返回当前偏移量失败时返回 -1 。</p><h4 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h4><pre><code>// 获取文件大小int lseek(fd,0,SEEK_END)// 扩充文件大小int add_len = 1024*8;int fd=open(&quot;test.txt&quot;,O_RDWR);if(fd == -1){    perror(&quot;open test.txt&quot;);    exit(-1);}lseek(fd,add_len-1,SEEK_END);</code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>经过前面博文中对 open，read，write，close 等函数的介绍，基本能够完成一般文件的 IO 操作，此篇幅有详细介绍了阻塞与非阻塞读取数据，文件属性修改函数 fcntl 的功能，其能够实现对打开文件的属性操作，函数功能非常强大。同时，lseek函数能够获取一打开文件的大小，还可以扩充文件的大小（lseek后必须进行一个write操作）。博文参考刑文鹏老师的Linux系统编程资料，特此记录以备不时之需。</p><blockquote><p>参考学习资料邢文鹏老师</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;文章概括描述文段&lt;/strong&gt;&lt;br&gt;上篇文章中介绍了linux中基本的io处理操作，对于一般的文件读取是没有问题的，但是考虑到终端以及网络等可能存在阻塞情况下，这时候就需要我们使用标志位 O_NONBLOCK 。其次，对于文件的阻塞和非阻塞操作，如果是一个已经打开的文件呢又该如何操作，fcntl 便应运而生了。&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="阻塞 非阻塞" scheme="https://891904833.gitee.io/FuckCode/tags/%E9%98%BB%E5%A1%9E-%E9%9D%9E%E9%98%BB%E5%A1%9E/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统编程之文件IO</title>
    <link href="https://891904833.gitee.io/FuckCode/2019/01/03/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B%E4%B9%8B%E6%96%87%E4%BB%B6IO/"/>
    <id>https://891904833.gitee.io/FuckCode/2019/01/03/Linux系统编程之文件IO/</id>
    <published>2019-01-03T04:04:14.000Z</published>
    <updated>2019-01-03T08:47:11.213Z</updated>
    
    <content type="html"><![CDATA[<p><strong>在Linux系统中，一切皆是文件。文件相当于操作系统和设备之间的一架桥梁。程序可以像使用文件那样访问磁盘文件，串行口，打印机等其他设备。通常情况下，我们可以通过以下5个函数，即可操作众多设备：open、close、read、write和ioctl。例外的情况： 目录的读写，网络连接等特殊文件</strong></p><a id="more"></a><h3 id="文件IO介绍"><a href="#文件IO介绍" class="headerlink" title="文件IO介绍"></a>文件IO介绍</h3><p>文件和设备</p><pre><code>/dev/console - 系统控制台。  /dev/tty - 访问不同的物理设备。   /dev/null - 空设备，向所有写这个设备的输出都将被丢弃。</code></pre><p>设备驱动程序</p><p>操作系统的核心部分，即内核，是由一组设备驱动程序组成。他们是一组对系统硬件进行控制的底层接口，为了向用户提供一个一致的接口，其封装了所有与硬件相关的特性。<br>硬件特有功能可通过ioctl(用于I/O控制)系统调用来提供。</p><pre><code>/dev 目录下的设备文件都可以被打开、读、写和关闭。1）open : 打开文件或设备。2）read : 从打开的文件或设备里读数据。3）write: 向文件或设备写数据。4）close: 关闭文件或设备。5） ioctl:  把控制信息传递给设备驱动程序,每个驱动都由自己的一组 ioctl 命令。</code></pre><p>库函数</p><p>针对输入输出操作直接使用底层系统调用效率非常低，原因由如下两点。</p><ol><li>使用系统调用会影响系统性能。  </li><li>硬件会对底层系统调用一次所读写的数据块大小做限制。磁盘：至少一个扇区512字节，磁带，一次 10K</li></ol><p>库函数给设备和磁盘文件提供了更高层的接口，即标准函数库。使用它你可以高效读写任意长度的数据块,库函数则在数据满足条件后再安排系统调用。这样极大降低了开销。<br>注：库函数的文档一般放在手册的第三页，每个库函数有其对应的头文件。</p><p>底层文件访问</p><p>运行中的程序称为进程，每个进程都有与之关联的文件描述符。</p><p>文件描述符 － 一些小值整数，通过他们访问打开的文件或设备。一旦一个进程启动，内核就会分配三个文件描述符FILE：</p><pre><code>0:   标准输入   STDIN_FILENO1:   标准输出   STDOUT_FILENO2:   标准错误   STDERR_FILENO</code></pre><p>文件描述符的变化范围是：0~OPEN_MAX-1，默认为1024(可通过ulmit -a 查看)</p><p>查看支持打开文件个数  cat /proc/sys/fs/file_max</p><h3 id="系统函数介绍"><a href="#系统函数介绍" class="headerlink" title="系统函数介绍"></a>系统函数介绍</h3><h4 id="open"><a href="#open" class="headerlink" title="open"></a>open</h4><p>作用：打开或者创建一个文件描述符（文件或设备）</p><h5 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h5><pre><code>#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;int open(const char *pathname, int flags);int open(const char *pathname, int flags, mode_t mode);</code></pre><h5 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h5><p>通过给定一个文件名或者路径，成功调用后，进程中返回系统能够分配最小的文件描述符。此文件描述符进程内唯一，两个不同进程访问同一个文件所分配的文件描述符不一定相同。</p><h5 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h5><pre><code>pathname - 指示准备打开的文件或设备的名字;flags    - 用于指定打开文件所采取的动作;mode    - 用于指定创建文件的权限，指定动作为 O_CREATE 才使用。</code></pre><p>必须制定的flag操作：</p><table><thead><tr><th>模式</th><th>说明</th></tr></thead><tbody><tr><td>O_RDONLY</td><td>以只读方式打开</td></tr><tr><td>O_WRONLY</td><td>以只写方式打开</td></tr><tr><td>O_RDWR</td><td>以读写方式打开</td></tr></tbody></table><p>可选的flag操作</p><ul><li>O_APPEND 表示追加。如果文件已有内容,这次打开文件所写的数据附加到文件的末尾<br>而不覆盖原来的内容。</li><li>O_CREAT 若此文件不存在则创建它。使用此选项时需要提供第三个参数mode,表示该<br>文件的访问权限。</li><li>O_EXCL 如果同时指定了O_CREAT,并且文件已存在,则出错返回。</li><li>O_TRUNC 如果文件已存在,并且以只写或可读可写方式打开,则将其长度截断(Trun-<br>cate)为0字节。</li><li>O_NONBLOCK 对于设备文件,以O_NONBLOCK方式打开可以做非阻塞I/O(Nonblock I/<br>O)，后续博客详细了解阻塞和非阻塞代码编程</li></ul><p>权限位 mode</p><p>可以用八进制数表示,比如0644表示-rw-r-r–,也可以用S_IRUSR、S_IWUSR等宏定义按位或起来表示,详见open(2)的Man Page。<br>注意文件权限由open的mode参数和当前进程的umask掩码共同决定。</p><pre><code>// 查看shell当前umask值allies@allies:~$ umask0002</code></pre><p>用touch命令创建一个文件时,创建权限是0666,而touch进程继承了Shell进程的umask<br>掩码,所以最终的文件权限是0666&amp;∼0002=0664</p><h4 id="read"><a href="#read" class="headerlink" title="read"></a>read</h4><h5 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h5><pre><code>#include &lt;unistd.h&gt;ssize_t read(int fd, void *buf, size_t count);</code></pre><h5 id="描述-1"><a href="#描述-1" class="headerlink" title="描述"></a>描述</h5><p>从文件描述符 fd 中读取 count 字节的数据并放入从 buf 开始的缓冲区中</p><h5 id="参数说明-1"><a href="#参数说明-1" class="headerlink" title="参数说明"></a>参数说明</h5><pre><code>fd： 文件唯一描述符buf： 缓冲区数组起始地址count： 读取文件大小</code></pre><p>返回值： 成功返回读取到的字节大小，失败返回-1，并置位 errno。</p><pre><code>EINTR  在读取到数据以前调用被信号所中断.EAGAIN 使用 O_NONBLOCK 标志指定了非阻塞式输入输出,但当前没有数据可读.EFAULT buf 超出用户可访问的地址空间.具体查看 Man Page。</code></pre><h4 id="write"><a href="#write" class="headerlink" title="write"></a>write</h4><p>在一个文件描述符上执行写操作</p><h5 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h5><pre><code>#include &lt;unistd.h&gt;ssize_t write(int fd, const void *buf, size_t count);</code></pre><h5 id="描述-2"><a href="#描述-2" class="headerlink" title="描述"></a>描述</h5><p>write 向文件描述符 fd 所引用的文件中写入从 buf 开始的缓冲区中 count 字节的数据。POSIX规定，当使用了write()之后再使用 read()，那么读取到的应该是更新后的数据。</p><h5 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h5><p>write 向文件描述符 fd 所引用的文件中写入 从 buf 开始的缓冲区中 count 字节的数据。POSIX规定,当使用了write()之后再使用 read(),那么读取到的应该是更新后的数据.失败返回-1，并置位errno</p><pre><code>EBADF  fd 不是一个合法的文件描述符或者没有以写方式打开EINVAL fd 所指向的对象不可写EFAULT buf 不在用户可访问地址空间内EAGAIN 读操作阻塞,但使用 O_NONBLOCK 指定了非阻塞式输入输出EINTR  在写数据以前调用被信号中断具体查看 Man Page</code></pre><h4 id="close"><a href="#close" class="headerlink" title="close"></a>close</h4><p>关闭一个文件描述符</p><h5 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h5><pre><code>#include &lt;unistd.h&gt;int close(int fd);</code></pre><h5 id="描述-3"><a href="#描述-3" class="headerlink" title="描述"></a>描述</h5><p>close 关闭一个文件描述符,使它不在指向任何文件和可以在新的文件操作中被再次使用。任何与此文件相关联的以及程序所拥有的锁,都会被删除 (忽略那些持有锁的文件描述符)<br>假如 fd 是最后一个文件描述符与此资源相关联,则这个资源将被释放。若此描述符是最后一个引用到此文件上的,则文件将使用 unlink(2) 删除。</p><h5 id="参数说明-2"><a href="#参数说明-2" class="headerlink" title="参数说明"></a>参数说明</h5><p>fd： 需要关闭文件的文件描述符</p><h5 id="返回值-1"><a href="#返回值-1" class="headerlink" title="返回值"></a>返回值</h5><p>close 返回 0 表示成功，或者 -1 表示有错误发生，置位 errno</p><pre><code>EBADF  fd 不是一个有效的已被打开的文件的描述符EINTR  函数 close() 调用被一信号中断EIO    I/O 有错误发生</code></pre><h4 id="ioctl"><a href="#ioctl" class="headerlink" title="ioctl"></a>ioctl</h4><p>ioctl用于向设备发控制和配置命令,有些命令也需要读写一些数据,但这些数据是<br>不能用read/write读写的,称为Out-of-band数据。也就是说,read/write读写的数据是<br>in-band数据,是I/O操作的主体,而ioctl命令传送的是控制信息,其中的数据是辅助的数<br>据。例如,在串口线上收发数据通过read/write操作,而串口的波特率、校验位、停止位通<br>过ioctl设置,A/D转换的结果通过read读取,而A/D转换的精度和工作频率通过ioctl设置。</p><h5 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h5><pre><code>#include &lt;sys/ioctl.h&gt;int ioctl(int d, int request, ...);</code></pre><h5 id="描述-4"><a href="#描述-4" class="headerlink" title="描述"></a>描述</h5><p>d是某个设备的文件描述符。request是ioctl的命令,可变参数取决于request,通常是<br>一个指向变量或结构体的指针。若出错则返回-1,若成功则返回其他值,返回值也是取决于<br>request。</p><h5 id="返回值-2"><a href="#返回值-2" class="headerlink" title="返回值"></a>返回值</h5><p>调用成功，返回 0 。出错返回 -1 ，置位 errno</p><pre><code>EBADF  fd 不是一个有效的文件描述符EFAULT 参数引用内存溢出EINVAL 无效request或参数更多参考 Man Page</code></pre><p>ioctl提供了一个用于控制设备及其描述行为和配置底层的服务的接口。终端文件描述符、套接字都可以定义他们的ioctl，具体需要参考特定设备的手册。</p><h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><pre><code>#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include&lt;string.h&gt;int main(int argc, char* argv[]){    printf(&quot;argc = %d\n&quot;, argc);    printf(&quot;argv = %s\n&quot;,argv[1]);    if(argc&lt;2)    {        printf(&quot;open file need  args!\n&quot;);        exit(1);    }    //置位创建文件权限    umask(0000);    int fd = -1;    // 打开文件，不存在创建，并且指定读写方式    fd = open(argv[1],O_CREAT | O_RDWR, 0777);    char buf[512] = &quot;This is a test file!&quot;;    // 将字符数组写入文件中    int size = write(fd, buf,strlen(buf));    printf(&quot;write size = %d\n&quot;,size);    printf(&quot;fd = %d\n&quot;,fd);    close(fd);    // 再次打开用户输入的文件    fd = open(argv[1],O_RDONLY);    // 重新读取文件中的内容    size = read(fd,buf,sizeof(buf));    printf(&quot;size = %d\n&quot;,size);    printf(&quot;read : %s\n&quot;,buf);    close(fd);    return 0;}</code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>本篇文章为本人第一次接触linux系统编程所学习的，内容设计最直接的文件io操作。文章中的大部分原型代码都可以通过 Linux 自带的 Man Page 进行翻阅。限于篇幅，后续文章会对文件操作的阻塞个非阻塞进行进一步的了解。</p><p><a href="https://www.cnblogs.com/wangkeqin/p/9226825.html" target="_blank" rel="noopener">参考博客</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;在Linux系统中，一切皆是文件。文件相当于操作系统和设备之间的一架桥梁。程序可以像使用文件那样访问磁盘文件，串行口，打印机等其他设备。通常情况下，我们可以通过以下5个函数，即可操作众多设备：open、close、read、write和ioctl。例外的情况： 目录的读写，网络连接等特殊文件&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://891904833.gitee.io/FuckCode/categories/Linux/"/>
    
    
      <category term="文件IO" scheme="https://891904833.gitee.io/FuckCode/tags/%E6%96%87%E4%BB%B6IO/"/>
    
  </entry>
  
  <entry>
    <title>源码分析之存储系统启动流程MountService模块</title>
    <link href="https://891904833.gitee.io/FuckCode/2018/09/28/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8B%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8BMountService%E6%A8%A1%E5%9D%97/"/>
    <id>https://891904833.gitee.io/FuckCode/2018/09/28/源码分析之存储系统启动流程MountService模块/</id>
    <published>2018-09-28T07:42:18.000Z</published>
    <updated>2019-04-10T07:31:13.464Z</updated>
    
    <content type="html"><![CDATA[<p><strong>上篇我们介绍了Android的存储系统中native层的Vold的启动流程，Vold在init进程中通过init脚本进行启动，而MountService则是在Java的服务总站SystemServer中配置启动的，下面我们就来看一下Java层的MountService是如何启动的。</strong></p><a id="more"></a><h1 id="MountService启动流程"><a href="#MountService启动流程" class="headerlink" title="MountService启动流程"></a>MountService启动流程</h1><p>Java层的MountService服务是在SystemServer中配置的，我们来分析一下其启动流程。</p><h2 id="SystemServer启动配置"><a href="#SystemServer启动配置" class="headerlink" title="SystemServer启动配置"></a>SystemServer启动配置</h2><pre><code>private static final String MOUNT_SERVICE_CLASS = &quot;com.android.server.MountService$Lifecycle&quot;;private void startOtherServices() {    ...    if (mFactoryTestMode != FactoryTest.FACTORY_TEST_LOW_LEVEL) {        if (!disableStorage &amp;&amp;            !&quot;0&quot;.equals(SystemProperties.get(&quot;system_init.startmountservice&quot;))) {            try {                /*                 * NotificationManagerService is dependant on MountService,                 * (for media / usb notifications) so we must start MountService first.                 */                //启动MountService服务                mSystemServiceManager.startService(MOUNT_SERVICE_CLASS);                //等价new IMountService.Stub.Proxy()，即获取MountService的proxy对象                mountService = IMountService.Stub.asInterface(                        ServiceManager.getService(&quot;mount&quot;));            } catch (Throwable e) {                reportWtf(&quot;starting Mount Service&quot;, e);            }        }    }    ...}</code></pre><p>这里可以看出，启动MountService并非直接启动，而是通过其静态内部类Lifecycle实现启动，下面我们看一下其内部静态类的实现。</p><h2 id="静态内部类Lifecycle"><a href="#静态内部类Lifecycle" class="headerlink" title="静态内部类Lifecycle"></a>静态内部类Lifecycle</h2><pre><code>// Lifecycle继承SystemService，受制于父类的生命周期方法回调，其为MountService的内部静态类，控制MountService的实例化操作public static class Lifecycle extends SystemService {    private MountService mMountService;    public Lifecycle(Context context) {        super(context);    }    @Override    public void onStart() {        //创建MountService对象【见小节2.3】        mMountService = new MountService(getContext());        //登记Binder服务        publishBinderService(&quot;mount&quot;, mMountService);    }    // 由SystenServer启动到指定阶段进行回调，其内部由mServices持有    @Override    public void onBootPhase(int phase) {        // 由于MountService的内部Lifecycle已添加SystemServiceManager的mServices服务列表；        // 系统启动到PHASE_ACTIVITY_MANAGER_READY时会回调mServices中的onBootPhase方法        if (phase == SystemService.PHASE_ACTIVITY_MANAGER_READY) {            mMountService.systemReady();        }    }    ...}</code></pre><p>静态内部类Lifecycle中实现MountService的初始化操作，然后通过publishBinderService将新建的MountService添加到服务列表中。</p><h2 id="MountService初始化"><a href="#MountService初始化" class="headerlink" title="MountService初始化"></a>MountService初始化</h2><p>MountService在其静态内部类LifeCycle中进行初始化，具体如下：</p><pre><code>public MountService(Context context) {    sSelf = this;    mContext = context;    //FgThread线程名为“&quot;android.fg&quot;，创建IMountServiceListener回调方法（1线程）    mCallbacks = new Callbacks(FgThread.get().getLooper());    // XXX: This will go away soon in favor of IMountServiceObserver    //获取PKMS的Client端对象    mPms = (PackageManagerService) ServiceManager.getService(&quot;package&quot;);    //创建“MountService”线程（2线程）    HandlerThread hthread = new HandlerThread(TAG);    hthread.start();    mHandler = new MountServiceHandler(hthread.getLooper());    // Add OBB Action Handler to MountService thread.    //IoThread线程名为&quot;android.io&quot;，创建OBB操作的handler（3线程）    mObbActionHandler = new ObbActionHandler(IoThread.get().getLooper());    // Initialize the last-fstrim tracking if necessary    File dataDir = Environment.getDataDirectory();    File systemDir = new File(dataDir, &quot;system&quot;);    mLastMaintenanceFile = new File(systemDir, LAST_FSTRIM_FILE);    //判断/data/system/last-fstrim文件，不存在则创建，存在则更新最后修改时间    if (!mLastMaintenanceFile.exists()) {        // Not setting mLastMaintenance here means that we will force an        // fstrim during reboot following the OTA that installs this code.        try {            (new FileOutputStream(mLastMaintenanceFile)).close();        } catch (IOException e) {            Slog.e(TAG, &quot;Unable to create fstrim record &quot; + mLastMaintenanceFile.getPath());        }    } else {        mLastMaintenance = mLastMaintenanceFile.lastModified();    }    mSettingsFile = new AtomicFile(            new File(Environment.getSystemSecureDirectory(), &quot;storage.xml&quot;));    synchronized (mLock) {        readSettingsLocked();    }    //将MountServiceInternalImpl登记到sLocalServiceObjects，区别Binder的RemoteService    LocalServices.addService(MountServiceInternal.class, mMountServiceInternal);    /*     * Create the connection to vold with a maximum queue of twice the     * amount of containers we&apos;d ever expect to have. This keeps an     * &quot;asec list&quot; from blocking a thread repeatedly.     */    //创建用于VoldConnector的NDC(native守护进程连接器)对象    mConnector = new NativeDaemonConnector(this, &quot;vold&quot;, MAX_CONTAINERS * 2, VOLD_TAG, 25,            null);    mConnector.setDebug(true);    //创建线程名为&quot;VoldConnector&quot;的线程，用于跟vold通信,启动（4线程）    Thread thread = new Thread(mConnector, VOLD_TAG);    thread.start();    // Reuse parameters from first connector since they are tested and safe    //创建用于CryptdConnector工作的NDC对象,同上面的一样    mCryptConnector = new NativeDaemonConnector(this, &quot;cryptd&quot;,            MAX_CONTAINERS * 2, CRYPTD_TAG, 25, null);    mCryptConnector.setDebug(true);    //创建线程名为&quot;CryptdConnector&quot;的线程，用于加密（5线程）    Thread crypt_thread = new Thread(mCryptConnector, CRYPTD_TAG);    crypt_thread.start();    //注册监听用户添加、删除的广播    final IntentFilter userFilter = new IntentFilter();    userFilter.addAction(Intent.ACTION_USER_ADDED);    userFilter.addAction(Intent.ACTION_USER_REMOVED);    mContext.registerReceiver(mUserReceiver, userFilter, null, mHandler);    //内部私有volume的路径为/data，该volume通过dumpsys mount是不会显示的    addInternalVolume();    // Add ourself to the Watchdog monitors if enabled.    if (WATCHDOG_ENABLE) {        Watchdog.getInstance().addMonitor(this);    }}</code></pre><p>代码内容比较多，我们梳理一下其主要工作：</p><ul><li>创建线程 Callbacks，线程名android.fg</li><li>创建线程 HandlerThread，线程名MountService，并开启</li><li>创建线程 ObbActionHandler，线程名android.io</li><li>创建 NativeDaemonConnector对象，线程 thread并开启，线程名VoldConnector</li><li>创建 NativeDaemonConnector对象，线程 crypt_thread并开启，线程名CryptdConnector</li><li>注册广播和其他操作</li></ul><p>这里设计线程数量比较多，新建了三个线程：MountService，VoldConnector，CryptdConnector；还有两个额外的系统进程中的线程android.fg和android.io</p><h3 id="Callbacke"><a href="#Callbacke" class="headerlink" title="Callbacke"></a>Callbacke</h3><pre><code>private static class Callbacks extends Handler {    private static final int MSG_STORAGE_STATE_CHANGED = 1;    private static final int MSG_VOLUME_STATE_CHANGED = 2;    private static final int MSG_VOLUME_RECORD_CHANGED = 3;    private static final int MSG_VOLUME_FORGOTTEN = 4;    private static final int MSG_DISK_SCANNED = 5;    private static final int MSG_DISK_DESTROYED = 6;    //通过 register() 方法添加 IMountServiceListener 对象信息到 mCallbacks 成员变量。    // RemoteCallbackList 的内部类 Callback 继承于 IBinder.DeathRecipient，这是死亡通知    //当 binder 服务端进程死亡后，回调 binderDied 方法通知 binder 客户端进行相应地处理    private final RemoteCallbackList&lt;IMountServiceListener&gt;            mCallbacks = new RemoteCallbackList&lt;&gt;();    public Callbacks(Looper looper) {        super(looper);    }    public void register(IMountServiceListener callback) {        mCallbacks.register(callback);    }    public void unregister(IMountServiceListener callback) {        mCallbacks.unregister(callback);    }    @Override    public void handleMessage(Message msg) {        final SomeArgs args = (SomeArgs) msg.obj;        final int n = mCallbacks.beginBroadcast();        for (int i = 0; i &lt; n; i++) {            final IMountServiceListener callback = mCallbacks.getBroadcastItem(i);            try {                invokeCallback(callback, msg.what, args);            } catch (RemoteException ignored) {            }        }        mCallbacks.finishBroadcast();        args.recycle();    }    private void invokeCallback(IMountServiceListener callback, int what, SomeArgs args)            throws RemoteException {        switch (what) {            case MSG_STORAGE_STATE_CHANGED: {                callback.onStorageStateChanged((String) args.arg1, (String) args.arg2,                        (String) args.arg3);                break;            }            ...        }    }    private void notifyStorageStateChanged(String path, String oldState, String newState) {        final SomeArgs args = SomeArgs.obtain();        args.arg1 = path;        args.arg2 = oldState;        args.arg3 = newState;        obtainMessage(MSG_STORAGE_STATE_CHANGED, args).sendToTarget();    }    ...}</code></pre><p>Callback继承自handler，其中Looper采用的是系统进程线程android.fg，其内部还有一个成员变量mCallbacks，通过register()方法添加IMountServiceListener对象信息到mCallbacks成员变量。RemoteCallbackList的内部类Callback继承于IBinder.DeathRecipient，很显然这是死亡通知，当binder服务端进程死亡后，回调binderDied方法通知binder客户端进行相应地处理。</p><h3 id="MountServiceHandler"><a href="#MountServiceHandler" class="headerlink" title="MountServiceHandler"></a>MountServiceHandler</h3><pre><code>class MountServiceHandler extends Handler {    public MountServiceHandler(Looper looper) {        super(looper);    }    @Override    public void handleMessage(Message msg) {        switch (msg.what) {            case H_SYSTEM_READY: {                // 处理system_server主线程发送 H_SYSTEM_READY 消息                handleSystemReady();                break;            }            case H_DAEMON_CONNECTED: {                // Daemon-Socket 已经建立连接                handleDaemonConnected();                break;            }            ...            case H_VOLUME_MOUNT: {                final VolumeInfo vol = (VolumeInfo) msg.obj;                if (isMountDisallowed(vol)) {                    Slog.i(TAG, &quot;Ignoring mount &quot; + vol.getId() + &quot; due to policy&quot;);                    break;                }                try {                    mConnector.execute(&quot;volume&quot;, &quot;mount&quot;, vol.id, vol.mountFlags,                            vol.mountUserId);                } catch (NativeDaemonConnectorException ignored) {                }                break;            }            ...        }    }}</code></pre><p>Handler主线程，负责处理所有的UI事件。</p><h2 id="NativeDaemonConnector初始化"><a href="#NativeDaemonConnector初始化" class="headerlink" title="NativeDaemonConnector初始化"></a>NativeDaemonConnector初始化</h2><pre><code>NativeDaemonConnector(INativeDaemonConnectorCallbacks callbacks, String socket,        int responseQueueSize, String logTag, int maxLogSize, PowerManager.WakeLock wl) {    this(callbacks, socket, responseQueueSize, logTag, maxLogSize, wl,            FgThread.get().getLooper());}// mLooper 为 FgThread.get().getLooper()，即运行在 ”android.fg” 线程NativeDaemonConnector(INativeDaemonConnectorCallbacks callbacks, String socket,        int responseQueueSize, String logTag, int maxLogSize, PowerManager.WakeLock wl,        Looper looper) {    mCallbacks = callbacks;    //socket名为&quot;vold&quot;    mSocket = socket;    // mResponseQueue 对象中成员变量 mPendingCmds 数据类型为 LinkedList    //记录着 vold 进程上报的响应事件，事件个数上限为500    mResponseQueue = new ResponseQueue(responseQueueSize);    mWakeLock = wl;    if (mWakeLock != null) {        mWakeLock.setReferenceCounted(true);    }    mLooper = looper;    mSequenceNumber = new AtomicInteger(0);    // TAG 为&quot;VoldConnector&quot;    TAG = logTag != null ? logTag : &quot;NativeDaemonConnector&quot;;    mLocalLog = new LocalLog(maxLogSize);}</code></pre><p>NativeDaemonConnector使用的线程为android.fg，其主要负责和Native层Vold进程通过Socket进行痛惜，其中mResponseQueue对象中成员变量mPendingCmds数据类型为LinkedList，记录着vold进程上报的响应事件，事件个数上限为500。</p><p>在线程VoldConnector中建立了名为vold的socket的客户端，通过循环方式不断监听Vold服务端发送过来的消息。 另外，同理还有一个线程CryptdConnector也采用类似的方式，建立了cryptd的socket客户端，监听Vold中另个线程发送过来的消息。</p><h3 id="NDC-gt-run"><a href="#NDC-gt-run" class="headerlink" title="NDC-&gt;run"></a>NDC-&gt;run</h3><p>接下来就是NDC的start进入run方法，如下；</p><pre><code>@Overridepublic void run() {    mCallbackHandler = new Handler(mLooper, this);    while (true) {        try {            //监听vold的socket            listenToSocket();        } catch (Exception e) {            loge(&quot;Error in NativeDaemonConnector: &quot; + e);            SystemClock.sleep(5000);        }    }}</code></pre><h3 id="listenToSocket"><a href="#listenToSocket" class="headerlink" title="listenToSocket"></a>listenToSocket</h3><pre><code>// 监听 socket 内数据private void listenToSocket() throws IOException {    LocalSocket socket = null;    try {        socket = new LocalSocket();        LocalSocketAddress address = determineSocketAddress();        // 建立与&quot;/dev/socket/vold&quot;的socket连接        socket.connect(address);        InputStream inputStream = socket.getInputStream();        synchronized (mDaemonLock) {            mOutputStream = socket.getOutputStream();        }        // 建立连接后，回调 MS.onDaemonConnected        mCallbacks.onDaemonConnected();        byte[] buffer = new byte[BUFFER_SIZE];        int start = 0;        while (true) {            int count = inputStream.read(buffer, start, BUFFER_SIZE - start);            if (count &lt; 0) {                loge(&quot;got &quot; + count + &quot; reading with start = &quot; + start);                break;            }            // Add our starting point to the count and reset the start.            count += start;            start = 0;            for (int i = 0; i &lt; count; i++) {                if (buffer[i] == 0) {                    // Note - do not log this raw message since it may contain                    // sensitive data                    final String rawEvent = new String(                            buffer, start, i - start, StandardCharsets.UTF_8);                    boolean releaseWl = false;                    try {                        //解析 socket 服务端发送的 event                        final NativeDaemonEvent event = NativeDaemonEvent.parseRawEvent(                                rawEvent);                        log(&quot;RCV &lt;- {&quot; + event + &quot;}&quot;);                        //当事件的响应码区间为[600,700)，则发送消息交由 mCallbackHandler 处理                        if (event.isClassUnsolicited()) {                            // TODO: migrate to sending NativeDaemonEvent instances                            if (mCallbacks.onCheckHoldWakeLock(event.getCode())                                    &amp;&amp; mWakeLock != null) {                                mWakeLock.acquire();                                releaseWl = true;                            }                            if (mCallbackHandler.sendMessage(mCallbackHandler.obtainMessage(                                    event.getCode(), event.getRawEvent()))) {                                releaseWl = false;                            }                        } else {                            // 对于其他的响应码则添加到 mResponseQueue 队列                            // 该方法便能触发 ResponseQueue.poll 阻塞操作继续往下执行                            mResponseQueue.add(event.getCmdNumber(), event);                        }                    } catch (IllegalArgumentException e) {                        log(&quot;Problem parsing message &quot; + e);                    } finally {                        if (releaseWl) {                            mWakeLock.acquire();                        }                    }                    start = i + 1;                }            }            if (start == 0) {                log(&quot;RCV incomplete&quot;);            }            // We should end at the amount we read. If not, compact then            // buffer and read again.            if (start != count) {                final int remaining = BUFFER_SIZE - start;                System.arraycopy(buffer, start, buffer, 0, remaining);                start = remaining;            } else {                start = 0;            }        }    } catch (IOException ex) {        loge(&quot;Communications error: &quot; + ex);        throw ex;    } finally {        //收尾清理类工作，关闭mOutputStream, socket        synchronized (mDaemonLock) {            if (mOutputStream != null) {                try {                    loge(&quot;closing stream for &quot; + mSocket);                    mOutputStream.close();                } catch (IOException e) {                    loge(&quot;Failed closing output stream: &quot; + e);                }                mOutputStream = null;            }        }        try {            if (socket != null) {                socket.close();            }        } catch (IOException ex) {            loge(&quot;Failed closing socket: &quot; + ex);        }    }}</code></pre><p>内容比较多，这里也是比较重要的部分，建立连接后，便会进行onDaemonConnected回调。之后进入死循环不断监听，调用NativeDaemonEvent.parseRawEvent解析Socket内的元数据，根据事件类型进行分类处理。当事件的响应码区间为[600,700)，则发送消息交由 mCallbackHandler 处理，其他的则添加到 mResponseQueue 队列，触发 ResponseQueue.poll 阻塞操作继续往下执行。最终在finally释放锁和清理工作。</p><h3 id="onDaemonConnected"><a href="#onDaemonConnected" class="headerlink" title="onDaemonConnected"></a>onDaemonConnected</h3><pre><code>// 主线程发送消息 H_DAEMON_CONNECTED 给线程 MountService// 该线程收到消息后调用 MountServiceHandler 的 handleMessage()@Overridepublic void onDaemonConnected() {    mDaemonConnected = true;    mHandler.obtainMessage(H_DAEMON_CONNECTED).sendToTarget();}</code></pre><p>其通过handler，最终调用到handleDaemonConnected，如下：</p><h3 id="MountService-handleDaemonConnected"><a href="#MountService-handleDaemonConnected" class="headerlink" title="MountService.handleDaemonConnected"></a>MountService.handleDaemonConnected</h3><pre><code>private void handleDaemonConnected() {    synchronized (mLock) {        // 重置清理工作        resetIfReadyAndConnectedLocked();    }    /*     * Now that we&apos;ve done our initialization, release     * the hounds!     */    // 类型为 CountDownLatch，用于多线程同步，阻塞await()直到计数器为零    mConnectedSignal.countDown();    if (mConnectedSignal.getCount() != 0) {        // More daemons need to connect        return;    }    // On an encrypted device we can&apos;t see system properties yet, so pull    // the system locale out of the mount service.    if (&quot;&quot;.equals(SystemProperties.get(&quot;vold.encrypt_progress&quot;))) {        copyLocaleFromMountService();    }    // Let package manager load internal ASECs.    //调用PMS来加载ASECs    mPms.scanAvailableAsecs();    // Notify people waiting for ASECs to be scanned that it&apos;s done.    //用于通知ASEC扫描已完成    mAsecsScanned.countDown();}</code></pre><h4 id="mResponseQueue-add"><a href="#mResponseQueue-add" class="headerlink" title="mResponseQueue.add"></a>mResponseQueue.add</h4><pre><code>public void add(int cmdNum, NativeDaemonEvent response) {        PendingCmd found = null;        synchronized (mPendingCmds) {            for (PendingCmd pendingCmd : mPendingCmds) {                if (pendingCmd.cmdNum == cmdNum) {                    found = pendingCmd;                    break;                }            }            //没有找到则创建相应的PendingCmd            if (found == null) {                // 没有找到,在添加之前,确保队列容量不溢出                while (mPendingCmds.size() &gt;= mMaxCount) {                    Slog.e(&quot;NativeDaemonConnector.ResponseQueue&quot;,                            &quot;more buffered than allowed: &quot; + mPendingCmds.size() +                            &quot; &gt;= &quot; + mMaxCount);                    // let any waiter timeout waiting for this                    PendingCmd pendingCmd = mPendingCmds.remove();                    Slog.e(&quot;NativeDaemonConnector.ResponseQueue&quot;,                            &quot;Removing request: &quot; + pendingCmd.logCmd + &quot; (&quot; +                            pendingCmd.cmdNum + &quot;)&quot;);                }                found = new PendingCmd(cmdNum, null);                mPendingCmds.add(found);            }            found.availableResponseCount++;            // if a matching remove call has already retrieved this we can remove this            // instance from our list            if (found.availableResponseCount == 0) mPendingCmds.remove(found);        }        try {            found.responses.put(response);        } catch (InterruptedException e) { }    }</code></pre><p>mResponseQueue.add()，通过该方法便能触发ResponseQueue.poll阻塞操作继续往下执行。</p><p>到此,MountService与NativeDaemonConnector都已经启动，之前我们看到MountService内部静态类LifeCycle继承自SystemService，那么接下来到系统启动到达阶段PHASE_ACTIVITY_MANAGER_READY，则调用到其onBootPhase方法。</p><h2 id="Lifecycle-gt-onBootPhase"><a href="#Lifecycle-gt-onBootPhase" class="headerlink" title="Lifecycle-&gt;onBootPhase"></a>Lifecycle-&gt;onBootPhase</h2><pre><code>// 由SystenServer启动到指定阶段进行回调，其内部由mServices持有    @Override    public void onBootPhase(int phase) {        // 由于MountService的内部Lifecycle已添加SystemServiceManager的mServices服务列表；        // 系统启动到PHASE_ACTIVITY_MANAGER_READY时会回调mServices中的onBootPhase方法        if (phase == SystemService.PHASE_ACTIVITY_MANAGER_READY) {            mMountService.systemReady();        }    }</code></pre><p>由SystemService统一管理其声明流程，之后调用systemReady方法，如下：</p><h3 id="systemReady"><a href="#systemReady" class="headerlink" title="systemReady"></a>systemReady</h3><pre><code>private void systemReady() {    mSystemReady = true;    // 此处 mHandler = new MountServiceHandler(hthread.getLooper())    // 采用的是线程 ”MountService” 中的 Looper    // system_server 主线程通过 handler 向线程 ”MountService” 发送 H_SYSTEM_READY 消息    mHandler.obtainMessage(H_SYSTEM_READY).sendToTarget();}</code></pre><p>MountService的systemReady通过Handler发送消息到MountServiceHandler中进行处理，其收到消息处理后进入handleSystemReady方法</p><h3 id="handleSystemReady"><a href="#handleSystemReady" class="headerlink" title="handleSystemReady"></a>handleSystemReady</h3><pre><code>private void handleSystemReady() {    synchronized (mLock) {        // 重置...        resetIfReadyAndConnectedLocked();    }    // Start scheduling nominally-daily fstrim operations    // 计划执行日常的 fstrim 操作【】    MountServiceIdler.scheduleIdlePass(mContext);}</code></pre><p>主要做了两件事情，一件事重置操作，一件事执行fstrim操作。</p><h3 id="resetIfReadyAndConnectedLocked"><a href="#resetIfReadyAndConnectedLocked" class="headerlink" title="resetIfReadyAndConnectedLocked"></a>resetIfReadyAndConnectedLocked</h3><pre><code>private void resetIfReadyAndConnectedLocked() {    Slog.d(TAG, &quot;Thinking about reset, mSystemReady=&quot; + mSystemReady            + &quot;, mDaemonConnected=&quot; + mDaemonConnected);    if (mSystemReady &amp;&amp; mDaemonConnected) {        killMediaProvider();        // Mstar Android Patch Begin        new Thread() {            public void run() {                try {                    mConnector.execute(&quot;volume&quot;, &quot;reset&quot;);                    // Tell vold about all existing and started users                    //告知所有已经存在和启动的 users                    final UserManager um = mContext.getSystemService(UserManager.class);                    final List&lt;UserInfo&gt; users = um.getUsers();                    for (UserInfo user : users) {                        mConnector.execute(&quot;volume&quot;, &quot;user_added&quot;, user.id, user.serialNumber);                    }                    for (int userId : mStartedUsers) {                        mConnector.execute(&quot;volume&quot;, &quot;user_started&quot;, userId);                    }                } catch (NativeDaemonConnectorException e) {                    Slog.w(TAG, &quot;Failed to reset vold&quot;, e);                }            }        }.start();        // Mstar Android Patch End    }}</code></pre><p>首先检测系统是否启动完毕，是否和Vold建立连接，之后开启线程通过mConnector执行execute(“volume”, “reset”)，并再次通过mConnector告知所有的用户user_added和user_started。</p><h3 id="NDC-gt-execute"><a href="#NDC-gt-execute" class="headerlink" title="NDC-&gt;execute"></a>NDC-&gt;execute</h3><p>NativeDaemonConnector执行execute经过层层调用，最终走到executeForList，此时命令执行超时时长为1分钟，具体如下：</p><pre><code>public NativeDaemonEvent[] executeForList(long timeoutMs, String cmd, Object... args)        throws NativeDaemonConnectorException {    final long startTime = SystemClock.elapsedRealtime();    final ArrayList&lt;NativeDaemonEvent&gt; events = Lists.newArrayList();    final StringBuilder rawBuilder = new StringBuilder();    final StringBuilder logBuilder = new StringBuilder();    // mSequenceNumber初始化值为0，每执行一次该方法则进行加1操作    final int sequenceNumber = mSequenceNumber.incrementAndGet();    makeCommand(rawBuilder, logBuilder, sequenceNumber, cmd, args);    //例如：“7 volume mount”    final String rawCmd = rawBuilder.toString();    final String logCmd = logBuilder.toString();    log(&quot;SND -&gt; {&quot; + logCmd + &quot;}&quot;);    synchronized (mDaemonLock) {        if (mOutputStream == null) {            throw new NativeDaemonConnectorException(&quot;missing output stream&quot;);        } else {            try {                //将 cmd 写入到 socket 的输出流                mOutputStream.write(rawCmd.getBytes(StandardCharsets.UTF_8));            } catch (IOException e) {                throw new NativeDaemonConnectorException(&quot;problem sending command&quot;, e);            }        }    }    NativeDaemonEvent event = null;    do {        event = mResponseQueue.remove(sequenceNumber, timeoutMs, logCmd);        if (event == null) {            loge(&quot;timed-out waiting for response to &quot; + logCmd);            throw new NativeDaemonTimeoutException(logCmd, event);        }        if (VDBG) log(&quot;RMV &lt;- {&quot; + event + &quot;}&quot;);        events.add(event);        // 当收到的事件响应码属于[100,200)区间，则继续等待后续事件上报    } while (event.isClassContinue());    final long endTime = SystemClock.elapsedRealtime();    //对于执行时间超过500ms则会记录到log    if (endTime - startTime &gt; WARN_EXECUTE_DELAY_MS) {        loge(&quot;NDC Command {&quot; + logCmd + &quot;} took too long (&quot; + (endTime - startTime) + &quot;ms)&quot;);    }    if (event.isClassClientError()) {        throw new NativeDaemonArgumentException(logCmd, event);    }    if (event.isClassServerError()) {        throw new NativeDaemonFailureException(logCmd, event);    }    return events.toArray(new NativeDaemonEvent[events.size()]);}</code></pre><p>其流程也是比较重要的，主要是和Socket进行打交道。首先，将带执行的命令mSequenceNumber执行加1操作，再将cmd(例如7 volume mount)写入到socket的输出流，通知Vold进行处理。然后通过循环与poll机制等待执行底层响应该操作结果，否则直到1分钟超时才结束该方法。即便收到底层的响应码，如果响应码属于[100,200)区间，则继续阻塞等待后续事件上报。</p><h3 id="mResponseQueue-remove"><a href="#mResponseQueue-remove" class="headerlink" title="mResponseQueue.remove"></a>mResponseQueue.remove</h3><p>和上面的add操作相反，这里是将阻塞队列ResponseQueue移除操作，多有操作都是由ReentrantLock同步锁来控制，实现多线程下并发操作。</p><pre><code>public NativeDaemonEvent remove(int cmdNum, long timeoutMs, String logCmd) {    PendingCmd found = null;    synchronized (mPendingCmds) {        // 从mPendingCmds查询cmdNum        for (PendingCmd pendingCmd : mPendingCmds) {            if (pendingCmd.cmdNum == cmdNum) {                found = pendingCmd;                break;            }        }        // 如果已有的mPendingCmds中查询不到，则创建一个新的PendingCmd        if (found == null) {            found = new PendingCmd(cmdNum, logCmd);            mPendingCmds.add(found);        }        found.availableResponseCount--;        // if a matching add call has already retrieved this we can remove this        // instance from our list        if (found.availableResponseCount == 0) mPendingCmds.remove(found);    }    NativeDaemonEvent result = null;    try {        //采用poll轮询方式等待底层上报该事件，直到1分钟超时        //这里用到poll，先来看看responses        result = found.responses.poll(timeoutMs, TimeUnit.MILLISECONDS);    } catch (InterruptedException e) {}    if (result == null) {        Slog.e(&quot;NativeDaemonConnector.ResponseQueue&quot;, &quot;Timeout waiting for response&quot;);    }    return result;}</code></pre><ul><li>关于ArrayBlockingQueue可以参考如下链接：</li></ul><p><a href="http://blog.csdn.net/javazejian/article/details/77410889" target="_blank" rel="noopener">ArrayBlockingQueue</a></p><h1 id="MountService总结"><a href="#MountService总结" class="headerlink" title="MountService总结"></a>MountService总结</h1><p>MountService启动方式和其他的Service有点不同，主要是由其静态内部类LifeCycle控制其实例化过程，然后LifeCycle又继承SystemService，从而其受到SystemService的控制，系统启动到Ready阶段便会回调LifeCycle对应的方法，然后由发送消息到MountService进行处理。</p><p>MountService启动时候，自行建立了三个子线程：MountService、VoldConnector、CryptdConnector，其中VoldConnector，CryptdConnector分别对应监听Vold进程中的两个线程，接受他它们传递过来的消息。</p><p>其中重要的NativeDaemonConnector负责和Vold传递来的Socket进行通信，其运行在VoldConnector线程中。同理还有一个NDC运行在CryptdConnecto线程中。</p><p>部分内容参考如下：</p><p><a href="http://gityuan.com/2016/07/17/android-io/" target="_blank" rel="noopener">GitYuan</a></p><p><a href="http://blog.csdn.net/javazejian/article/details/77410889" target="_blank" rel="noopener">AttayBlockingQueue</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;上篇我们介绍了Android的存储系统中native层的Vold的启动流程，Vold在init进程中通过init脚本进行启动，而MountService则是在Java的服务总站SystemServer中配置启动的，下面我们就来看一下Java层的MountService是如何启动的。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="https://891904833.gitee.io/FuckCode/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="系统存储" scheme="https://891904833.gitee.io/FuckCode/tags/%E7%B3%BB%E7%BB%9F%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>源码分析之存储系统启动流程Vold</title>
    <link href="https://891904833.gitee.io/FuckCode/2018/09/28/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8B%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8BVold%E6%A8%A1%E5%9D%97/"/>
    <id>https://891904833.gitee.io/FuckCode/2018/09/28/源码分析之存储系统启动流程Vold模块/</id>
    <published>2018-09-28T02:05:52.000Z</published>
    <updated>2019-04-10T07:22:17.209Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Android的存储系统相对来讲还是比较复杂，其中主要包括了native层的Vold以及Java层的MountService。其中Vold在init进程中通过init脚本进行启动，而MountService则是在Java的服务总站SystemServer中配置启动的。它们之间的通信采用的Socket进行，而不是Binder机制，其主要的原因就是架构上比较简单，代码量也少。本文就从启动流程上分析安卓系统中的存储模块是如何加载起来的，下面就让我们直接进入正题。</strong></p><a id="more"></a><h1 id="Vold启动流程"><a href="#Vold启动流程" class="headerlink" title="Vold启动流程"></a>Vold启动流程</h1><p>native层的Vlod进程启动在init.rc脚本中进行配置的，其定义于 /system/core/rootdir/init.rc 中，我们看一下脚本文件内容。</p><h2 id="init脚本相关配置"><a href="#init脚本相关配置" class="headerlink" title="init脚本相关配置"></a>init脚本相关配置</h2><p>init.rc 脚本配置如下：</p><pre><code>service vold /system/bin/vold \        --blkid_context=u:r:blkid:s0 --blkid_untrusted_context=u:r:blkid_untrusted:s0 \        --fsck_context=u:r:fsck:s0 --fsck_untrusted_context=u:r:fsck_untrusted:s0    class core    socket vold stream 0660 root mount    socket cryptd stream 0660 root mount    ioprio be 2</code></pre><p>脚本文件声明服务 service 为 vold，对应的程序文件路径 /system/bin/vold，系统级 core 核心服务。</p><h2 id="Vold-main"><a href="#Vold-main" class="headerlink" title="Vold.main"></a>Vold.main</h2><p>init进程通过解析上述脚本文件内容，启动对应的Vold进程，进入到vold的main函数中。其函数文件位于 system/vold/main.cpp 中</p><pre><code>// vold 启动时都创建了若干对象int main(int argc, char** argv) {    setenv(&quot;ANDROID_LOG_TAGS&quot;, &quot;*:v&quot;, 1);    android::base::InitLogging(argv, android::base::LogdLogger(android::base::SYSTEM));    ...    VolumeManager *vm;    CommandListener *cl;    CryptCommandListener *ccl;    NetlinkManager *nm;    //解析参数    parse_args(argc, argv);    ...    // Quickly throw a CLOEXEC on the socket we just inherited from init    fcntl(android_get_control_socket(&quot;vold&quot;), F_SETFD, FD_CLOEXEC);    fcntl(android_get_control_socket(&quot;cryptd&quot;), F_SETFD, FD_CLOEXEC);    mkdir(&quot;/dev/block/vold&quot;, 0755);    // 用于cryptfs检查，并mount加密的文件系统    klog_set_level(6);    //创建单例对象 VolumeManager    if (!(vm = VolumeManager::Instance())) {        LOG(ERROR) &lt;&lt; &quot;Unable to create VolumeManager&quot;;        exit(1);    }    //创建单例对象 NetlinkManager    if (!(nm = NetlinkManager::Instance())) {        LOG(ERROR) &lt;&lt; &quot;Unable to create NetlinkManager&quot;;        exit(1);    }    if (property_get_bool(&quot;vold.debug&quot;, false)) {        vm-&gt;setDebug(true);    }    // 创建 CommandListener 对象    cl = new CommandListener();    // 创建 CryptCommandListener 对象    ccl = new CryptCommandListener();    // 将新创建的 CommandListener 对象 sl 赋值给 vm 对象的成员变量 mBroadcaster    vm-&gt;setBroadcaster((SocketListener *) cl);    nm-&gt;setBroadcaster((SocketListener *) cl);    if (vm-&gt;start()) {        PLOG(ERROR) &lt;&lt; &quot;Unable to start VolumeManager&quot;;        exit(1);    }    if (process_config(vm)) {        PLOG(ERROR) &lt;&lt; &quot;Error reading configuration... continuing anyways&quot;;    }    if (nm-&gt;start()) {        PLOG(ERROR) &lt;&lt; &quot;Unable to start NetlinkManager&quot;;        exit(1);    }    coldboot(&quot;/sys/block&quot;);//    coldboot(&quot;/sys/class/switch&quot;);    //启动响应命令的监听器    if (cl-&gt;startListener()) {        PLOG(ERROR) &lt;&lt; &quot;Unable to start CommandListener&quot;;        exit(1);    }    if (ccl-&gt;startListener()) {        PLOG(ERROR) &lt;&lt; &quot;Unable to start CryptCommandListener&quot;;        exit(1);    }    //Vold成为监听线程    while(1) {        sleep(1000);    }    LOG(ERROR) &lt;&lt; &quot;Vold exiting&quot;;    exit(0);}</code></pre><p>上述代码可以看出，Vold.main函数中主要做了以下几件事：</p><ul><li>解析传入参数</li><li>创建 VolumeManager，设置监听并启动</li><li>创建 NetlinkManager，设置监听并启动</li><li>创建 CommandListener</li><li>创建 CryptCommandListener</li><li>循环成为监听线程</li></ul><h3 id="VM流程分析"><a href="#VM流程分析" class="headerlink" title="VM流程分析"></a>VM流程分析</h3><h4 id="VM初始化"><a href="#VM初始化" class="headerlink" title="VM初始化"></a>VM初始化</h4><p>VolumeManager是Vold进程中最重要的几个类之一，其简而言之就是Vold的统筹管理类，代码位于 /system/vold/VolumeBase.cpp 下，我们看一下其初始化流程。</p><pre><code>// 单例模式VolumeManager *VolumeManager::Instance() {    if (!sInstance)        sInstance = new VolumeManager();    return sInstance;}VolumeManager::VolumeManager() {    mDebug = false;    mActiveContainers = new AsecIdCollection();    mBroadcaster = NULL;    mUmsSharingCount = 0;    mSavedDirtyRatio = -1;    // 当UMS获取时，则设置 dirty ratio 为 0    mUmsDirtyRatio = 0;}</code></pre><p>VolumeManager类内部通过单例模式进行创建，创建同时创建AsecIdCollection负责aesc文件的收集，从析构函数中可以看出其被手动删除释放。下面便是设置监听。</p><h4 id="VM-gt-setBroadcaster"><a href="#VM-gt-setBroadcaster" class="headerlink" title="VM-&gt;setBroadcaster"></a>VM-&gt;setBroadcaster</h4><p>VolumeManager设置监听后，VM才可以开启，并且监听线程才能启动。其代码位于头文件中。</p><pre><code>void setBroadcaster(SocketListener *sl) { mBroadcaster = sl; }</code></pre><p>代码中，将CommandListener 对象sl赋值给 mBroadcaster。</p><h4 id="VM-gt-start"><a href="#VM-gt-start" class="headerlink" title="VM-&gt;start"></a>VM-&gt;start</h4><p>VolumeManager设置监听后，便开启工作。</p><pre><code>int VolumeManager::start() {    // Always start from a clean slate by unmounting everything in    // directories that we own, in case we crashed.    // 卸载所有设备,已提供最干净的环境    unmountAll();    // Assume that we always have an emulated volume on internal    // storage; the framework will decide if it should be mounted.    CHECK(mInternalEmulated == nullptr);    // 创建Emulated内部存储    // 其类型为 EmulatedVolume，设备路径为/data/media    // id和label为“emulated”，mMountFlags=0    // EmulatedVolume 继承于 VolumeBase    mInternalEmulated = std::shared_ptr&lt;android::vold::VolumeBase&gt;(            new android::vold::EmulatedVolume(&quot;/data/media&quot;));    // EmulatedVolume继承VolumeBase    mInternalEmulated-&gt;create();    return 0;}</code></pre><p>VolumeManager开启时候，会线卸载所有设备，保证干净的环境，之后在重新建立存储化境。</p><h5 id="unmountAll卸载设备"><a href="#unmountAll卸载设备" class="headerlink" title="unmountAll卸载设备"></a>unmountAll卸载设备</h5><pre><code>int VolumeManager::unmountAll() {    std::lock_guard&lt;std::mutex&gt; lock(mLock);    // First, try gracefully unmounting all known devices    // 1.卸载内部存储    if (mInternalEmulated != nullptr) {        mInternalEmulated-&gt;unmount();    }    // 2.卸载外部存储    for (auto disk : mDisks) {        disk-&gt;unmountAll();    }    // Worst case we might have some stale mounts lurking around, so    // force unmount those just to be safe.    // 有可能存在严重的潜在的旧的挂载设备存在,强力卸载以保证安全    FILE* fp = setmntent(&quot;/proc/mounts&quot;, &quot;r&quot;);    if (fp == NULL) {        SLOGE(&quot;Error opening /proc/mounts: %s&quot;, strerror(errno));        return -errno;    }    // Some volumes can be stacked on each other, so force unmount in    // reverse order to give us the best chance of success.    std::list&lt;std::string&gt; toUnmount;    mntent* mentry;    while ((mentry = getmntent(fp)) != NULL) {        if (strncmp(mentry-&gt;mnt_dir, &quot;/mnt/&quot;, 5) == 0                || strncmp(mentry-&gt;mnt_dir, &quot;/storage/&quot;, 9) == 0) {            toUnmount.push_front(std::string(mentry-&gt;mnt_dir));        }    }    endmntent(fp);    for (auto path : toUnmount) {        SLOGW(&quot;Tearing down stale mount %s&quot;, path.c_str());        android::vold::ForceUnmount(path);    }    return 0;}</code></pre><p>unmountAll中主要卸载系统内部和外部设备，以提供最干净的设备环境。先卸载内部设备，在遍历外部设备，遍历卸载，之后再针对其他情况卸载而外干扰设备。</p><p>其中mInternalEmulated设备为EmulatedVolume类型，其继承了父类VolumeBase，其子类调用unmount方法，优先走父类VolumeBase的方法，其内容如下：</p><pre><code>status_t VolumeBase::unmount() {    if (mState != State::kMounted) {        LOG(WARNING) &lt;&lt; getId() &lt;&lt; &quot; unmount requires state mounted&quot;;        return -EBUSY;    }    // 设置状态，651，5    setState(State::kEjecting);    for (auto vol : mVolumes) {        if (vol-&gt;destroy()) {            LOG(WARNING) &lt;&lt; getId() &lt;&lt; &quot; failed to destroy &quot; &lt;&lt; vol-&gt;getId()                    &lt;&lt; &quot; stacked above&quot;;        }    }    mVolumes.clear();    status_t res = doUnmount();    // 设置状态，发送 651，0    setState(State::kUnmounted);    return res;}</code></pre><p>父类的上述方法中，首先设置状态，其最终通过VM的方法，向Socket发送命令。其具体代码如下：</p><pre><code>void VolumeBase::setState(State state) {    mState = state;    notifyEvent(ResponseCode::VolumeStateChanged, StringPrintf(&quot;%d&quot;, mState));}void VolumeBase::notifyEvent(int event) {    if (mSilent) return;    // 通过socket向MountService发送创建volume的命令(650)    VolumeManager::Instance()-&gt;getBroadcaster()-&gt;sendBroadcast(event,            getId().c_str(), false);}</code></pre><p>其次是调用子类的doUnmount方法，其对应到实现的子类自己的方法，这里的子类为EmulatedVolume，其对应的实现方法如下：</p><pre><code>status_t EmulatedVolume::doUnmount() {    if (mFusePid &gt; 0) {        kill(mFusePid, SIGTERM);        TEMP_FAILURE_RETRY(waitpid(mFusePid, nullptr, 0));        mFusePid = 0;    }    // 强制卸载fuse路径    ForceUnmount(mFuseDefault);    ForceUnmount(mFuseRead);    ForceUnmount(mFuseWrite);    rmdir(mFuseDefault.c_str());    rmdir(mFuseRead.c_str());    rmdir(mFuseWrite.c_str());    mFuseDefault.clear();    mFuseRead.clear();    mFuseWrite.clear();    return OK;}</code></pre><p>其内部具体的卸载方法就不一一细看了。最终，在再一次设置状态。到此内部设备就完成卸载，下面是外部设备的卸载过程。</p><pre><code>status_t Disk::unmountAll() {    for (auto vol : mVolumes) {        vol-&gt;unmount();    }    return OK;}</code></pre><p>其通过遍历，调用所有Disk的unmount实现卸载操作，这里就不一一分析了。其具体内容可以参考GitYuan博客内容。</p><p><a href="http://gityuan.com/2016/07/17/android-io/" target="_blank" rel="noopener">博客推荐</a></p><h5 id="EmulatedVolume-gt-create"><a href="#EmulatedVolume-gt-create" class="headerlink" title="EmulatedVolume-&gt;create"></a>EmulatedVolume-&gt;create</h5><p>EmulatedVolume设备的create方法，首先走父类的VolumeBase的create方法，如下：</p><pre><code>status_t VolumeBase::create() {    CHECK(!mCreated);    mCreated = true;    status_t res = doCreate();    // 通知VolumeCreated事件，发送650    notifyEvent(ResponseCode::VolumeCreated,            StringPrintf(&quot;%d \&quot;%s\&quot; \&quot;%s\&quot;&quot;, mType, mDiskId.c_str(), mPartGuid.c_str()));    // 设置为非挂载状态，发送651，0    setState(State::kUnmounted);    return res;}status_t VolumeBase::doCreate() {    return OK;}</code></pre><p>父类中检查状态后，直接调用doCreate方法，之后通过VM的Socket发送消息到MountService中。至此，VolumeManager便启动完成。</p><h3 id="NM流程分析"><a href="#NM流程分析" class="headerlink" title="NM流程分析"></a>NM流程分析</h3><h4 id="NM初始化"><a href="#NM初始化" class="headerlink" title="NM初始化"></a>NM初始化</h4><p>Vold.main中初始化VM后，便初始化NM，如下：</p><pre><code>// 同样是单利模式获取NetlinkManager *NetlinkManager::Instance() {    if (!sInstance)        sInstance = new NetlinkManager();    return sInstance;}NetlinkManager::NetlinkManager() {    mBroadcaster = NULL;}</code></pre><p>同样是单例模式进行实例化对象，之后便启动start函数，如下：</p><pre><code>int NetlinkManager::start() {    struct sockaddr_nl nladdr;    int sz = 64 * 1024;    int on = 1;    memset(&amp;nladdr, 0, sizeof(nladdr));    nladdr.nl_family = AF_NETLINK;    // 记录当前进程的pid    nladdr.nl_pid = getpid();    nladdr.nl_groups = 0xffffffff;    // 创建 event socket    if ((mSock = socket(PF_NETLINK, SOCK_DGRAM | SOCK_CLOEXEC,            NETLINK_KOBJECT_UEVENT)) &lt; 0) {        SLOGE(&quot;Unable to create uevent socket: %s&quot;, strerror(errno));        return -1;    }    // 设置 uevent 的 SO_RCVBUFFORCE 选项    if (setsockopt(mSock, SOL_SOCKET, SO_RCVBUFFORCE, &amp;sz, sizeof(sz)) &lt; 0) {        SLOGE(&quot;Unable to set uevent socket SO_RCVBUFFORCE option: %s&quot;, strerror(errno));        goto out;    }    // 设置 uevent 的 SO_PASSCRED 选项    if (setsockopt(mSock, SOL_SOCKET, SO_PASSCRED, &amp;on, sizeof(on)) &lt; 0) {        SLOGE(&quot;Unable to set uevent socket SO_PASSCRED option: %s&quot;, strerror(errno));        goto out;    }    // 绑定 uevent socket    if (bind(mSock, (struct sockaddr *) &amp;nladdr, sizeof(nladdr)) &lt; 0) {        SLOGE(&quot;Unable to bind uevent socket: %s&quot;, strerror(errno));        goto out;    }    // 在 NetlinkManager 启动中创建 NetlinkHandler    mHandler = new NetlinkHandler(mSock);    if (mHandler-&gt;start()) {        SLOGE(&quot;Unable to start NetlinkHandler: %s&quot;, strerror(errno));        goto out;    }    return 0;out:    close(mSock);    return -1;}</code></pre><p>其中内容比较多，最后我们发现其start时候，新建了一个NetlinkHandler负责通信。下面我们具体看看内部流程。</p><h4 id="NH流程分析"><a href="#NH流程分析" class="headerlink" title="NH流程分析"></a>NH流程分析</h4><h5 id="NH初始化"><a href="#NH初始化" class="headerlink" title="NH初始化"></a>NH初始化</h5><p>NetlinkHandler为NetlinkManager内部的handler，负责通信操作，具体如下：</p><pre><code>// NetlinkHandler继承于 NetlinkListener，NetlinkListener 继承于 SocketListener// new NetlinkHandler(mSock) 中参数 mSock 是用于与 Kernel 进行通信的 socket 对象// 由于这个继承关系，当 NetlinkHandler 初始化时会调用基类的初始化NetlinkHandler::NetlinkHandler(int listenerSocket) :                NetlinkListener(listenerSocket) {}int NetlinkHandler::start() {    return this-&gt;startListener();}</code></pre><p>其中如注释所写，NetlinkHandler继承于 NetlinkListener，NetlinkListener 继承于 SocketListener，我们看一下其父类和爷爷类：</p><pre><code>NetlinkListener::NetlinkListener(int socket) :                            SocketListener(socket, false) {    mFormat = NETLINK_FORMAT_ASCII;}SocketListener::SocketListener(const char *socketName, bool listen) {    init(socketName, -1, listen, false);}// 通过层层继承调用至此（SocketListener &lt;- NetlinkListener &lt;- NetlinkHandler &lt;- NetlinkManager）void SocketListener::init(const char *socketName, int socketFd, bool listen, bool useCmdNum) {    mListen = listen;    mSocketName = socketName;    // 用于监听 Kernel 发送过程的 uevent 事件    mSock = socketFd;    mUseCmdNum = useCmdNum;    // 初始化同步锁    pthread_mutex_init(&amp;mClientsLock, NULL);    // 创建 socket 通信的 client 端    mClients = new SocketClientCollection();}</code></pre><p>由此最终走到了SocketListener，其最终实现监听kernel发送过来的uevent事件，当然最终上层调用的开始监听也是在这里最终实现，后下面分析到这里会具体分析。</p><h5 id="NH-gt-start"><a href="#NH-gt-start" class="headerlink" title="NH-&gt;start"></a>NH-&gt;start</h5><p>NH初始化完成后，便进入start状态。代码跟踪如下：</p><pre><code>int NetlinkHandler::start() {    return this-&gt;startListener();}</code></pre><p>然后调用父类的方法，这里只在爷爷类SocketListener发现其实现代码，如下：</p><pre><code>int SocketListener::startListener(int backlog) {    if (!mSocketName &amp;&amp; mSock == -1) {        SLOGE(&quot;Failed to start unbound listener&quot;);        errno = EINVAL;        return -1;    } else if (mSocketName) {        // 获取所对应的 socket 的句柄        if ((mSock = android_get_control_socket(mSocketName)) &lt; 0) {            SLOGE(&quot;Obtaining file descriptor socket &apos;%s&apos; failed: %s&quot;,                mSocketName, strerror(errno));            return -1;        }        SLOGV(&quot;got mSock = %d for %s&quot;, mSock, mSocketName);        fcntl(mSock, F_SETFD, FD_CLOEXEC);    }    // 开始监听    if (mListen &amp;&amp; listen(mSock, backlog) &lt; 0) {        SLOGE(&quot;Unable to listen on socket (%s)&quot;, strerror(errno));        return -1;    } else if (!mListen)        // 创建 SocketClient 对象，并加入到 mClients 队列        mClients-&gt;push_back(new SocketClient(mSock, false, mUseCmdNum));    // 创建匿名管道    // 这是一个二元数组，mCtrlPipe[0]从管道读数据，mCtrlPipe[1]从管道写数据    if (pipe(mCtrlPipe)) {        SLOGE(&quot;pipe failed (%s)&quot;, strerror(errno));        return -1;    }    // 创建工作线程，线程运行函数threadStart    if (pthread_create(&amp;mThread, NULL, SocketListener::threadStart, this)) {        SLOGE(&quot;pthread_create (%s)&quot;, strerror(errno));        return -1;    }    return 0;}</code></pre><p>在SocketListener开始监听中，其创建SocketClient对象，加入到mClients队列中，然后对二元数组管道进行读写，最终通过pthread_create另外开启一个线程进行监听。线程代码开启如下：</p><pre><code>void *SocketListener::threadStart(void *obj) {    SocketListener *me = reinterpret_cast&lt;SocketListener *&gt;(obj);    //开始监听    me-&gt;runListener();    // 线程退出    pthread_exit(NULL);    return NULL;}void SocketListener::runListener() {    SocketClientCollection pendingList;    while(1) {        SocketClientCollection::iterator it;        fd_set read_fds;        int rc = 0;        int max = -1;        FD_ZERO(&amp;read_fds);        if (mListen) {            max = mSock;            FD_SET(mSock, &amp;read_fds);        }        FD_SET(mCtrlPipe[0], &amp;read_fds);        if (mCtrlPipe[0] &gt; max)            max = mCtrlPipe[0];        pthread_mutex_lock(&amp;mClientsLock);        for (it = mClients-&gt;begin(); it != mClients-&gt;end(); ++it) {            // NB: calling out to an other object with mClientsLock held (safe)            int fd = (*it)-&gt;getSocket();            FD_SET(fd, &amp;read_fds);            if (fd &gt; max) {                max = fd;            }        }        pthread_mutex_unlock(&amp;mClientsLock);        SLOGV(&quot;mListen=%d, max=%d, mSocketName=%s&quot;, mListen, max, mSocketName);        if ((rc = select(max + 1, &amp;read_fds, NULL, NULL, NULL)) &lt; 0) {            if (errno == EINTR)                continue;            SLOGE(&quot;select failed (%s) mListen=%d, max=%d&quot;, strerror(errno), mListen, max);            sleep(1);            continue;        } else if (!rc)            continue;        if (FD_ISSET(mCtrlPipe[0], &amp;read_fds)) {            char c = CtrlPipe_Shutdown;            TEMP_FAILURE_RETRY(read(mCtrlPipe[0], &amp;c, 1));            if (c == CtrlPipe_Shutdown) {                break;            }            continue;        }        if (mListen &amp;&amp; FD_ISSET(mSock, &amp;read_fds)) {            struct sockaddr addr;            socklen_t alen;            int c;            do {                alen = sizeof(addr);                c = accept(mSock, &amp;addr, &amp;alen);                SLOGV(&quot;%s got %d from accept&quot;, mSocketName, c);            } while (c &lt; 0 &amp;&amp; errno == EINTR);            if (c &lt; 0) {                SLOGE(&quot;accept failed (%s)&quot;, strerror(errno));                sleep(1);                continue;            }            fcntl(c, F_SETFD, FD_CLOEXEC);            pthread_mutex_lock(&amp;mClientsLock);            mClients-&gt;push_back(new SocketClient(c, true, mUseCmdNum));            pthread_mutex_unlock(&amp;mClientsLock);        }        /* Add all active clients to the pending list first */        pendingList.clear();        pthread_mutex_lock(&amp;mClientsLock);        for (it = mClients-&gt;begin(); it != mClients-&gt;end(); ++it) {            SocketClient* c = *it;            // NB: calling out to an other object with mClientsLock held (safe)            int fd = c-&gt;getSocket();            if (FD_ISSET(fd, &amp;read_fds)) {                pendingList.push_back(c);                c-&gt;incRef();            }        }        pthread_mutex_unlock(&amp;mClientsLock);        /* Process the pending list, since it is owned by the thread,        * there is no need to lock it */        while (!pendingList.empty()) {            /* Pop the first item from the list */            it = pendingList.begin();            SocketClient* c = *it;            pendingList.erase(it);            /* Process it, if false is returned, remove from list */            if (!onDataAvailable(c)) {                release(c, false);            }            c-&gt;decRef();        }    }}</code></pre><p>runListener中，通过死循环和加锁机制，不断读取上述二元数组内的数据，最终将读写到的数据通过方法onDataAvailable实现回调。</p><h5 id="SL-gt-onDataAvailable"><a href="#SL-gt-onDataAvailable" class="headerlink" title="SL-&gt;onDataAvailable"></a>SL-&gt;onDataAvailable</h5><p>这里我们不妨多看看其回调过程,其SocketListener本身没有实现这个方法，其直接子类NetlinkListener实现了，如下：</p><pre><code>// 父类 SocketListener 回调此方法bool NetlinkListener::onDataAvailable(SocketClient *cli){    int socket = cli-&gt;getSocket();    ssize_t count;    uid_t uid = -1;    bool require_group = true;    if (mFormat == NETLINK_FORMAT_BINARY_UNICAST) {        require_group = false;    }    // 多次尝试获取socket数据    count = TEMP_FAILURE_RETRY(uevent_kernel_recv(socket,            mBuffer, sizeof(mBuffer), require_group, &amp;uid));    if (count &lt; 0) {        if (uid &gt; 0)            LOG_EVENT_INT(65537, uid);        SLOGE(&quot;recvmsg failed (%s)&quot;, strerror(errno));        return false;    }    NetlinkEvent *evt = new NetlinkEvent();    // 解析消息并封装成 NetlinkEvent    if (evt-&gt;decode(mBuffer, count, mFormat)) {        //事件处理        onEvent(evt);    } else if (mFormat != NETLINK_FORMAT_BINARY) {        // Don&apos;t complain if parseBinaryNetlinkMessage returns false. That can        // just mean that the buffer contained no messages we&apos;re interested in.        SLOGE(&quot;Error decoding NetlinkEvent&quot;);    }    delete evt;    return true;}</code></pre><p>NetlinkListener在得到父类传来的数据后，将数据进行封装NetlinkEvent。最终实现onEvent方法回调。这里的onEvent接口是在哪声明的呢？根据头文件我们发现，其是一个虚函数，需要子类进行实现，其声明如下：</p><pre><code>-&gt; /system/vold/NetlinkHandler.hprotected:    virtual void onEvent(NetlinkEvent *evt);};</code></pre><p>由于NetlinkHandler为NetlinkListener的子类，我们查看发现其具体实现如下：</p><pre><code>// NetlinkListener 在 onDataAvailable 回调到此方法void NetlinkHandler::onEvent(NetlinkEvent *evt) {    VolumeManager *vm = VolumeManager::Instance();    const char *subsys = evt-&gt;getSubsystem();    if (!subsys) {        SLOGW(&quot;No subsystem found in netlink event&quot;);        return;    }    if (!strcmp(subsys, &quot;block&quot;)) {        // 调用 VolumeManager 处理块设备        vm-&gt;handleBlockEvent(evt);    }}</code></pre><p>到这里，最终调用VolumeManager的handleBlockEvent实现处理。至此我们先告一段落，在后面存储系统工作流程中进行具体分析。</p><h3 id="CommandListener流程分析"><a href="#CommandListener流程分析" class="headerlink" title="CommandListener流程分析"></a>CommandListener流程分析</h3><h4 id="CL初始化"><a href="#CL初始化" class="headerlink" title="CL初始化"></a>CL初始化</h4><p>分析完程VM和NM的流程，下面便是CommandListener的实例化过程，如下：</p><pre><code>CommandListener::CommandListener() :                FrameworkListener(&quot;vold&quot;, true) {    // 加入到 mCommands 队列    registerCmd(new DumpCmd());    registerCmd(new VolumeCmd());    registerCmd(new AsecCmd());    registerCmd(new ObbCmd());    registerCmd(new StorageCmd());    registerCmd(new FstrimCmd());    // MStar Android Patch Begin    registerCmd(new ISOCmd());    registerCmd(new SambaCmd());    // MStar Android Patch End}void FrameworkListener::registerCmd(FrameworkCommand *cmd) {    // 加入到mCommands队列    mCommands-&gt;push_back(cmd);}</code></pre><p>CommandListener在实例化中，注册了各种设备的操作命令，例如DumpCmd，VolumCmd等，其父类FrameworkListener做了啥，我们查看一下：</p><pre><code>FrameworkListener::FrameworkListener(const char *socketName) :                            SocketListener(socketName, true, false) {    init(socketName, false);}void FrameworkListener::init(const char *socketName UNUSED, bool withSeq) {    mCommands = new FrameworkCommandCollection();    errorRate = 0;    mCommandCount = 0;    mWithSeq = withSeq;    mSkipToNextNullByte = false;}</code></pre><p>父类FrameworkListener初始化又新建了FrameworkCommandCollection，这是干嘛的呀？其通过typedef重声明定义如下：</p><pre><code>typedef android::sysutils::List&lt;FrameworkCommand *&gt; FrameworkCommandCollection;FrameworkCommand::FrameworkCommand(const char *cmd) {    mCommand = cmd;}</code></pre><h5 id="关于DumpCmd，VolumCmd等几种cmd的介绍"><a href="#关于DumpCmd，VolumCmd等几种cmd的介绍" class="headerlink" title="关于DumpCmd，VolumCmd等几种cmd的介绍"></a>关于DumpCmd，VolumCmd等几种cmd的介绍</h5><p>在这里有必要介绍这几种DumpCmd，VolumCmd之间的关系，例如VolumCmd继承自VoldCommand继承自FrameworkCommand，FrameworkCommand有一个重要的方法runCommand实现如下：</p><pre><code>int FrameworkCommand::runCommand(SocketClient *c UNUSED, int argc UNUSED,                                char **argv UNUSED) {    SLOGW(&quot;Command %s has no run handler!&quot;, getCommand());    errno = ENOSYS;    return -1;}</code></pre><p>其中其没有必要的实现代码，我们继续通过继承关系向上追溯，发现其在CommandListener类中实现了具体代码，如下：</p><p>int CommandListener::VolumeCmd::runCommand(SocketClient *cli,<br>                                           int argc, char **argv) {<br>    dumpArgs(argc, argv, -1);</p><pre><code>if (argc &lt; 2) {    cli-&gt;sendMsg(ResponseCode::CommandSyntaxError, &quot;Missing Argument&quot;, false);    return 0;}VolumeManager *vm = VolumeManager::Instance();std::lock_guard&lt;std::mutex&gt; lock(vm-&gt;getLock());std::string cmd(argv[1]);if (cmd == &quot;reset&quot;) {    return sendGenericOkFail(cli, vm-&gt;reset());}...} else if (cmd == &quot;mount&quot; &amp;&amp; argc &gt; 2) {    // mount [volId] [flags] [user]    std::string id(argv[2]);    auto vol = vm-&gt;findVolume(id);    if (vol == nullptr) {        return cli-&gt;sendMsg(ResponseCode::CommandSyntaxError, &quot;Unknown volume&quot;, false);    }    int mountFlags = (argc &gt; 3) ? atoi(argv[3]) : 0;    userid_t mountUserId = (argc &gt; 4) ? atoi(argv[4]) : -1;    vol-&gt;setMountFlags(mountFlags);    vol-&gt;setMountUserId(mountUserId);    // Mstar Android Patch Begin    CommandListener::VolumeCmd::finished = false;    CommandListener::VolumeCmd::res = 1;    std::thread mThread(&amp;CommandListener::VolumeCmd::doMount,this,vol);    mThread.detach();    int i = 0;    // if mount time &gt; 3S ,return fail    while(i&lt;6 &amp;&amp; CommandListener::VolumeCmd::finished == false) {        SLOGD(&quot;sleep 0.5s&quot;);        usleep(500000); //sleep 0.5s        i++;    }    if (mountFlags &amp; android::vold::VolumeBase::MountFlags::kPrimary) {        vm-&gt;setPrimary(vol);    }    return sendGenericOkFail(cli, CommandListener::VolumeCmd::res);    // Mstar Android Patch End} else if (cmd == &quot;unmount&quot; &amp;&amp; argc &gt; 2) {    // unmount [volId]    std::string id(argv[2]);    auto vol = vm-&gt;findVolume(id);    if (vol == nullptr) {        return cli-&gt;sendMsg(ResponseCode::CommandSyntaxError, &quot;Unknown volume&quot;, false);    }    return sendGenericOkFail(cli, vol-&gt;unmount());} else if (cmd == &quot;format&quot; &amp;&amp; argc &gt; 3) {    // format [volId] [fsType|auto]    std::string id(argv[2]);    std::string fsType(argv[3]);    auto vol = vm-&gt;findVolume(id);    if (vol == nullptr) {        return cli-&gt;sendMsg(ResponseCode::CommandSyntaxError, &quot;Unknown volume&quot;, false);    }    return sendGenericOkFail(cli, vol-&gt;format(fsType));} ...return cli-&gt;sendMsg(ResponseCode::CommandSyntaxError, nullptr, false);</code></pre><p>}<br>其中代码比较长，实现了各种针对Volume设备的各种操作，比如 挂载mount，格式化format等等。。。</p><h4 id="CL-gt-startListener"><a href="#CL-gt-startListener" class="headerlink" title="CL-&gt;startListener"></a>CL-&gt;startListener</h4><p>下面我们查看一下CommandListener的startListener方法，CommandListener没有实现此方法，但是其继承自FrameworkListener，FrameworkListener也没有实现，其又继承自SocketListener，最终由SocketListener实现，如下：</p><pre><code>int SocketListener::startListener() {    return startListener(4);}int SocketListener::startListener(int backlog) {    if (!mSocketName &amp;&amp; mSock == -1) {        SLOGE(&quot;Failed to start unbound listener&quot;);        errno = EINVAL;        return -1;    } else if (mSocketName) {        // 获取所对应的 socket 的句柄        if ((mSock = android_get_control_socket(mSocketName)) &lt; 0) {            SLOGE(&quot;Obtaining file descriptor socket &apos;%s&apos; failed: %s&quot;,                mSocketName, strerror(errno));            return -1;        }        SLOGV(&quot;got mSock = %d for %s&quot;, mSock, mSocketName);        fcntl(mSock, F_SETFD, FD_CLOEXEC);    }    // 开始监听    if (mListen &amp;&amp; listen(mSock, backlog) &lt; 0) {        SLOGE(&quot;Unable to listen on socket (%s)&quot;, strerror(errno));        return -1;    } else if (!mListen)        // 创建 SocketClient 对象，并加入到 mClients 队列        mClients-&gt;push_back(new SocketClient(mSock, false, mUseCmdNum));    // 创建匿名管道    // 这是一个二元数组，mCtrlPipe[0]从管道读数据，mCtrlPipe[1]从管道写数据    if (pipe(mCtrlPipe)) {        SLOGE(&quot;pipe failed (%s)&quot;, strerror(errno));        return -1;    }    // 创建工作线程，线程运行函数threadStart    if (pthread_create(&amp;mThread, NULL, SocketListener::threadStart, this)) {        SLOGE(&quot;pthread_create (%s)&quot;, strerror(errno));        return -1;    }    return 0;}</code></pre><p>至此，CommandListener的startListener就分析完毕。</p><h3 id="CryptCommandListener流程分析"><a href="#CryptCommandListener流程分析" class="headerlink" title="CryptCommandListener流程分析"></a>CryptCommandListener流程分析</h3><h4 id="CLL初始化"><a href="#CLL初始化" class="headerlink" title="CLL初始化"></a>CLL初始化</h4><p>CryptCommandListener的初始化流程和上述的CommandListener基本相似，这了我们粗略的看一下，有兴趣的读者自行查看：</p><pre><code>CryptCommandListener::CryptCommandListener() :FrameworkListener(&quot;cryptd&quot;, true) {    registerCmd(new CryptfsCmd());}CryptCommandListener::CryptfsCmd::CryptfsCmd() :                VoldCommand(&quot;cryptfs&quot;) {}</code></pre><p>通过额外的registerCmd注册CryptfsCmd命令，其具体的runCommand实现对应的操作，这里就不贴代码了。</p><h4 id="CLL-gt-startListener"><a href="#CLL-gt-startListener" class="headerlink" title="CLL-&gt;startListener"></a>CLL-&gt;startListener</h4><p>当然CryptCommandListener也没有实现方法startListener，其最终还是通过SocketListener来实现的。具体参考上述流程。</p><h2 id="相关类图"><a href="#相关类图" class="headerlink" title="相关类图"></a>相关类图</h2><p>相信到此，如果你认真的看完第一遍并且是第一次看完的话，肯定是一头雾水，什么跟什么啊，这么多类，各种跳真的是烦。不急不急，这里我们引用一张类图还是有必要的，根据这张类图，在回过去看代码流程，你就能轻松的知道各个类之间的关系是如何的。</p><h3 id="VM相关类图"><a href="#VM相关类图" class="headerlink" title="VM相关类图"></a>VM相关类图</h3><img src="/FuckCode/2018/09/28/源码分析之存储系统启动流程Vold模块/VM.jpg" class="VolumeManger相关类图"><p>VolumeManager作为Vold模块最重要的一个类，其承载了很多重要的工作流程，其中涉及到很多类，类中各种继承关系也很复杂，对着上述的类图就很容易理清各个类之间的关系了。</p><h3 id="NM相关类图"><a href="#NM相关类图" class="headerlink" title="NM相关类图"></a>NM相关类图</h3><img src="/FuckCode/2018/09/28/源码分析之存储系统启动流程Vold模块/NM.jpg" class="VolumeManger相关类图"><p>NetLinkManager负责直接接收Kernel发来的uevent事件，其类之间的关系更是复杂，相信有这张类图后，在回溯到代码流程中就很清晰了。SocketListener为最重要的父类，其直接和Socket打交道，之后在分发到子类中处理，其大概可以分为两个部分，如下：</p><h4 id="命令流程如下："><a href="#命令流程如下：" class="headerlink" title="命令流程如下："></a>命令流程如下：</h4><p>SocketListener发现Socket中有数据后，对数据进行封装（NetlinkEvent），通过dispatchCommand方法，将命令分别分发到指定的Cmd进行处理，各种Cmd的声明和处理流程在CommandListener中都有声明。其中各种Cmd相似的方法处理都由父类FrameworkCommand进行统一的抽象管理。</p><h4 id="命令分发如下："><a href="#命令分发如下：" class="headerlink" title="命令分发如下："></a>命令分发如下：</h4><p>当然SocketListener得到数据后，还会通过onDataAvaiable进行回传，到Netlinkhandler中通过onEventh回传到VolumeManager进行处理。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="内容说明"><a href="#内容说明" class="headerlink" title="内容说明"></a>内容说明</h3><p>本想一篇章写完Android存储系统中启动流程，其主要包括native层的Vold和Java层MountService。介于篇幅太长，下面会在独立篇章介绍MountService启动流程以及两者之间的通信过程，希望三篇博客可以结束这段痛苦的代码阅读之旅。</p><h3 id="引用说明"><a href="#引用说明" class="headerlink" title="引用说明"></a>引用说明</h3><p>此博客内容为作者实战编辑，并非随便的拷贝，内容参考学习GitYuan博客中内容，这里郑重说明一下。其链接见下面。</p><p><a href="http://gityuan.com/2016/07/17/android-io/" target="_blank" rel="noopener">GitYuan博客推荐</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Android的存储系统相对来讲还是比较复杂，其中主要包括了native层的Vold以及Java层的MountService。其中Vold在init进程中通过init脚本进行启动，而MountService则是在Java的服务总站SystemServer中配置启动的。它们之间的通信采用的Socket进行，而不是Binder机制，其主要的原因就是架构上比较简单，代码量也少。本文就从启动流程上分析安卓系统中的存储模块是如何加载起来的，下面就让我们直接进入正题。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="https://891904833.gitee.io/FuckCode/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="存储系统" scheme="https://891904833.gitee.io/FuckCode/tags/%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Android系统HAL层驱动开发实战</title>
    <link href="https://891904833.gitee.io/FuckCode/2018/08/22/Android%E7%B3%BB%E7%BB%9FHAL%E5%B1%82%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/"/>
    <id>https://891904833.gitee.io/FuckCode/2018/08/22/Android系统HAL层驱动开发实战/</id>
    <published>2018-08-22T01:09:21.000Z</published>
    <updated>2018-09-28T09:24:30.478Z</updated>
    
    <content type="html"><![CDATA[<p><strong>基于Android HAL层驱动框架，开发者很容易将自己的驱动实现在用户空间，实现驱动细节的隐藏，而不必在内核中进行修改，避免因修改内核，公开源码进而损害自己的商业利益。本章节，我们就对HAL层驱动框架进行实现，由底层驱动到中间层framework，继而进入上层应用端，彻底完成上层应用到底层驱动的一一实现。</strong></p><a id="more"></a><h3 id="HAL层驱动实现"><a href="#HAL层驱动实现" class="headerlink" title="HAL层驱动实现"></a>HAL层驱动实现</h3><p>HAL层驱动框架的介绍在上篇章中已经详细的说明了。本节就在上节篇章中进行补充和说明，在框架基础上，代码实现自定义的驱动程序。本篇章中实现的驱动原型为led灯，包括设备初始化，打开，关闭以及设备移除等。</p><h4 id="HAL头文件"><a href="#HAL头文件" class="headerlink" title="HAL头文件"></a>HAL头文件</h4><p>HAL驱动的头文件声明，可供实体文件导入，在JNI实现代码中同样需要导入，具体代码如下：</p><pre><code>路径：hardware/libhardware/include/hardware/led.h#ifndef ANDROID_LED_INTERFACE_H#define ANDROID_LED_INTERFACE_H#include &lt;hardware/hardware.h&gt;__BEGIN_DECLS#define LED_HARDWARE_MODULE_ID &quot;led&quot;#define LED_HARDWARE_DEVICE_ID &quot;led&quot;typedef struct led_module_t {    struct hw_module_t common;} led_module_t;typedef struct led_device_t {    struct hw_device_t common;    int (*set_on)(struct led_device_t* dev);    int (*set_off)(struct led_device_t* dev);    int (*getCount)(struct led_device_t* dev);} led_device_t;static inline int led_device_open(const struct hw_module_t* module,    led_device_t** dev){    return module-&gt;methods-&gt;open(module,LED_HARDWARE_DEVICE_ID,        (struct hw_device_t**) dev);}static inline int led_device_close(led_device_t* dev){    return dev-&gt;common.close(&amp;dev-&gt;common);}__END_DECLS#endif</code></pre><p>代码相关说明：</p><ol><li>声明模块ID，设备ID均为“led”</li><li>定义结构体 led_module_t 及 led_device_t 时便进行全局 typedef ，方便导包直接使用</li><li>设备声明三个函数指针，set_on开灯，set_off关灯，getCount灯数目</li><li>两个静态内联函数 led_device_open打开模块设备，led_device_close关闭模块设备</li></ol><p><strong><em>注意：文件路径的include同级目录下，应该有hardware.h 文件，此文件源码框架本身已经提供，不必自己实现</em></strong></p><h4 id="HAL实体文件"><a href="#HAL实体文件" class="headerlink" title="HAL实体文件"></a>HAL实体文件</h4><p>本体文件代码如下：</p><pre><code>代码路径：hardware/libhardware/modules/led/led.c#define LOG_TAG &quot;LedHALStub&quot;#include &lt;hardware/hardware.h&gt;#include &lt;hardware/led.h&gt;#include &lt;cutils/log.h&gt;#include &lt;malloc.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include &lt;errno.h&gt;#include &lt;math.h&gt;#include &lt;memory.h&gt;#define MODULE_NAME &quot;Led&quot;#define MODULE_AUTHOR &quot;891904833@qq.com&quot;static int led_set_on(led_device_t* dev) {    if(!dev) {        ALOGI(&quot;Null dev pointer.&quot;);        return -EFAULT;    }    ALOGI(&quot;led_set_on function call!&quot;);    return 0;}static int led_set_off(led_device_t* dev) {    if(!dev) {        ALOGI(&quot;Null dev pointer.&quot;);        return -EFAULT;    }    ALOGI(&quot;led_set_off function call!&quot;);    return 0;}static int led_getCount(led_device_t* device) {    ALOGI(&quot;led_getCount function call,return led_getCount is 4!&quot;);    return 4;}static int led_close(hw_device_t* device) {    if(device){        free(device);    }    ALOGI(&quot;led_device close successfully!&quot;);    return 0;}static int led_open(const hw_module_t* module, const char* id,    hw_device_t** device) {        // dev = (struct led_device_t*)malloc(sizeof(struct led_device_t));        led_device_t *dev = (led_device_t *)calloc(1, sizeof(led_device_t));        if(!dev) {            ALOGE(&quot;Failed to alloc space for led_device_t.&quot;);            return -EFAULT;        }        memset(dev, 0, sizeof(struct led_device_t));        dev-&gt;common.tag = HARDWARE_DEVICE_TAG;        dev-&gt;common.version = 0;        dev-&gt;common.module = (struct hw_module_t*)module;        dev-&gt;common.close = led_close;        dev-&gt;set_on = led_set_on;        dev-&gt;set_off = led_set_off;        dev-&gt;getCount = led_getCount;        *device = &amp;dev-&gt;common;        ALOGI(&quot;Open led_device successfully!&quot;);        return 0;}static struct hw_module_methods_t my_methods = {    .open = led_open,};struct led_module_t HAL_MODULE_INFO_SYM = {    .common = {        .tag = HARDWARE_MODULE_TAG,        .version_major = 1,        .version_minor = 0,        .id = LED_HARDWARE_MODULE_ID,        .name = MODULE_NAME,        .author = MODULE_AUTHOR,        .methods = &amp;my_methods,    },};</code></pre><p>代码相关说明：</p><ol><li>前面几个静态函数是函数指针方法的具体实现，下面在open设备时实现方法的一一映射</li><li>led_open 方法实现设备的初始化，包括内核分配内存，led_device_t 内相关指针函数方法的映射等</li><li>my_methods 方法为框架中结构体 hw_module_t 内 open 方法的具体映射</li><li>HAL_MODULE_INFO_SYM 为HAL框架的必要说明，方便加载动态库时识别为驱动模块，并依据ID找到对应的模块驱动。同时完成 hw_module_t 必要方法 methods 的映射</li><li>为了方便调试，HAL驱动的具体实现中仅进行Log打印，没有协助内核进行相关操作，之后在上层调用中如果查看到 “LedHALStub” 日志，即代表驱动调用ok</li></ol><h4 id="编译脚本"><a href="#编译脚本" class="headerlink" title="编译脚本"></a>编译脚本</h4><p>编译文件 Android.mk 将源代码编译成动态链接库，存放在设备的 /system/lib/hw/led.default.so 和 /system/lib64/hw/led.default.so，代码如下：</p><pre><code>LOCAL_PATH := $(call my-dir)include $(CLEAR_VARS)LOCAL_MODULE_TAGS := optionalLOCAL_PRELINK_MODULE := falseLOCAL_MODULE_RELATIVE_PATH := hwLOCAL_SHARED_LIBRARIES := \    liblog \    libutilsLOCAL_SRC_FILES := led.cppLOCAL_MODULE := led.defaultinclude $(BUILD_SHARED_LIBRARY)</code></pre><h3 id="framework层实现"><a href="#framework层实现" class="headerlink" title="framework层实现"></a>framework层实现</h3><p>HAL层驱动框架实现完成，下面就进行framework层的实现。大概调用流程如下：首先系统提供一个全局的LedService服务，系统设备启动中将其注册到ServiceManager中，其后上层应用通过跨进程aidl方式，远程调用服务内方法，最后在服务内通过jni方式再调用到底层驱动程序，实现上层到中间层继而底层驱动的全程跨度。</p><h4 id="AIDL声明"><a href="#AIDL声明" class="headerlink" title="AIDL声明"></a>AIDL声明</h4><p>AIDL的详细说明在之前的篇章中已经详细介绍过了，下面直接看服务的接口声明：</p><pre><code>代码路径：frameworks/base/core/java/android/os/ILedService.aidlpackage android.os;/** {@hide} */interface ILedService{    int LedInit();    void LedSetOn();    void LedSetOff();    void LedClose();    int LedGetCount();}</code></pre><p>代码说明：<br>声明五个接口，对应设备的初始化，打开，关闭，灯数目以及设备移除。</p><h5 id="编译AIDL文件"><a href="#编译AIDL文件" class="headerlink" title="编译AIDL文件"></a>编译AIDL文件</h5><p>声明完成AIDL文件，修改framework/base下的Android.mk文件，将ILedService.aidl加入到编译脚本aidl文件中去。</p><pre><code>LOCAL_SRC_FILES += \    core/java/android/accessibilityservice/IAccessibilityServiceConnection.aidl \    ...    core/java/android/os/ILedService.aidl \  ...</code></pre><p>之后执行 mmm framework/base 局部编译，生成aidl对应的java文件，文件位于 out/target/common/obj/JAVA_LIBRARIES/framework_intermediates/src/core/java/android/os/ILedService.java</p><h4 id="自定义服务实现"><a href="#自定义服务实现" class="headerlink" title="自定义服务实现"></a>自定义服务实现</h4><p>有了AIDL文件的接口方法，下面就进行自定义服务的实现。首先找到同级文件夹 frameworks/base/services/core/java/com/android/server/ 下，自定义 LedService 继承AIDL文件中间类 ILedService.Stub ，具体代码如下：</p><pre><code>package com.android.server;import android.os.ILedService;import android.util.Slog;public class LedService extends ILedService.Stub {    private static final String TAG = &quot;LedService&quot;;    public int LedInit() throws android.os.RemoteException    {        return native_LedInit();    }    public void LedSetOn() throws android.os.RemoteException    {        Slog.d(TAG,&quot;LedService native_LedSetOn&quot;);        native_LedSetOn();    }    public void LedSetOff() throws android.os.RemoteException    {        Slog.d(TAG,&quot;LedService native_LedSetOff&quot;);        native_LedSetOff();    }    public int LedGetCount() throws android.os.RemoteException    {        Slog.d(TAG,&quot;LedService native_LedGetCount&quot;);        return native_LedGetCount();    }    public void LedClose() throws android.os.RemoteException{        Slog.d(TAG,&quot;LedService native_LedClose&quot;);        native_LedClose();    }    public LedService()    {        Slog.d(TAG,&quot;LedService started!&quot;);    }    native int native_LedInit();    native void native_LedSetOn();    native void native_LedSetOff();    native int native_LedGetCount();    native void native_LedClose();}</code></pre><p>代码相关说明：</p><ol><li>ILedService.Stub 类为aidl文件经过系统编译后生成的本地Binder类，实现跨进层通信的基础是基于安卓Binder框架，这里通过继承 ILedService.Stub 类，可以直接调用框架中实现好的Binder体系中重要的两个方法 transact(…) 和 onTransact(…)</li><li>对于接口中的每个方法，最终都归结实现在native层jni方法中</li></ol><h4 id="加入系统服务SM"><a href="#加入系统服务SM" class="headerlink" title="加入系统服务SM"></a>加入系统服务SM</h4><p>完成自定义服务的编写，下面就需要将其加入到SM中，方便后续上层应用进行调用。由于系统开机会在SystemServer中进行相关服务的初始化登记动作，这里将LedService加入到SystemServer中去即可，具体代码如下：</p><pre><code>代码路径：frameworks/base/services/core/java/com/android/server/SystemServer.java...Slog.i(TAG, &quot;Vibrator Service&quot;);vibrator = new VibratorService(context);ServiceManager.addService(&quot;vibrator&quot;, vibrator);// new code by alliesLedService led = null;Slog.i(TAG, &quot;Led Service&quot;);led = new LedService();ServiceManager.addService(&quot;led&quot;, led);...</code></pre><p>这里找到 VibratorService 震动服务，将 LedService 加入到此段代码下面即可，其他代码这里省略。</p><h4 id="JNI方法实现"><a href="#JNI方法实现" class="headerlink" title="JNI方法实现"></a>JNI方法实现</h4><p>LedService中的native方法声明完成，下面就要对此jni方法进行实现。这里依据其他服务jni方法声明文件方式，同级目录下新建文档 com_android_server_LedService.cpp ,具体代码如下：</p><pre><code>代码路径：frameworks/base/services/core/jni/com_android_server_LedService.cpp#define LOG_TAG &quot;LedServiceJNI&quot;#include &quot;jni.h&quot;#include &quot;JNIHelp.h&quot;#include &quot;android_runtime/AndroidRuntime.h&quot;#include &lt;utils/misc.h&gt;#include &lt;utils/Log.h&gt;#include &lt;hardware/hardware.h&gt;#include &lt;hardware/led.h&gt;#include &lt;stdio.h&gt;namespace android{    static led_device_t* mLedDevice = 0;    static jint led_init(JNIEnv *env, jobject instance){        led_module_t* module;        ALOGI(&quot;led_init start ...&quot;);        if (hw_get_module(LED_HARDWARE_MODULE_ID, (hw_module_t const**)&amp;module) == 0) {              ALOGI(&quot;Device led found...&quot;);            if (led_device_open(&amp;(module-&gt;common), &amp;mLedDevice ) == 0) {                ALOGI(&quot;led_init success!!!&quot;);                return 0;            }            ALOGE(&quot;led_init error!!!&quot;);        }        ALOGI(&quot;hw_get_module function call error!!!&quot;);        return -1;    }    static jint led_getCount(JNIEnv *env, jobject thiz) {        ALOGI(&quot;led_getCount 4&quot;);        return mLedDevice-&gt;getCount(mLedDevice);;    }    static void led_setOn(JNIEnv *env, jobject thiz) {        ALOGI(&quot;led_set_on&quot;);        mLedDevice-&gt;set_on(mLedDevice);    }    static void led_setOff(JNIEnv *env, jobject thiz) {        ALOGI(&quot;led_set_off&quot;);        mLedDevice-&gt;set_off(mLedDevice);    }    static void led_close(JNIEnv *env, jobject thiz) {        ALOGI(&quot;led_close&quot;);        led_device_close(mLedDevice);    }    static const JNINativeMethod method_table[] = {        {&quot;native_LedInit&quot;, &quot;()I&quot;, (void*)led_init},        {&quot;native_LedSetOn&quot;, &quot;()V&quot;, (void*)led_setOn},        {&quot;native_LedSetOff&quot;, &quot;()V&quot;, (void*)led_setOff},        {&quot;native_LedGetCount&quot;, &quot;()I&quot;, (void*)led_getCount},        {&quot;native_LedClose&quot;, &quot;()V&quot;, (void*)led_close},    };    int register_android_server_LedService(JNIEnv *env) {            return jniRegisterNativeMethods(env, &quot;com/android/server/LedService&quot;, method_table, NELEM(method_table));    }};</code></pre><p>代码相关说明：</p><ol><li>led_init 为设备的初始化，主要有两个重要操作：其一，调用HAL框架公用方法 hw_get_module 依据传入的模块ID，加载对应的驱动模块文件，并完成框架结构体 hw_module_t 内 methods方法的映射；其二，调用 led.h 头文件内联函数 led_device_open 打开模块设备，完成 led_device_t 结构体内相关指针函数的映射，之后，将设备引用存放于 mLedDevice 中，方便后续具体方法的调用</li><li>几个静态方法的具体实现，实质上是调用 mLedDevice 变量设备的方法，及驱动中对应的方法</li><li>method_table 静态常数组，其内存放了上层native方法声明和本c层实体方法的一一映射</li><li>register_android_server_LedService jni方法的注册具体实现，此方法需要在系统启动时，在onload.cpp中指定</li></ol><h4 id="自定义方法JNI注册实现"><a href="#自定义方法JNI注册实现" class="headerlink" title="自定义方法JNI注册实现"></a>自定义方法JNI注册实现</h4><p>修改同级目录下 frameworks/base/services/core/jni/onload.cpp 文件，将 com_android_server_LedService.cpp 文件注册到系统中去。</p><pre><code>代码路径：frameworks/base/services/core/jni/onload.cpp#include &quot;JNIHelp.h&quot;#include &quot;jni.h&quot;#include &quot;utils/Log.h&quot;#include &quot;utils/misc.h&quot;namespace android {int register_android_server_AlarmManagerService(JNIEnv* env);int register_android_server_AssetAtlasService(JNIEnv* env);...int register_android_server_LedService(JNIEnv* env);};using namespace android;extern &quot;C&quot; jint JNI_OnLoad(JavaVM* vm, void* /* reserved */){    JNIEnv* env = NULL;    jint result = -1;    if (vm-&gt;GetEnv((void**) &amp;env, JNI_VERSION_1_4) != JNI_OK) {        ALOGE(&quot;GetEnv failed!&quot;);        return result;    }    ALOG_ASSERT(env, &quot;Could not retrieve the env!&quot;);    register_android_server_PowerManagerService(env);    register_android_server_SerialService(env);    ...    register_android_server_LedService(env);    return JNI_VERSION_1_4;}</code></pre><p>在对应的地方，依照其他代码方式，添加自己声明的jni方法注册实现即可。</p><h4 id="编译脚本修改"><a href="#编译脚本修改" class="headerlink" title="编译脚本修改"></a>编译脚本修改</h4><p>到这里已经修改完成了framework层相关文件的编写，之后我们就需要修改此层文件中的编译脚本，将添加的文件加入编译系统中去。</p><pre><code>代码路径：frameworks/base/services/core/jni/Android.mkLOCAL_REL_DIR := core/jniLOCAL_CFLAGS += -Wall -Werror -Wno-unused-parameterLOCAL_SRC_FILES += \    $(LOCAL_REL_DIR)/com_android_server_AlarmManagerService.cpp \    $(LOCAL_REL_DIR)/com_android_server_am_BatteryStatsService.cpp \    ...    $(LOCAL_REL_DIR)/com_android_server_LedService.cpp \    $(LOCAL_REL_DIR)/onload.cpp \    ...</code></pre><p>源码跟路径下，执行 mmm frameworks/base/services/ 命令，编译生成相关文件之后，make snod 将编译变动文件打包到system.img镜像中，升级系统。</p><h3 id="应用层实现"><a href="#应用层实现" class="headerlink" title="应用层实现"></a>应用层实现</h3><p>到这里，HAL层驱动，framework层框架都以搭建完成，系统刷写新的rom后，下面我们就要对此进行上层应用的调用。</p><h4 id="导入AIDL文件"><a href="#导入AIDL文件" class="headerlink" title="导入AIDL文件"></a>导入AIDL文件</h4><p>由于需要跨进程进行系统服务的调用，这里需要将之前声明的AIDL文件拷贝到Android Studio中，锤子一下，生成相应的Stub文件，方便后续服务的获取。注意AIDL文件的包名要和系统中的相一致。</p><h4 id="项目源码示例"><a href="#项目源码示例" class="headerlink" title="项目源码示例"></a>项目源码示例</h4><p>对于Android Studio项目的全程源码，在这里就不贴了。主要的逻辑还是要看一下。如下：</p><p>package allies.showame.com.jni_hal_demo;</p><p>import android.os.Bundle;<br>import android.os.ILedService;<br>import android.os.RemoteException;<br>import android.os.ServiceManager;<br>import android.support.v7.app.AppCompatActivity;<br>import android.util.Log;<br>import android.view.View;</p><p>public class MainActivity extends AppCompatActivity implements View.OnClickListener {</p><pre><code>private static final String TAG = &quot;Led_MainActivity&quot;;private View btn_setOff;private View btn_setOn;private View btn_init;private View btn_getCount;private View btn_close;static boolean ifInited = false;private ILedService ledProxy = null;@Overrideprotected void onCreate(Bundle savedInstanceState) {    super.onCreate(savedInstanceState);    setContentView(R.layout.activity_main);    initViews();    setOnClick();    enableBtn();}@Overrideprotected void onResume() {    ledProxy = ILedService.Stub.asInterface(ServiceManager.getService(&quot;led&quot;));    if (ledProxy == null)        Log.e(TAG, &quot;led service is null!!!&quot;);    else        Log.e(TAG, &quot;led service stub get success!!!&quot;);    super.onResume();}private void enableBtn() {    if (ifInited) {        btn_setOff.setEnabled(true);        btn_setOn.setEnabled(true);        btn_getCount.setEnabled(true);        btn_close.setEnabled(true);    } else {        btn_setOff.setEnabled(false);        btn_setOn.setEnabled(false);        btn_getCount.setEnabled(false);        btn_close.setEnabled(false);    }}private void initViews() {    btn_init = findViewById(R.id.btn_init);    btn_setOn = findViewById(R.id.btn_setOn);    btn_setOff = findViewById(R.id.btn_setOff);    btn_getCount = findViewById(R.id.btn_getCount);    btn_close = findViewById(R.id.btn_close);}private void setOnClick() {    btn_init.setOnClickListener(this);    btn_setOn.setOnClickListener(this);    btn_setOff.setOnClickListener(this);    btn_getCount.setOnClickListener(this);    btn_close.setOnClickListener(this);}@Overridepublic void onClick(View v) {    int id = v.getId();    int i = -1;    switch (id) {        case R.id.btn_init:            try {                i = ledProxy.LedInit();            } catch (RemoteException e) {                e.printStackTrace();            }            Log.d(TAG, &quot;led init...&quot; + String.valueOf(i));            if (!ifInited) {                ifInited = true;                enableBtn();            }            break;        case R.id.btn_setOn:            try {                ledProxy.LedSetOn();            } catch (RemoteException e) {                e.printStackTrace();            }            Log.d(TAG, &quot;led setOn...&quot;);            break;        case R.id.btn_setOff:            try {                ledProxy.LedSetOff();            } catch (RemoteException e) {                e.printStackTrace();            }            Log.d(TAG, &quot;led setOff...&quot;);            break;        case R.id.btn_getCount:            try {                i = ledProxy.LedGetCount();            } catch (RemoteException e) {                e.printStackTrace();            }            Log.d(TAG, &quot;led getCount...&quot; + String.valueOf(i));            break;        case R.id.btn_close:            try {                ledProxy.LedClose();            } catch (RemoteException e) {                e.printStackTrace();            }            Log.d(TAG, &quot;led close...&quot;);            if (ifInited) {                ifInited = false;                enableBtn();            }        default:            break;    }}</code></pre><p>}</p><p>代码相关说明：</p><ol><li>onCreate实现控件的获取绑定、点击监听以及状态切换</li><li>onResume实现获取LedService的远程Proxy代理对象，方便跨进程方法调用</li><li>onClick实现相关控件的点击实现，即调用Proxy代理类相关方法，其中还有控件状态使能控制</li></ol><h4 id="SM获取系统服务的说明"><a href="#SM获取系统服务的说明" class="headerlink" title="SM获取系统服务的说明"></a>SM获取系统服务的说明</h4><p>对于方法 ServiceManager.getService(“led”)，此方法为系统api，直接调用在Android Studio无法实现，这里提供两个方法实现。</p><ul><li>方法一：本项目实现方法，导入系统编译中间framework.jar实现系统api无缝调用</li><li><p>方法二：使用反射方式进行调用，具体方式如下：</p><pre><code>public ILedService getILedServiceProxy() {    ILedService LedService = null;    try {        // 根据指定文件，反射获取类 ServiceManager        Class&lt;?&gt; aClass = Class.forName(&quot;android.os.ServiceManager&quot;);        // 获取类中定义的具体方法 getService        Method getService = aClass.getDeclaredMethod(&quot;getService&quot;, String.class);        // 反射方法到类，即 method + invoke + 类        Object ledIBinder = getService.invoke(aClass, &quot;led&quot;);        // 将 IBinder 对象转成远程代理 Proxy        LedService = ILedService.Stub.asInterface((IBinder) ledIBinder);    } catch (IllegalAccessException e) {        e.printStackTrace();    } catch (InvocationTargetException e) {        e.printStackTrace();    } catch (ClassNotFoundException e) {        e.printStackTrace();    } catch (NoSuchMethodException e) {        e.printStackTrace();    }    return LedService;}</code></pre></li></ul><h3 id="安装调试"><a href="#安装调试" class="headerlink" title="安装调试"></a>安装调试</h3><p>将之前打包好的rom刷写到设备中，重启开机后，Android Studio中终端adb连接到设备中去，开启软件进行调试。</p><h4 id="Log日志"><a href="#Log日志" class="headerlink" title="Log日志"></a>Log日志</h4><p>终端输入 logcat | grep “Led”,查看最终的log日志如下：</p><ol><li><p>led_init</p><p> 3673 I LedServiceJNI: led_init start …<br> 3673 I LedServiceJNI: Device led found…<br> 3673 I LedHALStub: Open led_device successfully!<br> 3673 I LedServiceJNI: led_init success!!!<br> 3659 D Led_MainActivity: led init…0</p></li><li><p>led_setOn</p><p> 2340 D LedService: LedService native_LedSetOn<br> 2340 I LedServiceJNI: led_set_on<br> 2340 I LedHALStub: led_set_on function call!<br> 3659 D Led_MainActivity: led setOn…</p></li><li><p>led_setOff</p><p> 2384 D LedService: LedService native_LedSetOff<br> 2384 I LedServiceJNI: led_set_off<br> 2384 I LedHALStub: led_set_off function call!<br> 3659 D Led_MainActivity: led setOff…</p></li><li><p>led_getCount</p><p> 3673 D LedService: LedService native_LedGetCount<br> 3673 I LedServiceJNI: led_getCount 4<br> 3673 I LedHALStub: led_getCount function call,return led_getCount is 4!<br> 3659 D Led_MainActivity: led getCount…4</p></li><li><p>led_close</p><p> 2391 D LedService: LedService native_LedClose<br> 2391 I LedServiceJNI: led_close<br> 2391 I LedHALStub: led_device close successfully!<br> 3659 D Led_MainActivity: led close…</p></li></ol><p>从log日志可以看出，从上层应用的调用到驱动，经过的流程 Led_MainActivity -&gt; LedServic -&gt; nativce_jni&gt;HAL -&gt; Led_MainActivity ,一次调用过程，完整的透过了安卓的整个框架。至此从安卓HAL层驱动到上层应用的实现就完整结束了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>经过上面的代码实战，透过安卓的硬件抽象层框架，完整的实现了从上层应用到中间层framework，再到驱动的全程调用。这里总结一下整个流程。</p><ol><li>基于安卓HAL框架，实现自定义的驱动，生成相关 *.so 动态库</li><li>对应驱动具体方法，定义framework层AIDL文件，添加到编译文件，并生成相应的Stub类文件</li><li>新建 <em>Service.java 自定义服务，继承于AIDL的中间编译文件 </em>Service.Stub</li><li>将自定义服务添加到SysterServer中，便于登记注册到SM中</li><li>新建 com_android_server_*Service.cpp，完成对应native层代码实现，主要包括方法注册，模块的初始化和设备的打开</li><li>将新建c层jni文件添加到onload.cpp中，实现方法的注册和映射</li><li>修改对应的 Android.mk，将新建文件添加到编译系统中去</li><li>生成最新系统包进行刷写</li><li>提取AIDL文件到应用中，新建应用App通过SM获取LedService服务的远程代理对象Proxy，测试调用其方法，方法经过层层调用，最终指向驱动层实现代码</li></ol><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>相关参考</p><blockquote><p>高焕堂安卓架构师系列视频</p></blockquote><blockquote><p>Android系统源代码情景分析</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;基于Android HAL层驱动框架，开发者很容易将自己的驱动实现在用户空间，实现驱动细节的隐藏，而不必在内核中进行修改，避免因修改内核，公开源码进而损害自己的商业利益。本章节，我们就对HAL层驱动框架进行实现，由底层驱动到中间层framework，继而进入上层应用端，彻底完成上层应用到底层驱动的一一实现。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="驱动" scheme="https://891904833.gitee.io/FuckCode/categories/%E9%A9%B1%E5%8A%A8/"/>
    
    
      <category term="Hal" scheme="https://891904833.gitee.io/FuckCode/tags/Hal/"/>
    
  </entry>
  
</feed>
